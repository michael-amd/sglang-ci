INFO 10-24 11:30:41 __init__.py:179] Automatically detected platform rocm.
WARNING 10-24 11:30:41 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-24 11:30:42] WARNING server_args.py:1185: DP attention is enabled. The chunked prefill size is adjusted to 16384 to avoid MoE kernel issues. 
[2025-10-24 11:30:42] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-24 11:30:43] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=0.3, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', elastic_ep_backend=None, mooncake_ib_device=None, tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=892752498, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=8, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', enable_beta_spec=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=True, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-24 11:30:43] Using default HuggingFace chat template with detected content format: string
INFO 10-24 11:30:51 __init__.py:179] Automatically detected platform rocm.
INFO 10-24 11:30:51 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-24 11:30:52] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-24 11:30:52] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
INFO 10-24 11:31:01 __init__.py:179] Automatically detected platform rocm.
INFO 10-24 11:31:01 __init__.py:179] Automatically detected platform rocm.
INFO 10-24 11:31:01 __init__.py:179] Automatically detected platform rocm.
INFO 10-24 11:31:01 __init__.py:179] Automatically detected platform rocm.
INFO 10-24 11:31:01 __init__.py:179] Automatically detected platform rocm.
INFO 10-24 11:31:01 __init__.py:179] Automatically detected platform rocm.
INFO 10-24 11:31:01 __init__.py:179] Automatically detected platform rocm.
INFO 10-24 11:31:01 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-24 11:31:03] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-24 11:31:03 DP2 TP2] Process 817 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-24 11:31:03] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-24 11:31:03 DP0 TP0] Process 815 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
`torch_dtype` is deprecated! Use `dtype` instead!
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-24 11:31:03] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-24 11:31:03 DP2 TP2] Init torch distributed begin.
[2025-10-24 11:31:03] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-24 11:31:03 DP5 TP5] Process 820 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
`torch_dtype` is deprecated! Use `dtype` instead!
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-24 11:31:03 DP7 TP7] Process 822 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-24 11:31:03 DP0 TP0] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-24 11:31:03] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-24 11:31:03] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-24 11:31:04 DP6 TP6] Process 821 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
[2025-10-24 11:31:04 DP3 TP3] Process 818 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-24 11:31:04 DP5 TP5] Init torch distributed begin.
[2025-10-24 11:31:04 DP7 TP7] Init torch distributed begin.
[2025-10-24 11:31:04] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-24 11:31:04] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-24 11:31:04 DP4 TP4] Process 819 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-24 11:31:04 DP1 TP1] Process 816 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-24 11:31:04 DP3 TP3] Init torch distributed begin.
[2025-10-24 11:31:04 DP6 TP6] Init torch distributed begin.
[2025-10-24 11:31:04 DP4 TP4] Init torch distributed begin.
[2025-10-24 11:31:04 DP1 TP1] Init torch distributed begin.
[2025-10-24 11:31:04 DP0 TP0] sglang is using nccl==2.21.5
[2025-10-24 11:31:06 DP0 TP0] Init torch distributed ends. mem usage=3.63 GB
[2025-10-24 11:31:06 DP7 TP7] Init torch distributed ends. mem usage=3.92 GB
[2025-10-24 11:31:06 DP6 TP6] Init torch distributed ends. mem usage=3.93 GB
[2025-10-24 11:31:06 DP5 TP5] Init torch distributed ends. mem usage=3.91 GB
[2025-10-24 11:31:06 DP3 TP3] Init torch distributed ends. mem usage=4.04 GB
[2025-10-24 11:31:06 DP4 TP4] Init torch distributed ends. mem usage=3.99 GB
[2025-10-24 11:31:06 DP2 TP2] Init torch distributed ends. mem usage=4.05 GB
[2025-10-24 11:31:06 DP1 TP1] Init torch distributed ends. mem usage=4.05 GB
[2025-10-24 11:31:08 DP5 TP5] Load weight begin. avail mem=187.35 GB
[2025-10-24 11:31:08 DP2 TP2] Load weight begin. avail mem=187.21 GB
[2025-10-24 11:31:08 DP6 TP6] Load weight begin. avail mem=187.33 GB
[2025-10-24 11:31:08 DP3 TP3] Load weight begin. avail mem=187.22 GB
[2025-10-24 11:31:08 DP0 TP0] Load weight begin. avail mem=187.63 GB
[2025-10-24 11:31:08 DP4 TP4] Load weight begin. avail mem=187.27 GB
[2025-10-24 11:31:08 DP0 TP0] Detected fp8 checkpoint.
[2025-10-24 11:31:08 DP0 TP0] Only Deepseek V3/R1 on NV-platform with capability >= 80 can use shared experts fusion optimization. Shared experts fusion optimization is disabled.
[2025-10-24 11:31:08 DP1 TP1] Load weight begin. avail mem=187.21 GB
[2025-10-24 11:31:08 DP7 TP7] Load weight begin. avail mem=187.34 GB
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:30,  5.37it/s]
Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:00<00:30,  5.34it/s]
Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:00<00:16,  9.64it/s]
Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:00<00:13, 11.62it/s]
Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:00<00:11, 13.92it/s]
Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:01<00:27,  5.52it/s]
Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:02<00:47,  3.20it/s]
Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:02<00:39,  3.75it/s]
Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:03<00:35,  4.19it/s]
Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:03<00:31,  4.72it/s]
Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:03<00:27,  5.28it/s]
Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:03<00:27,  5.31it/s]
Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:03<00:30,  4.65it/s]
Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:03<00:30,  4.75it/s]
Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:04<00:30,  4.71it/s]
Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:04<00:29,  4.76it/s]
Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:04<00:25,  5.40it/s]
Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:04<00:19,  7.12it/s]
Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:04<00:12, 10.93it/s]
Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:04<00:09, 14.10it/s]
Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:05<00:06, 18.33it/s]
Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:06<00:19,  6.57it/s]
Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:06<00:15,  7.74it/s]
Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:06<00:12,  9.72it/s]
Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:06<00:13,  8.75it/s]
Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:06<00:14,  8.22it/s]
Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:07<00:14,  7.70it/s]
Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:07<00:14,  7.64it/s]
Loading safetensors checkpoint shards:  33% Completed | 53/163 [00:07<00:11,  9.25it/s]
Loading safetensors checkpoint shards:  34% Completed | 56/163 [00:07<00:08, 11.91it/s]
Loading safetensors checkpoint shards:  36% Completed | 58/163 [00:07<00:08, 12.44it/s]
Loading safetensors checkpoint shards:  37% Completed | 60/163 [00:08<00:07, 13.78it/s]
Loading safetensors checkpoint shards:  39% Completed | 63/163 [00:08<00:05, 16.84it/s]
Loading safetensors checkpoint shards:  40% Completed | 65/163 [00:08<00:05, 17.15it/s]
Loading safetensors checkpoint shards:  41% Completed | 67/163 [00:08<00:05, 16.86it/s]
Loading safetensors checkpoint shards:  42% Completed | 69/163 [00:09<00:17,  5.51it/s]
Loading safetensors checkpoint shards:  44% Completed | 71/163 [00:09<00:13,  6.86it/s]
Loading safetensors checkpoint shards:  45% Completed | 73/163 [00:10<00:19,  4.71it/s]
Loading safetensors checkpoint shards:  46% Completed | 75/163 [00:10<00:22,  3.94it/s]
Loading safetensors checkpoint shards:  47% Completed | 76/163 [00:11<00:26,  3.30it/s]
Loading safetensors checkpoint shards:  47% Completed | 77/163 [00:11<00:25,  3.41it/s]
Loading safetensors checkpoint shards:  48% Completed | 78/163 [00:11<00:22,  3.85it/s]
Loading safetensors checkpoint shards:  50% Completed | 81/163 [00:12<00:12,  6.41it/s]
Loading safetensors checkpoint shards:  51% Completed | 83/163 [00:12<00:10,  7.93it/s]
Loading safetensors checkpoint shards:  52% Completed | 85/163 [00:12<00:13,  5.99it/s]
Loading safetensors checkpoint shards:  53% Completed | 86/163 [00:12<00:14,  5.26it/s]
Loading safetensors checkpoint shards:  53% Completed | 87/163 [00:13<00:18,  4.08it/s]
Loading safetensors checkpoint shards:  54% Completed | 88/163 [00:13<00:20,  3.69it/s]
Loading safetensors checkpoint shards:  56% Completed | 91/163 [00:13<00:11,  6.29it/s]
Loading safetensors checkpoint shards:  57% Completed | 93/163 [00:13<00:08,  8.00it/s]
Loading safetensors checkpoint shards:  58% Completed | 95/163 [00:14<00:07,  9.23it/s]
Loading safetensors checkpoint shards:  60% Completed | 97/163 [00:14<00:07,  9.26it/s]
Loading safetensors checkpoint shards:  61% Completed | 99/163 [00:14<00:06, 10.65it/s]
Loading safetensors checkpoint shards:  63% Completed | 102/163 [00:14<00:04, 13.10it/s]
Loading safetensors checkpoint shards:  64% Completed | 104/163 [00:14<00:04, 13.47it/s]
Loading safetensors checkpoint shards:  65% Completed | 106/163 [00:15<00:10,  5.69it/s]
Loading safetensors checkpoint shards:  66% Completed | 108/163 [00:15<00:08,  6.73it/s]
Loading safetensors checkpoint shards:  67% Completed | 110/163 [00:15<00:06,  8.33it/s]
Loading safetensors checkpoint shards:  69% Completed | 113/163 [00:16<00:04, 11.07it/s]
Loading safetensors checkpoint shards:  71% Completed | 116/163 [00:16<00:03, 13.58it/s]
Loading safetensors checkpoint shards:  73% Completed | 119/163 [00:16<00:02, 16.03it/s]
Loading safetensors checkpoint shards:  75% Completed | 122/163 [00:16<00:03, 12.60it/s]
Loading safetensors checkpoint shards:  76% Completed | 124/163 [00:16<00:02, 13.78it/s]
Loading safetensors checkpoint shards:  78% Completed | 127/163 [00:16<00:02, 16.08it/s]
Loading safetensors checkpoint shards:  79% Completed | 129/163 [00:17<00:02, 13.04it/s]
Loading safetensors checkpoint shards:  81% Completed | 132/163 [00:17<00:02, 15.29it/s]
Loading safetensors checkpoint shards:  83% Completed | 135/163 [00:17<00:01, 17.32it/s]
Loading safetensors checkpoint shards:  85% Completed | 138/163 [00:17<00:01, 18.33it/s]
Loading safetensors checkpoint shards:  87% Completed | 141/163 [00:17<00:01, 17.88it/s]
Loading safetensors checkpoint shards:  88% Completed | 143/163 [00:18<00:01, 11.62it/s]
Loading safetensors checkpoint shards:  90% Completed | 146/163 [00:18<00:01, 13.64it/s]
Loading safetensors checkpoint shards:  91% Completed | 148/163 [00:18<00:01, 14.71it/s]
Loading safetensors checkpoint shards:  92% Completed | 150/163 [00:18<00:00, 15.53it/s]
Loading safetensors checkpoint shards:  94% Completed | 153/163 [00:18<00:00, 17.07it/s]
Loading safetensors checkpoint shards:  95% Completed | 155/163 [00:19<00:01,  5.83it/s]
Loading safetensors checkpoint shards:  96% Completed | 157/163 [00:19<00:00,  7.12it/s]
Loading safetensors checkpoint shards:  98% Completed | 160/163 [00:19<00:00,  9.07it/s]
Loading safetensors checkpoint shards:  99% Completed | 162/163 [00:19<00:00, 10.46it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:19<00:00,  8.16it/s]

[2025-10-24 11:32:01 DP1 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.74 GB, mem usage=90.47 GB.
[2025-10-24 11:32:02 DP2 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.74 GB, mem usage=90.47 GB.
[2025-10-24 11:32:02 DP0 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=97.16 GB, mem usage=90.47 GB.
[2025-10-24 11:32:03 DP3 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.75 GB, mem usage=90.47 GB.
[2025-10-24 11:32:12 DP6 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.87 GB, mem usage=90.47 GB.
[2025-10-24 11:32:12 DP5 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.88 GB, mem usage=90.47 GB.
[2025-10-24 11:32:13 DP4 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.80 GB, mem usage=90.47 GB.
[2025-10-24 11:32:13 DP7 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.88 GB, mem usage=90.47 GB.
[2025-10-24 11:32:13 DP0 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-24 11:32:13 DP4 TP4] KV Cache is allocated. #tokens: 561715, KV size: 36.76 GB
[2025-10-24 11:32:13 DP4 TP4] Memory pool end. avail mem=58.73 GB
[2025-10-24 11:32:13 DP6 TP6] KV Cache is allocated. #tokens: 561715, KV size: 36.76 GB
[2025-10-24 11:32:13 DP6 TP6] Memory pool end. avail mem=58.80 GB
[2025-10-24 11:32:13 DP7 TP7] KV Cache is allocated. #tokens: 561715, KV size: 36.76 GB
[2025-10-24 11:32:13 DP7 TP7] Memory pool end. avail mem=58.81 GB
[2025-10-24 11:32:13 DP5 TP5] KV Cache is allocated. #tokens: 561715, KV size: 36.76 GB
[2025-10-24 11:32:13 DP5 TP5] Memory pool end. avail mem=58.82 GB
[2025-10-24 11:32:13 DP0 TP0] KV Cache is allocated. #tokens: 561715, KV size: 36.76 GB
[2025-10-24 11:32:13 DP2 TP2] KV Cache is allocated. #tokens: 561715, KV size: 36.76 GB
[2025-10-24 11:32:13 DP3 TP3] KV Cache is allocated. #tokens: 561715, KV size: 36.76 GB
[2025-10-24 11:32:13 DP0 TP0] Memory pool end. avail mem=59.09 GB
[2025-10-24 11:32:13 DP2 TP2] Memory pool end. avail mem=58.67 GB
[2025-10-24 11:32:13 DP3 TP3] Memory pool end. avail mem=58.68 GB
[2025-10-24 11:32:13 DP1 TP1] KV Cache is allocated. #tokens: 561715, KV size: 36.76 GB
[2025-10-24 11:32:13 DP1 TP1] Memory pool end. avail mem=58.68 GB
[2025-10-24 11:32:15 DP7 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=58.60 GB
[2025-10-24 11:32:15 DP1 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=58.47 GB
[2025-10-24 11:32:15 DP6 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=58.59 GB
[2025-10-24 11:32:15 DP3 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=58.48 GB
[2025-10-24 11:32:15 DP5 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=58.61 GB
[2025-10-24 11:32:15 DP0 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=58.89 GB
[2025-10-24 11:32:15 DP0 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
[2025-10-24 11:32:15 DP2 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=58.46 GB
[2025-10-24 11:32:15 DP4 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=58.53 GB
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=58.25 GB):   0%|          | 0/52 [00:00<?, ?it/s][aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-24 11:32:16 DP2 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-24 11:32:16 DP0 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-24 11:32:16 DP1 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-24 11:32:16 DP7 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-24 11:32:16 DP4 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-24 11:32:16 DP3 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-24 11:32:16 DP5 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-24 11:32:16 DP6 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-24 11:32:18 DP5 TP5] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-24 11:32:18 DP7 TP7] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-24 11:32:18 DP6 TP6] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-24 11:32:18 DP3 TP3] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-24 11:32:18 DP2 TP2] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-24 11:32:18 DP1 TP1] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-24 11:32:18 DP0 TP0] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:18 DP5 TP5] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:18 DP5 TP5] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:18 DP7 TP7] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:18 DP7 TP7] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:18 DP6 TP6] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:18 DP6 TP6] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:18 DP3 TP3] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:18 DP3 TP3] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:18 DP2 TP2] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:18 DP2 TP2] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-24 11:32:18 DP4 TP4] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:18 DP1 TP1] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:18 DP1 TP1] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:18 DP0 TP0] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:18 DP0 TP0] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:19 DP4 TP4] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:19 DP4 TP4] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:19 DP3 TP3] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:19 DP2 TP2] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:19 DP1 TP1] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:19 DP3 TP3] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:19 DP1 TP1] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:19 DP2 TP2] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:19 DP7 TP7] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:19 DP6 TP6] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:19 DP5 TP5] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:19 DP4 TP4] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:19 DP0 TP0] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:19 DP7 TP7] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:19 DP6 TP6] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:19 DP5 TP5] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:19 DP4 TP4] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:19 DP0 TP0] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:19 DP2 TP2] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:19 DP3 TP3] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:19 DP1 TP1] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:19 DP7 TP7] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:19 DP4 TP4] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:19 DP5 TP5] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:19 DP6 TP6] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:19 DP0 TP0] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[rank2]:[W1024 11:32:21.982207973 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank3]:[W1024 11:32:21.982287974 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank0]:[W1024 11:32:21.982290041 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank6]:[W1024 11:32:21.982299279 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank1]:[W1024 11:32:21.982347781 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank5]:[W1024 11:32:21.982308108 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank7]:[W1024 11:32:21.982347312 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank4]:[W1024 11:32:21.982383618 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Capturing batches (bs=512 avail_mem=58.25 GB):   2%|         | 1/52 [00:06<05:06,  6.01s/it]Capturing batches (bs=496 avail_mem=50.25 GB):   2%|         | 1/52 [00:06<05:06,  6.01s/it]Capturing batches (bs=496 avail_mem=50.25 GB):   4%|         | 2/52 [00:06<02:24,  2.88s/it]Capturing batches (bs=480 avail_mem=50.24 GB):   4%|         | 2/52 [00:06<02:24,  2.88s/it]Capturing batches (bs=480 avail_mem=50.24 GB):   6%|         | 3/52 [00:07<01:32,  1.89s/it]Capturing batches (bs=464 avail_mem=50.23 GB):   6%|         | 3/52 [00:07<01:32,  1.89s/it]Capturing batches (bs=464 avail_mem=50.23 GB):   8%|         | 4/52 [00:08<01:08,  1.42s/it]Capturing batches (bs=448 avail_mem=50.22 GB):   8%|         | 4/52 [00:08<01:08,  1.42s/it]Capturing batches (bs=448 avail_mem=50.22 GB):  10%|         | 5/52 [00:08<00:54,  1.16s/it]Capturing batches (bs=432 avail_mem=50.22 GB):  10%|         | 5/52 [00:08<00:54,  1.16s/it]Capturing batches (bs=432 avail_mem=50.22 GB):  12%|        | 6/52 [00:09<00:46,  1.00s/it]Capturing batches (bs=416 avail_mem=50.21 GB):  12%|        | 6/52 [00:09<00:46,  1.00s/it]Capturing batches (bs=416 avail_mem=50.21 GB):  13%|        | 7/52 [00:10<00:40,  1.10it/s]Capturing batches (bs=400 avail_mem=50.20 GB):  13%|        | 7/52 [00:10<00:40,  1.10it/s]Capturing batches (bs=400 avail_mem=50.20 GB):  15%|        | 8/52 [00:10<00:35,  1.24it/s]Capturing batches (bs=384 avail_mem=50.20 GB):  15%|        | 8/52 [00:10<00:35,  1.24it/s]Capturing batches (bs=384 avail_mem=50.20 GB):  17%|        | 9/52 [00:11<00:30,  1.41it/s]Capturing batches (bs=368 avail_mem=50.19 GB):  17%|        | 9/52 [00:11<00:30,  1.41it/s]Capturing batches (bs=368 avail_mem=50.19 GB):  19%|        | 10/52 [00:12<00:29,  1.41it/s]Capturing batches (bs=352 avail_mem=50.18 GB):  19%|        | 10/52 [00:12<00:29,  1.41it/s]Capturing batches (bs=352 avail_mem=50.18 GB):  21%|        | 11/52 [00:12<00:29,  1.41it/s]Capturing batches (bs=336 avail_mem=50.18 GB):  21%|        | 11/52 [00:12<00:29,  1.41it/s]Capturing batches (bs=336 avail_mem=50.18 GB):  23%|       | 12/52 [00:13<00:26,  1.49it/s]Capturing batches (bs=320 avail_mem=50.17 GB):  23%|       | 12/52 [00:13<00:26,  1.49it/s]Capturing batches (bs=320 avail_mem=50.17 GB):  25%|       | 13/52 [00:13<00:25,  1.53it/s]Capturing batches (bs=304 avail_mem=50.16 GB):  25%|       | 13/52 [00:13<00:25,  1.53it/s]Capturing batches (bs=304 avail_mem=50.16 GB):  27%|       | 14/52 [00:14<00:22,  1.66it/s]Capturing batches (bs=288 avail_mem=50.15 GB):  27%|       | 14/52 [00:14<00:22,  1.66it/s]Capturing batches (bs=288 avail_mem=50.15 GB):  29%|       | 15/52 [00:14<00:20,  1.85it/s]Capturing batches (bs=272 avail_mem=50.15 GB):  29%|       | 15/52 [00:14<00:20,  1.85it/s]Capturing batches (bs=272 avail_mem=50.15 GB):  31%|       | 16/52 [00:15<00:20,  1.78it/s]Capturing batches (bs=256 avail_mem=50.14 GB):  31%|       | 16/52 [00:15<00:20,  1.78it/s][aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:31 DP5 TP5] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:31 DP5 TP5] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:31 DP5 TP5] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:31 DP5 TP5] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:31 DP6 TP6] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:31 DP6 TP6] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:31 DP6 TP6] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:31 DP6 TP6] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:31 DP7 TP7] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:31 DP7 TP7] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:31 DP7 TP7] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:31 DP7 TP7] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:31 DP2 TP2] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:31 DP2 TP2] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:31 DP2 TP2] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:31 DP2 TP2] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:31 DP4 TP4] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:31 DP4 TP4] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:31 DP4 TP4] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:31 DP4 TP4] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:31 DP0 TP0] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:31 DP0 TP0] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:31 DP0 TP0] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:31 DP0 TP0] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:31 DP3 TP3] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:31 DP3 TP3] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:31 DP3 TP3] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:31 DP3 TP3] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:31 DP1 TP1] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:31 DP1 TP1] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:31 DP1 TP1] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:31 DP1 TP1] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
Capturing batches (bs=256 avail_mem=50.14 GB):  33%|      | 17/52 [00:16<00:20,  1.74it/s]Capturing batches (bs=248 avail_mem=50.13 GB):  33%|      | 17/52 [00:16<00:20,  1.74it/s]Capturing batches (bs=248 avail_mem=50.13 GB):  35%|      | 18/52 [00:16<00:19,  1.72it/s]Capturing batches (bs=240 avail_mem=50.13 GB):  35%|      | 18/52 [00:16<00:19,  1.72it/s]Capturing batches (bs=240 avail_mem=50.13 GB):  37%|      | 19/52 [00:17<00:19,  1.70it/s]Capturing batches (bs=232 avail_mem=50.12 GB):  37%|      | 19/52 [00:17<00:19,  1.70it/s]Capturing batches (bs=232 avail_mem=50.12 GB):  38%|      | 20/52 [00:17<00:18,  1.69it/s]Capturing batches (bs=224 avail_mem=50.11 GB):  38%|      | 20/52 [00:17<00:18,  1.69it/s]Capturing batches (bs=224 avail_mem=50.11 GB):  40%|      | 21/52 [00:18<00:18,  1.68it/s]Capturing batches (bs=216 avail_mem=50.11 GB):  40%|      | 21/52 [00:18<00:18,  1.68it/s]Capturing batches (bs=216 avail_mem=50.11 GB):  42%|     | 22/52 [00:18<00:16,  1.77it/s]Capturing batches (bs=208 avail_mem=50.10 GB):  42%|     | 22/52 [00:18<00:16,  1.77it/s]Capturing batches (bs=208 avail_mem=50.10 GB):  44%|     | 23/52 [00:19<00:15,  1.84it/s]Capturing batches (bs=200 avail_mem=50.09 GB):  44%|     | 23/52 [00:19<00:15,  1.84it/s]Capturing batches (bs=200 avail_mem=50.09 GB):  46%|     | 24/52 [00:20<00:15,  1.77it/s]Capturing batches (bs=192 avail_mem=50.09 GB):  46%|     | 24/52 [00:20<00:15,  1.77it/s][aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP0 TP0] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP1 TP1] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP3 TP3] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP7 TP7] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP5 TP5] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP4 TP4] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP2 TP2] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP6 TP6] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:35 DP1 TP1] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:35 DP0 TP0] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:35 DP3 TP3] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:35 DP7 TP7] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:35 DP5 TP5] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:35 DP2 TP2] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:35 DP4 TP4] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:35 DP6 TP6] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP1 TP1] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP3 TP3] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP2 TP2] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP7 TP7] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP0 TP0] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP1 TP1] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP6 TP6] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP5 TP5] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP3 TP3] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP2 TP2] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP4 TP4] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP7 TP7] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP0 TP0] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP6 TP6] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP5 TP5] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:35 DP4 TP4] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
Capturing batches (bs=192 avail_mem=50.09 GB):  48%|     | 25/52 [00:20<00:13,  1.94it/s]Capturing batches (bs=184 avail_mem=50.08 GB):  48%|     | 25/52 [00:20<00:13,  1.94it/s]Capturing batches (bs=184 avail_mem=50.08 GB):  50%|     | 26/52 [00:20<00:12,  2.09it/s]Capturing batches (bs=176 avail_mem=50.08 GB):  50%|     | 26/52 [00:20<00:12,  2.09it/s]Capturing batches (bs=176 avail_mem=50.08 GB):  52%|    | 27/52 [00:21<00:11,  2.18it/s]Capturing batches (bs=168 avail_mem=50.07 GB):  52%|    | 27/52 [00:21<00:11,  2.18it/s]Capturing batches (bs=168 avail_mem=50.07 GB):  54%|    | 28/52 [00:21<00:11,  2.00it/s]Capturing batches (bs=160 avail_mem=50.06 GB):  54%|    | 28/52 [00:21<00:11,  2.00it/s]Capturing batches (bs=160 avail_mem=50.06 GB):  56%|    | 29/52 [00:22<00:10,  2.12it/s]Capturing batches (bs=152 avail_mem=50.06 GB):  56%|    | 29/52 [00:22<00:10,  2.12it/s]Capturing batches (bs=152 avail_mem=50.06 GB):  58%|    | 30/52 [00:22<00:11,  1.96it/s]Capturing batches (bs=144 avail_mem=50.05 GB):  58%|    | 30/52 [00:22<00:11,  1.96it/s]Capturing batches (bs=144 avail_mem=50.05 GB):  60%|    | 31/52 [00:23<00:10,  2.09it/s]Capturing batches (bs=136 avail_mem=50.01 GB):  60%|    | 31/52 [00:23<00:10,  2.09it/s]Capturing batches (bs=136 avail_mem=50.01 GB):  62%|   | 32/52 [00:23<00:09,  2.19it/s]Capturing batches (bs=128 avail_mem=49.99 GB):  62%|   | 32/52 [00:23<00:09,  2.19it/s][aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:39 DP1 TP1] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:39 DP5 TP5] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:39 DP3 TP3] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:39 DP7 TP7] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:39 DP6 TP6] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:39 DP2 TP2] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:39 DP0 TP0] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:39 DP4 TP4] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:39 DP1 TP1] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:39 DP5 TP5] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:39 DP3 TP3] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:39 DP7 TP7] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:39 DP6 TP6] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:39 DP2 TP2] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:39 DP0 TP0] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-24 11:32:39 DP4 TP4] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:39 DP1 TP1] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:39 DP3 TP3] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:39 DP5 TP5] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:39 DP2 TP2] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:39 DP7 TP7] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:39 DP1 TP1] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:39 DP3 TP3] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:39 DP0 TP0] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:39 DP6 TP6] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:39 DP5 TP5] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:39 DP2 TP2] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:39 DP7 TP7] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:39 DP4 TP4] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:39 DP0 TP0] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:39 DP6 TP6] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:39 DP4 TP4] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
Capturing batches (bs=128 avail_mem=49.99 GB):  63%|   | 33/52 [00:24<00:08,  2.29it/s]Capturing batches (bs=120 avail_mem=49.98 GB):  63%|   | 33/52 [00:24<00:08,  2.29it/s][aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:40 DP4 TP4] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:40 DP6 TP6] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:40 DP1 TP1] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:40 DP5 TP5] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:40 DP7 TP7] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:40 DP0 TP0] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:40 DP2 TP2] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:40 DP3 TP3] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=120 avail_mem=49.98 GB):  65%|   | 34/52 [00:24<00:08,  2.06it/s]Capturing batches (bs=112 avail_mem=49.97 GB):  65%|   | 34/52 [00:24<00:08,  2.06it/s][aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:40 DP2 TP2] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:40 DP1 TP1] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:40 DP7 TP7] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:40 DP0 TP0] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:40 DP3 TP3] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:40 DP5 TP5] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:40 DP6 TP6] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:40 DP4 TP4] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=112 avail_mem=49.97 GB):  67%|   | 35/52 [00:25<00:07,  2.18it/s]Capturing batches (bs=104 avail_mem=49.97 GB):  67%|   | 35/52 [00:25<00:07,  2.18it/s][aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:41 DP7 TP7] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:41 DP5 TP5] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:41 DP0 TP0] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:41 DP3 TP3] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:41 DP1 TP1] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:41 DP2 TP2] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:41 DP6 TP6] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:41 DP4 TP4] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=104 avail_mem=49.97 GB):  69%|   | 36/52 [00:25<00:08,  1.99it/s]Capturing batches (bs=96 avail_mem=49.96 GB):  69%|   | 36/52 [00:25<00:08,  1.99it/s] [aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:41 DP1 TP1] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:41 DP3 TP3] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:41 DP0 TP0] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:41 DP7 TP7] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:41 DP5 TP5] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:41 DP6 TP6] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:41 DP2 TP2] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:41 DP4 TP4] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=96 avail_mem=49.96 GB):  71%|   | 37/52 [00:26<00:07,  2.12it/s]Capturing batches (bs=88 avail_mem=49.94 GB):  71%|   | 37/52 [00:26<00:07,  2.12it/s][aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:42 DP6 TP6] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:42 DP4 TP4] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:42 DP5 TP5] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:42 DP7 TP7] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:42 DP1 TP1] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:42 DP0 TP0] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:42 DP2 TP2] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:42 DP3 TP3] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=88 avail_mem=49.94 GB):  73%|  | 38/52 [00:26<00:07,  1.95it/s]Capturing batches (bs=80 avail_mem=49.91 GB):  73%|  | 38/52 [00:26<00:07,  1.95it/s][aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:42 DP2 TP2] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:42 DP3 TP3] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:42 DP1 TP1] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:42 DP7 TP7] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:42 DP0 TP0] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:42 DP5 TP5] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:42 DP6 TP6] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:42 DP4 TP4] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=80 avail_mem=49.91 GB):  75%|  | 39/52 [00:27<00:06,  2.09it/s]Capturing batches (bs=72 avail_mem=49.89 GB):  75%|  | 39/52 [00:27<00:06,  2.09it/s][aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:43 DP4 TP4] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:43 DP7 TP7] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:43 DP6 TP6] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:43 DP0 TP0] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:43 DP2 TP2] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:43 DP3 TP3] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:43 DP1 TP1] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:43 DP5 TP5] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=72 avail_mem=49.89 GB):  77%|  | 40/52 [00:27<00:06,  1.95it/s]Capturing batches (bs=64 avail_mem=49.88 GB):  77%|  | 40/52 [00:27<00:06,  1.95it/s][aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP3 TP3] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP1 TP1] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP2 TP2] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP5 TP5] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP6 TP6] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP7 TP7] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP0 TP0] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP4 TP4] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP3 TP3] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP1 TP1] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP5 TP5] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP2 TP2] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP6 TP6] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP7 TP7] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP0 TP0] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP4 TP4] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP3 TP3] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP1 TP1] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP2 TP2] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP7 TP7] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP5 TP5] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP3 TP3] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP0 TP0] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP1 TP1] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP6 TP6] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP2 TP2] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP7 TP7] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP5 TP5] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP4 TP4] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP0 TP0] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP6 TP6] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:32:43 DP4 TP4] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:43 DP3 TP3] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:43 DP1 TP1] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:43 DP2 TP2] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:43 DP7 TP7] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:43 DP5 TP5] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:43 DP0 TP0] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:43 DP6 TP6] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:43 DP4 TP4] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=64 avail_mem=49.88 GB):  79%|  | 41/52 [00:28<00:05,  2.09it/s]Capturing batches (bs=56 avail_mem=49.87 GB):  79%|  | 41/52 [00:28<00:05,  2.09it/s][aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:44 DP6 TP6] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:44 DP7 TP7] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:44 DP2 TP2] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:44 DP1 TP1] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:44 DP3 TP3] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:44 DP5 TP5] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:44 DP4 TP4] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:44 DP0 TP0] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=56 avail_mem=49.87 GB):  81%|  | 42/52 [00:28<00:05,  1.94it/s]Capturing batches (bs=48 avail_mem=49.86 GB):  81%|  | 42/52 [00:28<00:05,  1.94it/s][aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:44 DP1 TP1] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:44 DP2 TP2] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:44 DP7 TP7] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:44 DP5 TP5] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:44 DP4 TP4] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:44 DP6 TP6] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:44 DP3 TP3] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:44 DP0 TP0] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=48 avail_mem=49.86 GB):  83%| | 43/52 [00:29<00:04,  2.08it/s]Capturing batches (bs=40 avail_mem=49.86 GB):  83%| | 43/52 [00:29<00:04,  2.08it/s][aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:45 DP1 TP1] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:45 DP2 TP2] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:45 DP3 TP3] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:45 DP0 TP0] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:45 DP4 TP4] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:45 DP7 TP7] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:45 DP5 TP5] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:32:45 DP6 TP6] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=40 avail_mem=49.86 GB):  85%| | 44/52 [00:29<00:04,  1.93it/s]Capturing batches (bs=32 avail_mem=49.85 GB):  85%| | 44/52 [00:29<00:04,  1.93it/s][rank4]:W1024 11:32:47.916000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:32:47.925000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:32:47.934000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1024 11:32:47.952000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1024 11:32:47.962000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:32:47.969000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1024 11:32:47.979000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1024 11:32:47.992000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1024 11:32:48.000000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1024 11:32:48.009000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:32:48.027000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:32:48.037000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:32:48.045000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:32:48.054000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:32:48.090000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:32:48.097000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:32:48.106000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:32:48.123000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:32:48.133000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:32:48.142000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:32:48.151000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1024 11:32:48.382000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1024 11:32:48.460000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:32:48.558000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1024 11:32:48.650000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:32:48.658000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:32:48.665000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:32:48.681000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:32:48.689000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:32:48.698000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:32:48.707000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:32:48.764000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:32:48.770000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:32:48.777000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:32:48.793000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:32:48.803000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:32:48.818000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:32:48.826000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:32:48.849000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:32:48.855000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:32:48.861000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:32:48.877000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:32:48.888000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:32:48.895000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:32:48.902000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:32:48.918000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:32:48.922000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:32:48.928000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:32:48.944000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:32:48.954000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:32:48.961000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:32:48.969000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:32:49.119000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:32:49.251000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:32:49.320000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:32:49.386000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:32:49.443000 819 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank7]:W1024 11:32:49.445000 822 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank3]:W1024 11:32:49.448000 818 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank1]:W1024 11:32:49.467000 816 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank2]:W1024 11:32:49.474000 817 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank6]:W1024 11:32:49.484000 821 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank5]:W1024 11:32:49.490000 820 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank0]:W1024 11:32:50.174000 815 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0105 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0107 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0109 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0109 ms 96.3% 
  triton_bmm_19 0.0109 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0110 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0112 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_9 0.0115 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0116 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_22 0.0116 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3450 seconds and 0.5769 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0105 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0107 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0108 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0108 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0108 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0111 ms 94.6% 
  triton_bmm_15 0.0111 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_22 0.0114 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_16 0.0114 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0115 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.4161 seconds and 0.7131 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0106 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0108 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_23 0.0108 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0108 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0109 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0109 ms 97.1% 
  triton_bmm_15 0.0113 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_22 0.0115 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_16 0.0116 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_20 0.0116 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.4190 seconds and 0.7253 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0105 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0107 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_21 0.0108 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0108 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0109 ms 96.3% 
  triton_bmm_19 0.0109 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0110 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0115 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0115 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_22 0.0116 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.4622 seconds and 0.6080 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0105 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0107 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_21 0.0108 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0109 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_25 0.0109 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0111 ms 95.3% 
  triton_bmm_15 0.0113 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0115 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_22 0.0116 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0117 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.4283 seconds and 0.6887 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0104 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0104 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_21 0.0107 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0107 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_19 0.0107 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0108 ms 96.3% 
  triton_bmm_15 0.0110 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_22 0.0112 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_16 0.0114 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0115 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3839 seconds and 0.6690 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0106 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0107 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0109 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0110 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0111 ms 95.7% 
  triton_bmm_19 0.0111 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0112 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0116 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0117 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_22 0.0117 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.4316 seconds and 0.6294 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0105 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0107 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_23 0.0107 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0108 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0108 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0115 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_22 0.0116 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0116 ms 90.7% 
  triton_bmm_16 0.0116 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0118 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3829 seconds and 0.4602 seconds precompiling for 27 choices
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1024 11:33:01.108000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:01.474000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:01.543000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:01.604000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:01.611000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:01.635000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:01.639000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:01.901000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:01.975000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:02.058000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:02.107000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:02.122000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:02.170000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1024 11:33:02.408000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1024 11:33:02.872000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1024 11:33:02.961000 817 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1024 11:33:03.381000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:03.458000 816 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
[rank3]:W1024 11:33:03.589000 818 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
[rank6]:W1024 11:33:03.631000 821 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
[rank5]:W1024 11:33:03.766000 820 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
[rank0]:W1024 11:33:03.850000 815 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
[rank4]:W1024 11:33:03.918000 819 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1024 11:33:04.923000 822 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_46 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0101 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0101 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_41 0.0101 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_37 0.0103 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_47 0.0105 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_49 0.0106 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0106 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_31 0.0109 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_45 0.0111 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3083 seconds and 0.6142 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_41 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_39 0.0104 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_48 0.0105 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0106 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0106 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_51 0.0107 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_49 0.0109 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_47 0.0109 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0111 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_45 0.0115 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2395 seconds and 0.6159 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_39 0.0105 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_48 0.0105 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0106 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_41 0.0106 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_37 0.0107 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_47 0.0109 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0110 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_49 0.0111 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0112 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_45 0.0115 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2883 seconds and 0.6212 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_39 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_41 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0103 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0103 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0107 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_31 0.0108 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0108 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_49 0.0109 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_45 0.0111 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3291 seconds and 0.6202 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_46 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0104 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_41 0.0105 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_37 0.0106 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_39 0.0106 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_51 0.0107 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0109 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0110 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_49 0.0111 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_45 0.0113 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3217 seconds and 0.6171 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_39 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_48 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_41 0.0102 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0103 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0104 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0105 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0106 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0107 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_49 0.0107 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_45 0.0111 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2979 seconds and 0.6162 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_39 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_41 0.0104 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0104 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0105 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0107 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0108 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_49 0.0110 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_47 0.0110 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0113 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_45 0.0113 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3180 seconds and 0.6180 seconds precompiling for 27 choices
[rank2]:W1024 11:33:10.241000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:10.325000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:10.425000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_41 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_39 0.0102 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_46 0.0102 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0102 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_51 0.0105 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_37 0.0105 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_47 0.0106 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_49 0.0106 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0110 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_45 0.0112 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3272 seconds and 0.6294 seconds precompiling for 27 choices
[rank1]:W1024 11:33:10.996000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:11.071000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:11.105000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:11.150000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:11.170000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:11.180000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:11.221000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:11.225000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:11.277000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:11.295000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:11.322000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:11.393000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:11.473000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:11.484000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:11.547000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:11.559000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:11.646000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:11.658000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:12.509000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:12.583000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:12.681000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:15.232000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:15.307000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:15.406000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:33:15 DP2 TP2] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank1]:W1024 11:33:15.924000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:15.998000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:16.015000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:16.050000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:16.089000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:16.096000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:16.103000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:16.132000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:33:16 DP1 TP1] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank6]:W1024 11:33:16.177000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:16.196000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:16.231000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:33:16 DP0 TP0] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank6]:W1024 11:33:16.275000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:33:16 DP3 TP3] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:33:16 DP6 TP6] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank4]:W1024 11:33:16.384000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:16.460000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:16.474000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:16.548000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:16.559000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:33:16 DP4 TP4] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank5]:W1024 11:33:16.647000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:33:16 DP5 TP5] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank7]:W1024 11:33:17.391000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:17.466000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:17.565000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:33:17 DP7 TP7] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank6]:W1024 11:33:18.148000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:18.186000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:18.193000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:18.193000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:18.202000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:18.224000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:18.245000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:18.261000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:18.267000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:18.269000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:18.276000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:18.320000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:18.323000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:18.360000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:18.365000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:18.369000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:18.374000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:33:18 DP6 TP6] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank3]:W1024 11:33:18.421000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:33:18 DP4 TP4] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:33:18 DP5 TP5] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:33:18 DP2 TP2] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:33:18 DP0 TP0] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:33:18 DP3 TP3] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank1]:W1024 11:33:18.628000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:18.702000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:18.797000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:18.801000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:33:18 DP1 TP1] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank7]:W1024 11:33:18.872000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:18.970000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:33:19 DP7 TP7] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank5]:W1024 11:33:19.539000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:19.550000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:19.560000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:19.568000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:19.580000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:19.590000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:19.597000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:19.619000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:19.779000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:19.793000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:19.803000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:19.811000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:19.822000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:19.832000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:19.839000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:19.855000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:20.015000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:20.037000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:20.047000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:20.055000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:20.066000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:20.075000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:20.084000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:20.097000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:33:20 DP5 TP5] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:33:20 DP2 TP2] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:33:20 DP1 TP1] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:33:20 DP6 TP6] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:33:20 DP0 TP0] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:33:20 DP4 TP4] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:33:20 DP3 TP3] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:33:20 DP7 TP7] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1024 11:33:21.674000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:21.682000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:21.708000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:21.730000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:21.738000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:21.748000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:21.750000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:21.756000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:21.786000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:21.800000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:21.806000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:21.813000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:21.826000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:21.832000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:21.831000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:21.869000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:21.873000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 11:33:21 DP5 TP5] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[rank6]:W1024 11:33:21.881000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:21.887000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:21.899000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:21.907000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:21.910000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 11:33:21 DP2 TP2] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 11:33:21 DP6 TP6] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 11:33:21 DP1 TP1] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[rank4]:W1024 11:33:21.944000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 11:33:21 DP3 TP3] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 11:33:21 DP7 TP7] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 11:33:21 DP0 TP0] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1024 11:33:22.019000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 11:33:22 DP4 TP4] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0143 ms 100.0% 
  triton_mm_55 0.0275 ms 52.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0422 ms 33.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0457 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0505 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0534 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0582 ms 24.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0584 ms 24.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0610 ms 23.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0749 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1660 seconds and 0.7679 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0151 ms 100.0% 
  triton_mm_55 0.0274 ms 55.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0422 ms 35.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0457 ms 33.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0505 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0536 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_67 0.0582 ms 25.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_69 0.0582 ms 25.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_66 0.0611 ms 24.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0753 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2263 seconds and 0.7503 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0148 ms 100.0% 
  triton_mm_55 0.0273 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0422 ms 35.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0457 ms 32.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0506 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0534 ms 27.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0582 ms 25.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0583 ms 25.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0610 ms 24.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0783 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2561 seconds and 0.7186 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0146 ms 100.0% 
  triton_mm_55 0.0277 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0421 ms 34.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0457 ms 31.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0506 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0534 ms 27.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0582 ms 25.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0583 ms 25.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0610 ms 23.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0764 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2974 seconds and 0.4450 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0139 ms 100.0% 
  triton_mm_55 0.0275 ms 50.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0420 ms 33.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0457 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0505 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0533 ms 26.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0581 ms 23.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0584 ms 23.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0609 ms 22.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0751 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2904 seconds and 0.6189 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0146 ms 100.0% 
  triton_mm_55 0.0275 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0423 ms 34.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0458 ms 31.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0506 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0535 ms 27.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0582 ms 25.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0583 ms 25.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0610 ms 23.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0750 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2678 seconds and 0.4503 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0146 ms 100.0% 
  triton_mm_55 0.0274 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0421 ms 34.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0457 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0504 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0532 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0581 ms 25.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0583 ms 25.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0609 ms 24.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0755 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2724 seconds and 0.7966 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0146 ms 100.0% 
  triton_mm_55 0.0279 ms 52.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0421 ms 34.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0456 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0506 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0534 ms 27.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0582 ms 25.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0584 ms 25.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0610 ms 23.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0761 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2722 seconds and 0.6742 seconds precompiling for 39 choices
[rank1]:W1024 11:33:32.951000 816 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f43715c3ae0>
[rank4]:W1024 11:33:32.959000 819 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eda859e7660>
[rank3]:W1024 11:33:32.962000 818 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5b48347810>
[rank7]:W1024 11:33:32.967000 822 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dbb9180f0>
[rank1]:W1024 11:33:32.983000 816 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f43715c3ab0>
[rank2]:W1024 11:33:32.988000 817 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ece7b9f3630>
[rank4]:W1024 11:33:32.990000 819 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eda859e7690>
[rank3]:W1024 11:33:32.993000 818 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5b48347d80>
[rank7]:W1024 11:33:32.999000 822 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dbbbef990>
[rank6]:W1024 11:33:33.011000 821 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f93d5183a20>
[rank2]:W1024 11:33:33.019000 817 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ece7b9f3750>
[rank6]:W1024 11:33:33.042000 821 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f93d5183b70>
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:33:33 DP1 TP1] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:33:33 DP2 TP2] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1024 11:33:33.154000 815 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fabda487ed0>
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:33:33 DP3 TP3] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:33:33 DP4 TP4] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:33:33 DP6 TP6] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:33:33 DP7 TP7] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1024 11:33:33.185000 815 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fabda4877b0>
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1024 11:33:33.291000 820 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb395abfab0>
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:33:33 DP0 TP0] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1024 11:33:33.323000 820 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb395abf750>
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:33:33 DP5 TP5] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1024 11:33:34.877000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:34.887000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:34.897000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:34.906000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:34.941000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:34.961000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:34.975000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:34.989000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:35.125000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:35.135000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:35.145000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:35.154000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:35.193000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:35.215000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:35.223000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:35.244000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:35.375000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:35.384000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:35.392000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:35.400000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:35.435000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:35.454000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:35.465000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:35.487000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:35.619000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:35.627000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:35.636000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:35.644000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:35.675000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:35.698000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:35.708000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:35.723000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:35.859000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:35.870000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:35.878000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:35.887000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:35.914000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:35.943000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:35.952000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:35.963000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:36.101000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:36.115000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:36.124000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:36.134000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:36.153000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:36.187000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:36.195000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:36.205000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:36.345000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:36.355000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:36.365000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:36.374000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:36.393000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:36.428000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:36.436000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:36.446000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:36.591000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:36.599000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:36.608000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:36.616000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:36.636000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:36.666000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:36.677000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:36.690000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:36.843000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:36.851000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:36.860000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:36.879000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:36.903000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:36.912000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:36.922000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:36.934000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:37.087000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:37.098000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:37.106000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:37.119000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:37.146000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:37.175000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:37.210000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:37.220000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:37.349000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:37.358000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:37.366000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:37.376000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:37.390000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:37.415000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:37.454000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:37.465000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:37.597000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:37.611000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:37.620000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:37.635000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:37.645000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:37.655000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:37.703000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:37.712000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:37.841000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:37.861000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:37.879000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:37.889000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:37.899000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:37.910000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:37.949000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:37.959000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:38.123000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:38.132000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:38.145000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:38.154000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:38.164000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:38.191000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:38.202000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:38.211000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:38.367000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:38.376000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:38.395000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:38.405000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:38.417000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:38.431000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:38.458000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:38.466000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:38.607000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:38.616000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:38.634000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:38.657000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:38.669000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:38.679000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:38.709000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:38.719000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:38.851000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:38.860000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:38.874000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:38.901000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:38.910000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:38.920000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:38.955000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:38.963000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:39.095000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:39.104000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:39.114000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:39.144000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:39.153000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:39.164000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:39.198000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:39.207000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:39.339000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:39.348000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:39.357000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:39.389000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:39.397000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:39.408000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:39.442000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:39.452000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:39.587000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:39.596000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:39.606000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:39.633000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:39.641000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:39.652000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:39.685000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:39.695000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:39.831000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:39.840000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:39.850000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:39.877000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:39.887000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:39.896000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:39.925000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:39.935000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:40.071000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:40.080000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:40.090000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:40.121000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:40.131000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:40.140000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:40.165000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:40.177000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:40.323000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:40.335000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:40.365000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:40.375000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:40.383000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:40.394000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:40.405000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:40.417000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:40.568000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:40.579000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:40.613000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:40.624000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:40.631000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:40.650000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:40.699000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:40.717000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:40.850000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:40.859000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:40.868000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:40.878000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:40.886000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:40.895000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:40.946000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:40.958000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:41.107000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:41.123000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:41.130000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:41.139000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:41.149000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:41.159000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:41.191000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:41.201000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:41.367000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:41.377000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:41.385000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:41.393000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:41.405000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:41.414000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:41.435000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:41.445000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:41.611000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:41.621000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:41.629000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:41.638000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:41.649000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:41.659000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:41.675000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:41.686000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:41.851000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:41.891000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:41.903000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:41.915000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:41.929000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:41.938000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:41.947000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:41.958000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:42.094000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:42.133000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:42.146000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:42.154000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:42.171000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:42.182000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:42.193000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:42.205000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:42.337000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:42.378000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:42.389000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:42.398000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:42.411000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:42.422000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:42.435000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:42.447000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:42.581000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:42.622000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:42.634000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:42.642000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:42.653000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:42.663000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:42.675000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:42.687000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:42.825000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:42.865000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:42.878000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:42.886000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:42.897000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:42.907000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:42.919000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:42.932000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:43.069000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:43.109000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:43.121000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:43.130000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:43.147000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:43.158000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:43.170000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:43.182000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:43.313000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:43.354000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:43.362000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:43.373000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:43.387000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:43.398000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:43.410000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:43.423000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:43.557000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:43.597000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:43.606000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:43.618000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:43.628000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:43.638000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:43.655000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:43.671000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:43.805000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:43.841000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:43.849000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:43.862000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:43.872000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:43.883000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:43.895000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:43.911000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:44.090000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:44.097000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:44.110000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:44.119000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:44.128000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:44.139000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:44.151000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:44.163000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:44.338000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:44.346000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:44.366000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:44.382000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:44.395000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:44.407000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:44.422000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:44.435000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:44.581000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:44.589000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:44.610000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:44.626000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:44.639000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:44.651000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:44.671000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:44.681000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:44.825000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:44.833000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:44.853000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:44.870000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:44.882000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:44.893000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:44.918000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:44.929000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:45.082000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:45.102000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:45.115000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:45.126000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:45.138000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:45.147000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:45.166000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:45.177000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:45.350000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:45.359000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:45.370000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:45.381000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:45.394000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:45.402000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:45.415000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:45.424000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:45.598000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:45.614000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:45.625000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:45.637000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:45.646000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:45.654000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:45.667000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:45.676000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:45.847000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:45.891000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:45.901000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:45.913000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:45.924000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:45.941000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:45.951000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:45.961000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:46.103000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:46.135000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:46.144000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:46.157000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:46.168000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:46.189000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:46.198000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:46.209000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:46.351000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:46.379000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:46.388000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:46.409000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:46.419000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:46.446000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:46.454000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:46.464000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:46.603000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:46.623000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:46.632000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:46.653000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:46.664000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:46.693000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:46.701000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:46.711000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:46.847000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:46.867000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:46.876000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:46.897000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:46.908000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:46.941000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:46.949000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:46.959000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:47.099000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:47.111000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:47.120000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:47.142000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:47.151000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:47.189000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:47.197000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:47.207000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:47.343000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:47.355000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:47.364000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:47.389000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:47.399000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:47.432000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:47.442000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:47.453000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:47.591000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:47.600000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:47.610000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:47.633000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:47.643000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:47.680000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:47.689000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:47.699000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:47.846000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:47.857000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:47.866000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:47.877000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:47.888000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:47.928000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:47.937000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:47.949000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:48.091000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:48.103000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:48.112000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:48.121000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:48.131000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:48.176000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:48.185000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:48.195000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:48.339000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:48.358000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:48.378000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:48.387000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:48.420000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:48.428000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:48.437000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:48.447000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:48.587000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:48.607000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:48.667000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:48.677000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:48.685000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:48.696000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:48.707000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:48.716000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:48.854000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:48.862000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:48.914000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:48.926000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:48.937000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:48.948000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:48.960000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:48.970000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:33:49.741000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:33:49.751000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:33:49.773000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:33:49.789000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:33:49.795000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:33:49.805000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:33:49.813000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:33:49.843000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1793 ms 100.0% 
  triton_mm_127 0.2442 ms 73.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2527 ms 70.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2584 ms 69.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2624 ms 68.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_107 0.2640 ms 67.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_112 0.2645 ms 67.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2659 ms 67.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2713 ms 66.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_114 0.2925 ms 61.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.9100 seconds and 0.5457 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1824 ms 100.0% 
  triton_mm_127 0.2488 ms 73.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2553 ms 71.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2619 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_112 0.2679 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_124 0.2684 ms 67.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2708 ms 67.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2770 ms 65.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2824 ms 64.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_105 0.2999 ms 60.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9308 seconds and 0.6007 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1812 ms 100.0% 
  triton_mm_127 0.2462 ms 73.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2536 ms 71.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2587 ms 70.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2620 ms 69.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_112 0.2629 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_124 0.2643 ms 68.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2667 ms 68.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2701 ms 67.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_105 0.2939 ms 61.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.8919 seconds and 0.6828 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1810 ms 100.0% 
  triton_mm_127 0.2466 ms 73.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2551 ms 71.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2604 ms 69.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2636 ms 68.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2659 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2679 ms 67.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2692 ms 67.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2766 ms 65.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_121 0.2959 ms 61.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.8746 seconds and 0.4790 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1886 ms 100.0% 
  triton_mm_127 0.2432 ms 77.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2524 ms 74.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2557 ms 73.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_112 0.2607 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_124 0.2626 ms 71.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2640 ms 71.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2754 ms 68.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2835 ms 66.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_105 0.2938 ms 64.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.8528 seconds and 0.4647 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1768 ms 100.0% 
  triton_mm_127 0.2485 ms 71.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2565 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2626 ms 67.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2654 ms 66.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2670 ms 66.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2709 ms 65.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2729 ms 64.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2831 ms 62.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_105 0.2994 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9267 seconds and 0.5165 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1783 ms 100.0% 
  triton_mm_127 0.2429 ms 73.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2539 ms 70.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_124 0.2596 ms 68.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_113 0.2600 ms 68.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_112 0.2653 ms 67.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2655 ms 67.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2709 ms 65.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2801 ms 63.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_121 0.2928 ms 60.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9200 seconds and 0.5567 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1804 ms 100.0% 
  triton_mm_127 0.2471 ms 73.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2556 ms 70.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2616 ms 69.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2652 ms 68.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2678 ms 67.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2689 ms 67.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2737 ms 65.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2839 ms 63.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_105 0.2991 ms 60.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9433 seconds and 0.3702 seconds precompiling for 39 choices
Capturing batches (bs=32 avail_mem=49.85 GB):  87%| | 45/52 [01:49<02:49, 24.18s/it]Capturing batches (bs=24 avail_mem=49.22 GB):  87%| | 45/52 [01:49<02:49, 24.18s/it][rank0]:W1024 11:34:06.364000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:06.385000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:06.402000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:06.414000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:06.420000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:06.429000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:06.431000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:06.442000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:06.452000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:06.470000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:06.482000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:06.487000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:06.496000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:06.503000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:06.510000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:06.523000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:06.541000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:06.554000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:06.558000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:06.567000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:06.582000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:06.852000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:06.919000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:06.988000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:06.991000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:07.009000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:07.029000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:07.041000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:07.049000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:07.058000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:07.068000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:07.071000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:07.088000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:07.109000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:07.123000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:07.129000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:07.136000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:07.139000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:07.146000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:07.156000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:07.177000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:07.191000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:07.196000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:07.203000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:07.207000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:07.214000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:07.223000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:07.245000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:07.258000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:07.264000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:07.270000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:07.282000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:07.474000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:07.552000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:07.619000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:07.683000 815 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank4]:W1024 11:34:07.684000 819 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank7]:W1024 11:34:07.686000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:07.719000 816 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank5]:W1024 11:34:07.731000 820 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank2]:W1024 11:34:07.732000 817 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank3]:W1024 11:34:07.735000 818 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank6]:W1024 11:34:07.746000 821 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank7]:W1024 11:34:08.199000 822 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0102 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_148 0.0102 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_151 0.0102 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_153 0.0102 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_147 0.0103 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0103 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0105 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_149 0.0105 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0106 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2724 seconds and 0.4927 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_144 0.0098 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_145 0.0098 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_148 0.0101 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_150 0.0101 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_151 0.0101 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_153 0.0101 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_147 0.0102 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_143 0.0103 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_149 0.0103 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0105 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2890 seconds and 0.4970 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_144 0.0098 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_145 0.0098 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_147 0.0099 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_148 0.0099 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_151 0.0099 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0100 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_153 0.0100 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_143 0.0101 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_149 0.0101 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0103 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3010 seconds and 0.2721 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0100 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_147 0.0102 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_148 0.0103 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_151 0.0103 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0103 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0104 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_153 0.0104 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_149 0.0105 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0108 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3098 seconds and 0.4484 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_151 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0101 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_147 0.0101 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_148 0.0101 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_153 0.0101 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0102 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0104 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_149 0.0104 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0107 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2834 seconds and 0.3606 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0101 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_151 0.0101 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_147 0.0102 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_148 0.0103 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_150 0.0103 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_153 0.0103 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_143 0.0103 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_149 0.0104 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0107 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2813 seconds and 0.3960 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_147 0.0103 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_143 0.0103 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_151 0.0103 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0104 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_153 0.0104 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_148 0.0105 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_149 0.0105 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0109 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2723 seconds and 0.3931 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0098 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0101 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_151 0.0101 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_153 0.0101 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0103 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_148 0.0103 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_147 0.0103 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_149 0.0104 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_143 0.0105 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0107 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3141 seconds and 0.3870 seconds precompiling for 27 choices
[rank2]:W1024 11:34:18.532000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:18.894000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:19.032000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:19.170000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:19.185000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:19.255000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:19.403000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:19.507000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:19.672000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:19.690000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:19.765000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:19.807000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:20.007000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:20.307000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:20.330000 817 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank7]:W1024 11:34:20.501000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:20.696000 820 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank7]:W1024 11:34:21.013000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:21.140000 815 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank1]:W1024 11:34:21.156000 816 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank4]:W1024 11:34:21.311000 819 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank3]:W1024 11:34:21.416000 818 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank6]:W1024 11:34:21.611000 821 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank7]:W1024 11:34:22.474000 822 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_167 0.0096 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_169 0.0096 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_176 0.0099 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_174 0.0101 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0103 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_175 0.0103 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0104 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_177 0.0105 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0108 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_173 0.0110 ms 87.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2545 seconds and 0.5238 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_167 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_169 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_165 0.0101 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_176 0.0104 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_174 0.0104 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_159 0.0106 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_179 0.0107 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_175 0.0108 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_173 0.0108 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_177 0.0109 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3011 seconds and 0.5347 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_169 0.0097 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_167 0.0098 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_176 0.0100 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0101 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_174 0.0102 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_179 0.0104 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_175 0.0105 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_177 0.0105 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0106 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_173 0.0109 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2079 seconds and 0.5569 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_169 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_167 0.0101 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_174 0.0103 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0104 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_176 0.0104 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_175 0.0107 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0107 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_177 0.0109 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0111 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_173 0.0113 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2645 seconds and 0.5523 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_169 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_167 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_176 0.0102 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_174 0.0103 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0104 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_175 0.0107 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0107 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_177 0.0107 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0109 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0112 ms 88.5% 
SingleProcess AUTOTUNE benchmarking takes 5.2906 seconds and 0.5348 seconds precompiling for 27 choices
[rank2]:W1024 11:34:27.338000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_169 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_167 0.0100 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_174 0.0102 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_176 0.0103 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0104 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_175 0.0107 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_177 0.0109 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0109 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_159 0.0111 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_173 0.0112 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3285 seconds and 0.5293 seconds precompiling for 27 choices
[rank2]:W1024 11:34:27.415000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_167 0.0097 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_169 0.0098 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_165 0.0099 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_174 0.0100 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_176 0.0101 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_175 0.0103 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0104 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_177 0.0105 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0107 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_173 0.0108 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2407 seconds and 0.5233 seconds precompiling for 27 choices
[rank2]:W1024 11:34:27.514000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:27.765000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:27.841000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:27.940000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:28.161000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:28.237000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:28.336000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_167 0.0097 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_169 0.0097 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_174 0.0101 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_176 0.0101 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0102 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0103 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_177 0.0105 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_175 0.0106 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0107 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_173 0.0110 ms 87.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3263 seconds and 0.5365 seconds precompiling for 27 choices
[rank1]:W1024 11:34:28.501000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:28.586000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:28.595000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:28.663000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:28.670000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:28.684000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:28.739000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:28.769000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:28.837000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:28.839000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:28.912000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:29.010000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:29.854000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:29.930000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:30.031000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:32.059000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:32.135000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:32.163000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:32.236000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:32.238000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:32.349000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:32.483000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:32.559000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:32.660000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:33.242000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:33.302000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:33.316000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:33.376000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:33.377000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:33.413000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:33.452000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:33.476000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:33.551000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:33.554000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:33.629000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:33.727000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:34.583000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:34.659000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:34.759000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:35.364000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:35.371000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:35.395000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:35.398000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:35.447000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:35.471000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:35.473000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:35.476000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:35.500000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:35.518000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:35.548000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:35.552000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:35.553000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:35.577000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:35.580000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:35.597000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:35.652000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:35.655000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:35.678000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:35.690000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:35.702000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:35.957000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:36.134000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:36.234000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:36.795000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:36.808000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:36.822000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:36.831000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:36.841000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:36.853000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:36.863000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:36.878000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:37.043000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:37.056000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:37.074000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:37.084000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:37.094000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:37.105000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:37.115000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:37.127000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:37.291000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:37.304000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:37.326000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:37.335000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:37.345000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:37.357000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:37.367000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:37.378000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:38.924000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:38.935000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:38.970000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:38.978000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:38.991000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:39.000000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:39.003000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:39.010000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:39.010000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:39.045000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:39.048000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:39.054000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:39.059000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:39.067000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:39.078000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:39.085000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:39.093000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:39.103000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:39.115000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:39.126000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:39.134000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:39.258000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:39.334000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:39.382000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0132 ms 100.0% 
  triton_mm_183 0.0276 ms 47.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0409 ms 32.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0457 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0497 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0535 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0582 ms 22.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0598 ms 22.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 21.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0755 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1822 seconds and 0.7297 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0129 ms 100.0% 
  triton_mm_183 0.0276 ms 46.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0409 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0457 ms 28.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0497 ms 26.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0535 ms 24.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0583 ms 22.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0597 ms 21.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0755 ms 17.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2165 seconds and 0.7194 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0133 ms 100.0% 
  triton_mm_183 0.0275 ms 48.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0408 ms 32.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0454 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0496 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0533 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0582 ms 22.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0598 ms 22.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0612 ms 21.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0752 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2013 seconds and 0.7310 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0131 ms 100.0% 
  triton_mm_183 0.0275 ms 47.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0408 ms 32.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0455 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0497 ms 26.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0534 ms 24.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0583 ms 22.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0598 ms 21.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0612 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0753 ms 17.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2419 seconds and 0.5045 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0134 ms 100.0% 
  triton_mm_183 0.0276 ms 48.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0408 ms 32.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0456 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0497 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0534 ms 25.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0582 ms 23.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0598 ms 22.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0612 ms 21.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0755 ms 17.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1938 seconds and 0.5758 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0125 ms 100.0% 
  triton_mm_183 0.0274 ms 45.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0409 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0455 ms 27.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0496 ms 25.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0535 ms 23.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0582 ms 21.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0596 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0612 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0752 ms 16.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2092 seconds and 0.6092 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0131 ms 100.0% 
  triton_mm_183 0.0275 ms 47.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0408 ms 32.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0455 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0498 ms 26.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0534 ms 24.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0582 ms 22.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0597 ms 22.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 21.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0754 ms 17.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2485 seconds and 0.5629 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0131 ms 100.0% 
  triton_mm_183 0.0275 ms 47.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0407 ms 32.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0454 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0497 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0532 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0582 ms 22.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0598 ms 22.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0612 ms 21.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0753 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2206 seconds and 0.4567 seconds precompiling for 39 choices
[rank1]:W1024 11:34:49.758000 816 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f43715c3ae0>
[rank1]:W1024 11:34:49.781000 816 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f43715c3ab0>
[rank0]:W1024 11:34:49.799000 815 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fabda487ed0>
[rank6]:W1024 11:34:49.821000 821 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f93d5183a20>
[rank0]:W1024 11:34:49.822000 815 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fabda4877b0>
[rank6]:W1024 11:34:49.844000 821 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f93d5183b70>
[rank5]:W1024 11:34:49.845000 820 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb395abfab0>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:34:49 DP1 TP1] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1024 11:34:49.867000 820 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb395abf750>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:34:49 DP0 TP0] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1024 11:34:49.899000 822 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dbb9180f0>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:34:49 DP6 TP6] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1024 11:34:49.915000 817 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ece7b9f3630>
[rank7]:W1024 11:34:49.922000 822 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dbbbef990>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:34:49 DP5 TP5] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1024 11:34:49.938000 817 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ece7b9f3750>
[rank3]:W1024 11:34:49.943000 818 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5b48347810>
[rank3]:W1024 11:34:49.966000 818 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5b48347d80>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:34:49 DP7 TP7] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:34:50 DP2 TP2] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:34:50 DP3 TP3] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1024 11:34:50.177000 819 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eda859e7660>
[rank4]:W1024 11:34:50.200000 819 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eda859e7690>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:34:50 DP4 TP4] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1024 11:34:51.410000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:51.419000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:51.430000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:51.440000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:51.449000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:51.458000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:51.468000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:51.522000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:51.667000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:51.681000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:51.704000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:51.713000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:51.724000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:51.774000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:51.804000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:51.839000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:51.980000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:51.990000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:51.998000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:52.008000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:52.018000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:52.052000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:52.091000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:52.148000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:52.287000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:52.297000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:52.307000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:52.317000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:52.328000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:52.342000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:52.366000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:52.398000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:52.538000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:52.548000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:52.559000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:52.569000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:52.581000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:52.591000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:52.614000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:52.646000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:52.786000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:52.796000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:52.808000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:52.818000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:52.830000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:52.840000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:52.862000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:52.894000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:53.034000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:53.044000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:53.056000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:53.066000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:53.078000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:53.090000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:53.110000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:53.142000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:53.282000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:53.292000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:53.304000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:53.315000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:53.326000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:53.338000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:53.358000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:53.390000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:53.530000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:53.540000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:53.552000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:53.563000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:53.574000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:53.586000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:53.606000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:53.638000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:53.780000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:53.788000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:53.798000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:53.810000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:53.818000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:53.836000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:53.855000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:53.888000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:54.027000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:54.038000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:54.048000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:54.063000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:54.073000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:54.085000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:54.099000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:54.131000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:54.275000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:54.298000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:54.306000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:54.317000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:54.325000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:54.337000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:54.349000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:54.376000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:54.519000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:54.550000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:54.558000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:54.567000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:54.577000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:54.588000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:54.599000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:54.620000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:54.766000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:54.804000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:54.815000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:54.826000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:54.837000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:54.847000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:54.857000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:54.867000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:55.014000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:55.056000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:55.067000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:55.077000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:55.088000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:55.100000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:55.108000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:55.119000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:55.262000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:55.308000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:55.319000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:55.330000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:55.340000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:55.352000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:55.360000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:55.371000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:55.510000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:55.556000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:55.568000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:55.578000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:55.589000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:55.603000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:55.612000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:55.622000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:55.762000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:55.818000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:55.827000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:55.837000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:55.847000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:55.863000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:55.874000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:55.886000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:56.023000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:56.078000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:56.087000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:56.096000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:56.106000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:56.117000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:56.128000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:56.140000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:56.279000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:56.330000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:56.339000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:56.347000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:56.357000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:56.368000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:56.379000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:56.391000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:56.531000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:56.582000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:56.591000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:56.599000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:56.609000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:56.620000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:56.632000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:56.644000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:56.783000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:56.837000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:56.848000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:56.858000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:56.870000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:56.880000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:56.890000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:56.901000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:57.042000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:57.088000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:57.100000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:57.111000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:57.121000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:57.133000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:57.142000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:57.152000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:57.294000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:57.340000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:57.351000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:57.362000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:57.372000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:57.384000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:57.395000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:57.404000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:57.546000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:57.604000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:57.615000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:57.626000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:57.637000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:57.647000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:57.658000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:57.667000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:57.886000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:57.896000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:57.907000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:57.917000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:57.927000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:57.937000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:57.947000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:58.001000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:58.153000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:58.170000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:58.190000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:58.203000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:58.261000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:58.275000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:58.321000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:58.344000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:58.486000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:58.495000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:58.520000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:58.531000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:58.541000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:58.563000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:58.576000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:58.599000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:58.776000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:58.785000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:58.795000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:58.805000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:58.819000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:58.830000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:58.851000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:58.887000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:59.072000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:59.081000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:59.092000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:59.101000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:59.111000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:59.122000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:59.142000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:59.179000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:59.324000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:59.338000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:59.350000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:59.359000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:59.369000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:59.380000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:59.398000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:59.430000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:59.588000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:59.597000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:59.612000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:59.621000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:59.632000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:59.642000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:59.655000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:59.682000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:34:59.840000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:34:59.850000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:34:59.864000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:34:59.875000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:34:59.885000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:34:59.896000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:34:59.910000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:34:59.934000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:00.092000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:00.106000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:00.117000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:00.130000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:00.140000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:00.151000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:00.162000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:00.190000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:00.344000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:00.362000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:00.372000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:00.386000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:00.397000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:00.407000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:00.418000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:00.442000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:00.594000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:00.616000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:00.624000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:00.640000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:00.649000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:00.658000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:00.672000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:00.696000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:00.846000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:00.868000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:00.878000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:00.892000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:00.902000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:00.913000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:00.925000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:00.948000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:01.098000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:01.119000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:01.134000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:01.145000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:01.154000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:01.166000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:01.177000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:01.196000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:01.350000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:01.371000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:01.390000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:01.400000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:01.409000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:01.419000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:01.430000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:01.444000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:01.604000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:01.622000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:01.648000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:01.657000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:01.668000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:01.679000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:01.689000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:01.700000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:01.856000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:01.874000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:01.900000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:01.911000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:01.921000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:01.931000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:01.943000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:01.952000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:02.120000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:02.130000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:02.164000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:02.173000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:02.184000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:02.194000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:02.207000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:02.215000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:02.372000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:02.382000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:02.416000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:02.427000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:02.437000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:02.448000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:02.459000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:02.469000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:02.624000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:02.634000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:02.668000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:02.679000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:02.689000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:02.699000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:02.711000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:02.722000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:02.884000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:02.892000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:02.932000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:02.940000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:02.952000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:02.963000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:02.974000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:02.983000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:03.136000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:03.146000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:03.184000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:03.194000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:03.204000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:03.215000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:03.228000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:03.236000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:03.388000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:03.404000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:03.434000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:03.452000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:03.461000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:03.470000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:03.485000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:03.496000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:03.642000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:03.659000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:03.690000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:03.708000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:03.717000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:03.726000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:03.738000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:03.751000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:03.898000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:03.911000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:03.946000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:03.960000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:03.970000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:03.981000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:03.992000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:04.004000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:04.154000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:04.164000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:04.202000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:04.212000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:04.222000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:04.237000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:04.249000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:04.259000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:04.410000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:04.419000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:04.458000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:04.468000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:04.477000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:04.493000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:04.505000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:04.516000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:04.669000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:04.680000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:04.720000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:04.734000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:04.749000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:04.760000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:04.772000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:04.874000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:05.019000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:05.027000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:05.035000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:05.045000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:05.056000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:05.065000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:05.075000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:05.131000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:05.377000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:05.386000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:05.396000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:05.407000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:05.416000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:05.428000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:05.486000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:05.497000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:05.704000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:05.713000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:05.723000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:05.733000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:05.745000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:05.755000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:05.794000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:05.817000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:06.024000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:06.031000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:06.041000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:06.049000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:06.058000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:06.068000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:06.080000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:06.132000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:06.337000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:06.345000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:06.354000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:06.363000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:06.373000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:06.383000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:06.395000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:06.452000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:07.147000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:07.158000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:07.168000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:07.178000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:07.189000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:07.198000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:07.208000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:07.216000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1458 ms 100.0% 
  triton_mm_235 0.2158 ms 67.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2163 ms 67.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2389 ms 61.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2439 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_227 0.2449 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_241 0.2475 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2494 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_254 0.2510 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2613 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.7495 seconds and 0.6211 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1486 ms 100.0% 
  triton_mm_235 0.2249 ms 66.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2252 ms 66.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2426 ms 61.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2469 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2500 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2533 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_254 0.2550 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_227 0.2557 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_252 0.2640 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8807 seconds and 0.7531 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1485 ms 100.0% 
  triton_mm_235 0.2236 ms 66.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2240 ms 66.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2427 ms 61.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2471 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2497 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2519 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_254 0.2558 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_227 0.2561 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_252 0.2648 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8712 seconds and 0.6527 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1468 ms 100.0% 
  triton_mm_234 0.2161 ms 68.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_235 0.2172 ms 67.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_233 0.2393 ms 61.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_227 0.2433 ms 60.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_255 0.2449 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2473 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2505 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_254 0.2523 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2620 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8869 seconds and 0.7988 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1472 ms 100.0% 
  triton_mm_235 0.2207 ms 66.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2221 ms 66.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2410 ms 61.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2456 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2491 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2513 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_227 0.2514 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_254 0.2537 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2624 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8929 seconds and 0.5326 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1456 ms 100.0% 
  triton_mm_235 0.2207 ms 66.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2224 ms 65.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2406 ms 60.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2440 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2467 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2494 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_227 0.2503 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_254 0.2508 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2615 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8732 seconds and 0.4381 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1487 ms 100.0% 
  triton_mm_235 0.2226 ms 66.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2232 ms 66.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2426 ms 61.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2480 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2503 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2519 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_227 0.2526 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_254 0.2553 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2637 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.9147 seconds and 0.4818 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1518 ms 100.0% 
  triton_mm_235 0.2252 ms 67.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2274 ms 66.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2366 ms 64.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2431 ms 62.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2450 ms 62.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2485 ms 61.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_254 0.2507 ms 60.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_227 0.2560 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_252 0.2620 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.9129 seconds and 0.8759 seconds precompiling for 39 choices
Capturing batches (bs=24 avail_mem=49.22 GB):  88%| | 46/52 [03:06<04:01, 40.22s/it]Capturing batches (bs=16 avail_mem=48.62 GB):  88%| | 46/52 [03:06<04:01, 40.22s/it][rank6]:W1024 11:35:23.989000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:24.036000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:24.049000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:24.057000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:24.061000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:24.073000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:24.085000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:24.100000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:24.103000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:24.117000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:24.128000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:24.129000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:24.142000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:24.153000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:24.167000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:24.175000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:24.189000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:24.200000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:24.214000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:24.225000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:24.239000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:24.494000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:24.562000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:24.617000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:24.634000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:24.658000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:24.678000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:24.687000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:24.698000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:24.702000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:24.711000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:24.727000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:24.737000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:24.757000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:24.766000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:24.768000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:24.783000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:24.794000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:24.805000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:24.805000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:24.825000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:24.834000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:24.836000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:24.852000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:24.863000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:24.872000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:24.873000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:24.893000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:24.901000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:24.920000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:24.931000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:24.940000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:25.128000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:25.208000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:25.276000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:25.292000 821 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank3]:W1024 11:35:25.322000 818 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank5]:W1024 11:35:25.345000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:25.347000 815 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank2]:W1024 11:35:25.354000 817 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank7]:W1024 11:35:25.373000 822 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank4]:W1024 11:35:25.384000 819 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank1]:W1024 11:35:25.394000 816 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank5]:W1024 11:35:25.881000 820 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_271 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_270 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_276 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_279 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_277 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_272 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_273 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_274 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_278 0.0091 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_275 0.0092 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8561 seconds and 0.1908 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_276 0.0090 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_277 0.0090 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_279 0.0090 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_278 0.0090 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_273 0.0090 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_272 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_274 0.0091 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_275 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8152 seconds and 0.1389 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0091 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0091 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_272 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_273 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_278 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_276 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_274 0.0094 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_275 0.0094 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_279 0.0094 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_277 0.0095 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8345 seconds and 0.2143 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0088 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_273 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_274 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_276 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_278 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_277 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_272 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_275 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_279 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8193 seconds and 0.1995 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0091 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0092 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_279 0.0092 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_273 0.0093 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_278 0.0093 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_272 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_276 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_277 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_274 0.0094 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_275 0.0094 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7880 seconds and 0.1941 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_271 0.0091 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_270 0.0091 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_279 0.0092 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_274 0.0092 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_278 0.0092 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_272 0.0092 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_273 0.0092 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_275 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_277 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_276 0.0093 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8435 seconds and 0.1447 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0090 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0090 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_272 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_274 0.0093 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_273 0.0093 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_275 0.0093 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_276 0.0093 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_277 0.0093 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_278 0.0093 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_279 0.0094 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8967 seconds and 0.1631 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0090 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0091 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_272 0.0092 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_276 0.0092 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_278 0.0092 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_273 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_274 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_277 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_279 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_268 0.0093 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9251 seconds and 0.1422 seconds precompiling for 25 choices
[rank1]:W1024 11:35:35.789000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:36.068000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:36.175000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:36.189000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:36.293000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:36.411000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:36.460000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:36.576000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:36.620000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:36.675000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:36.684000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:36.912000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:36.967000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:37.120000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:37.307000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:37.754000 816 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank2]:W1024 11:35:37.831000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:37.879000 815 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank6]:W1024 11:35:38.120000 821 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank4]:W1024 11:35:38.154000 819 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank3]:W1024 11:35:38.238000 818 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank7]:W1024 11:35:38.395000 822 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank5]:W1024 11:35:38.592000 820 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank2]:W1024 11:35:39.312000 817 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_291 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_290 0.0086 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_292 0.0093 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0093 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_302 0.0094 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_303 0.0095 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_284 0.0099 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0099 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_298 0.0099 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_285 0.0100 ms 85.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7908 seconds and 0.4036 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_291 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_290 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_292 0.0091 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0091 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_302 0.0096 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_303 0.0097 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_298 0.0099 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0099 ms 87.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_285 0.0101 ms 86.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0101 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9534 seconds and 0.3972 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_290 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_291 0.0085 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0092 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0092 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0093 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_285 0.0093 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_302 0.0095 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_303 0.0096 ms 87.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_298 0.0096 ms 87.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0097 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.0564 seconds and 0.4051 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_290 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_291 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_293 0.0092 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0093 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_284 0.0095 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_302 0.0096 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_303 0.0097 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_299 0.0097 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_285 0.0099 ms 87.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_298 0.0099 ms 87.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.0427 seconds and 0.4077 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_291 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_290 0.0088 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_292 0.0094 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0094 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_302 0.0097 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_303 0.0097 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_298 0.0099 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0099 ms 87.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_285 0.0101 ms 86.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_301 0.0101 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.0389 seconds and 0.3982 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_291 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_290 0.0087 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0091 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0092 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_303 0.0095 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_302 0.0095 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_298 0.0096 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0098 ms 88.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0100 ms 86.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_285 0.0100 ms 86.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.1234 seconds and 0.4014 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_290 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_291 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0092 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0093 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_302 0.0093 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_303 0.0095 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_299 0.0095 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_298 0.0096 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_285 0.0097 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0099 ms 87.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.0769 seconds and 0.3949 seconds precompiling for 25 choices
[rank0]:W1024 11:35:44.493000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:44.571000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:44.624000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_291 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_290 0.0085 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_292 0.0091 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0091 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_303 0.0094 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_302 0.0095 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_298 0.0097 ms 87.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0097 ms 86.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_285 0.0099 ms 85.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_300 0.0099 ms 85.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8428 seconds and 0.4094 seconds precompiling for 25 choices
[rank0]:W1024 11:35:44.673000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:44.701000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:44.781000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:44.813000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:44.857000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:44.893000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:44.956000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:44.971000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:45.072000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:45.133000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:45.210000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:45.295000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:45.310000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:45.371000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:45.446000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:45.471000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:45.523000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:45.625000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:46.082000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:46.158000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:46.452000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:49.399000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:49.476000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:49.496000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:49.546000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:49.572000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:49.577000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:49.622000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:49.624000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:35:49 DP0 TP0] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank1]:W1024 11:35:49.674000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:49.701000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:49.722000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:35:49 DP1 TP1] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:35:49 DP6 TP6] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank3]:W1024 11:35:49.801000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:35:49 DP3 TP3] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank4]:W1024 11:35:49.907000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:49.984000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:49.999000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:50.038000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:50.075000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:50.086000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:50.114000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:35:50 DP4 TP4] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank5]:W1024 11:35:50.175000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:50.215000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:35:50 DP5 TP5] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:35:50 DP7 TP7] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank2]:W1024 11:35:51.167000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:51.245000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:51.345000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:35:51 DP2 TP2] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank7]:W1024 11:35:51.932000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:51.953000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:51.967000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:51.980000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:51.988000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:52.009000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:52.029000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:52.044000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:52.057000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:52.063000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:52.096000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:52.110000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:52.140000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:52.155000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:52.158000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:52.163000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:35:52 DP7 TP7] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank6]:W1024 11:35:52.171000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:35:52 DP0 TP0] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:35:52 DP3 TP3] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:35:52 DP4 TP4] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:35:52 DP5 TP5] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank6]:W1024 11:35:52.270000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:35:52 DP6 TP6] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank1]:W1024 11:35:52.413000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:52.489000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:52.589000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:35:52 DP1 TP1] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank2]:W1024 11:35:52.675000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:52.751000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:52.850000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:35:52 DP2 TP2] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank7]:W1024 11:35:53.412000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:53.420000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:53.431000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:53.441000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:53.453000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:53.463000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:53.478000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:53.498000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:53.668000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:53.686000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:53.696000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:53.708000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:53.716000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:53.727000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:53.738000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:53.754000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:53.924000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:53.948000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:53.957000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:53.967000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:53.982000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:53.993000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:54.002000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:54.018000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:35:54 DP7 TP7] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:35:54 DP4 TP4] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:35:54 DP1 TP1] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:35:54 DP5 TP5] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:35:54 DP3 TP3] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:35:54 DP6 TP6] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:35:54 DP0 TP0] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:35:54 DP2 TP2] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1024 11:35:55.549000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:55.580000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:55.592000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:55.617000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:55.626000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:55.626000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:55.636000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:55.647000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:55.657000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:55.669000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:35:55.693000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:55.703000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:35:55.703000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:55.712000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:35:55.723000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:35:55.734000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:35:55.746000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 11:35:55 DP7 TP7] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank3]:W1024 11:35:55.769000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:35:55.780000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:35:55.788000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 11:35:55 DP4 TP4] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank2]:W1024 11:35:55.800000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 11:35:55 DP1 TP1] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 11:35:55 DP3 TP3] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 11:35:55 DP6 TP6] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 11:35:55 DP0 TP0] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 11:35:55 DP2 TP2] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank5]:W1024 11:35:56.058000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:56.134000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:35:56.222000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-24 11:35:56 DP5 TP5] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0124 ms 100.0% 
  triton_mm_307 0.0270 ms 45.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0419 ms 29.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0453 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0495 ms 25.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0530 ms 23.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0582 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0599 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0612 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0742 ms 16.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1840 seconds and 0.6400 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0123 ms 100.0% 
  triton_mm_307 0.0270 ms 45.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0418 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0452 ms 27.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0495 ms 24.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0532 ms 23.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0582 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0599 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0612 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0742 ms 16.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2150 seconds and 0.7018 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0126 ms 100.0% 
  triton_mm_307 0.0272 ms 46.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0420 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0454 ms 27.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0495 ms 25.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0533 ms 23.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0582 ms 21.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0600 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0612 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0743 ms 16.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2159 seconds and 0.6829 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0123 ms 100.0% 
  triton_mm_307 0.0270 ms 45.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0418 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0452 ms 27.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0493 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0530 ms 23.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0580 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0598 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0612 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0742 ms 16.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2418 seconds and 0.6569 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0120 ms 100.0% 
  triton_mm_307 0.0270 ms 44.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0421 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0454 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0496 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0534 ms 22.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0584 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0601 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0613 ms 19.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0743 ms 16.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1989 seconds and 0.4268 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0124 ms 100.0% 
  triton_mm_307 0.0272 ms 45.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0419 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0454 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0497 ms 25.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0532 ms 23.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0583 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0601 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0612 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0744 ms 16.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2484 seconds and 0.3908 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0125 ms 100.0% 
  triton_mm_307 0.0271 ms 46.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0422 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0455 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0496 ms 25.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0534 ms 23.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0582 ms 21.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0600 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0612 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0744 ms 16.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2002 seconds and 0.4499 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0126 ms 100.0% 
  triton_mm_307 0.0271 ms 46.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0421 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0454 ms 27.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0496 ms 25.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0533 ms 23.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0582 ms 21.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0600 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0613 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0742 ms 17.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1123 seconds and 0.4735 seconds precompiling for 39 choices
[rank7]:W1024 11:36:06.264000 822 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dbb9180f0>
[rank7]:W1024 11:36:06.287000 822 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dbbbef990>
[rank6]:W1024 11:36:06.348000 821 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f93d5183a20>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:36:06 DP7 TP7] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1024 11:36:06.361000 819 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eda859e7660>
[rank6]:W1024 11:36:06.371000 821 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f93d5183b70>
[rank4]:W1024 11:36:06.384000 819 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eda859e7690>
[rank0]:W1024 11:36:06.397000 815 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fabda487ed0>
[rank0]:W1024 11:36:06.420000 815 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fabda4877b0>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:36:06 DP6 TP6] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1024 11:36:06.445000 816 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f43715c3ae0>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:36:06 DP4 TP4] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1024 11:36:06.471000 816 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f43715c3ab0>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:36:06 DP0 TP0] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1024 11:36:06.531000 817 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ece7b9f3630>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:36:06 DP1 TP1] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1024 11:36:06.553000 817 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ece7b9f3750>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:36:06 DP2 TP2] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1024 11:36:06.690000 818 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5b48347810>
[rank3]:W1024 11:36:06.712000 818 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5b48347d80>
[rank5]:W1024 11:36:06.758000 820 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb395abfab0>
[rank5]:W1024 11:36:06.780000 820 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb395abf750>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:36:06 DP3 TP3] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:36:06 DP5 TP5] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1024 11:36:08.260000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:08.271000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:08.281000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:08.289000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:08.298000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:08.307000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:08.316000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:08.372000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:08.523000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:08.539000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:08.552000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:08.565000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:08.578000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:08.635000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:08.739000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:08.778000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:08.946000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:08.954000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:08.963000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:08.974000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:08.998000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:09.040000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:09.056000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:09.067000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:09.212000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:09.222000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:09.233000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:09.242000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:09.260000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:09.294000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:09.310000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:09.321000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:09.472000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:09.482000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:09.493000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:09.502000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:09.516000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:09.550000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:09.566000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:09.577000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:09.728000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:09.740000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:09.750000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:09.762000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:09.773000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:09.806000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:09.822000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:09.833000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:09.984000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:09.996000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:10.006000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:10.022000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:10.032000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:10.062000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:10.078000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:10.089000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:10.239000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:10.250000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:10.260000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:10.284000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:10.294000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:10.320000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:10.336000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:10.347000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:10.498000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:10.507000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:10.516000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:10.538000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:10.556000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:10.570000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:10.586000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:10.601000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:10.756000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:10.767000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:10.778000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:10.794000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:10.816000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:10.825000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:10.842000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:10.857000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:11.012000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:11.028000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:11.038000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:11.050000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:11.072000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:11.081000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:11.098000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:11.113000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:11.267000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:11.282000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:11.292000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:11.320000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:11.331000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:11.348000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:11.364000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:11.383000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:11.526000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:11.538000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:11.547000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:11.576000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:11.590000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:11.604000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:11.616000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:11.639000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:11.786000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:11.795000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:11.804000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:11.832000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:11.850000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:11.861000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:11.873000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:11.891000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:12.046000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:12.055000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:12.064000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:12.088000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:12.110000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:12.121000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:12.133000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:12.145000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:12.306000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:12.315000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:12.324000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:12.344000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:12.370000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:12.381000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:12.394000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:12.405000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:12.566000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:12.575000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:12.584000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:12.600000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:12.630000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:12.641000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:12.654000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:12.665000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:12.826000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:12.835000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:12.844000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:12.856000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:12.890000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:12.901000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:12.914000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:12.926000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:13.086000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:13.095000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:13.103000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:13.115000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:13.150000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:13.161000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:13.174000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:13.186000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:13.346000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:13.356000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:13.365000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:13.376000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:13.411000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:13.422000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:13.435000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:13.448000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:13.615000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:13.624000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:13.635000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:13.645000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:13.676000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:13.688000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:13.699000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:13.711000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:13.878000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:13.887000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:13.900000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:13.909000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:13.940000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:13.951000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:13.964000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:13.976000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:14.138000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:14.147000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:14.158000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:14.168000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:14.196000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:14.212000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:14.222000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:14.234000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:14.398000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:14.407000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:14.418000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:14.428000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:14.452000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:14.471000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:14.483000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:14.494000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:14.660000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:14.671000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:14.688000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:14.696000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:14.718000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:14.732000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:14.743000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:14.752000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:14.920000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:14.930000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:14.944000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:14.954000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:14.973000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:14.988000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:15.003000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:15.012000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:15.180000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:15.190000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:15.201000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:15.214000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:15.230000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:15.244000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:15.263000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:15.271000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:15.440000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:15.450000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:15.461000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:15.474000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:15.486000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:15.500000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:15.522000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:15.532000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:15.698000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:15.707000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:15.717000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:15.736000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:15.748000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:15.759000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:15.785000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:15.796000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:15.958000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:15.967000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:15.976000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:15.996000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:16.008000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:16.019000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:16.040000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:16.052000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:16.218000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:16.227000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:16.236000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:16.256000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:16.268000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:16.279000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:16.304000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:16.319000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:16.478000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:16.486000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:16.496000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:16.512000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:16.524000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:16.538000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:16.560000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:16.577000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:16.738000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:16.747000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:16.755000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:16.780000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:16.793000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:16.803000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:16.824000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:16.839000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:16.998000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:17.007000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:17.016000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:17.036000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:17.048000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:17.063000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:17.080000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:17.095000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:17.258000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:17.270000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:17.279000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:17.292000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:17.304000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:17.322000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:17.336000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:17.351000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:17.518000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:17.530000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:17.539000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:17.550000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:17.562000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:17.584000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:17.595000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:17.608000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:17.949000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:17.960000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:17.968000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:17.977000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:17.986000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:17.995000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:18.058000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:18.069000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:18.411000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:18.421000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:18.432000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:18.441000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:18.479000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:18.500000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:18.511000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:18.525000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:18.860000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:18.870000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:18.880000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:18.889000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:18.900000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:18.910000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:18.966000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:18.975000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:19.124000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:19.135000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:19.144000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:19.153000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:19.165000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:19.177000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:19.226000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:19.237000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:19.387000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:19.396000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:19.409000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:19.420000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:19.431000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:19.440000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:19.488000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:19.500000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:19.650000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:19.659000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:19.672000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:19.684000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:19.695000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:19.704000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:19.747000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:19.757000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:19.916000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:19.926000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:19.936000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:19.945000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:19.961000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:19.971000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:20.006000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:20.017000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:20.176000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:20.188000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:20.198000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:20.207000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:20.220000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:20.232000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:20.266000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:20.277000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:20.435000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:20.450000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:20.464000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:20.476000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:20.486000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:20.496000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:20.540000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:20.552000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:20.699000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:20.714000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:20.725000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:20.740000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:20.751000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:20.761000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:20.800000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:20.812000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:20.962000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:20.978000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:20.989000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:21.000000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:21.015000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:21.025000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:21.060000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:21.072000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:21.230000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:21.242000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:21.260000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:21.272000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:21.283000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:21.292000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:21.328000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:21.343000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:21.494000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:21.503000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:21.520000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:21.532000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:21.546000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:21.556000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:21.588000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:21.603000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:21.758000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:21.767000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:21.780000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:21.792000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:21.807000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:21.819000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:21.848000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:21.863000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:22.022000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:22.031000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:22.042000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:22.054000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:22.067000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:22.078000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:22.108000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:22.123000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:22.286000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:22.295000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:22.306000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:22.318000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:22.328000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:22.338000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:22.368000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:22.383000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:22.550000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:22.559000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:22.570000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:22.582000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:22.592000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:22.602000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:22.628000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:22.643000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:22.814000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:22.823000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:22.834000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:22.846000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:22.856000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:22.866000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:22.888000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:22.900000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:23.079000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:23.088000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:23.099000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:23.110000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:23.121000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:23.131000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:23.148000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:23.160000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:23.342000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:23.351000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:23.363000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:23.374000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:23.384000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:23.395000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:23.408000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:23.420000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:23.606000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:23.615000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:23.626000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:23.638000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:23.649000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:23.660000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:23.671000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:23.683000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:24.376000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:24.384000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:24.395000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:24.407000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:24.416000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:24.425000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:24.435000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:24.445000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1082 ms 100.0% 
  triton_mm_359 0.1506 ms 71.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1547 ms 70.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1649 ms 65.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1668 ms 64.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_365 0.1717 ms 63.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1721 ms 62.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_357 0.1788 ms 60.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_351 0.1825 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_345 0.2084 ms 51.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.7001 seconds and 0.6909 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1053 ms 100.0% 
  triton_mm_359 0.1492 ms 70.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1522 ms 69.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1670 ms 63.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1682 ms 62.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_365 0.1775 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1778 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_351 0.1824 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1850 ms 56.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2061 ms 51.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6733 seconds and 0.7154 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1065 ms 100.0% 
  triton_mm_359 0.1482 ms 71.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1548 ms 68.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1673 ms 63.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1697 ms 62.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1779 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.1781 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_351 0.1841 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1857 ms 57.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2081 ms 51.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6495 seconds and 0.7067 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1056 ms 100.0% 
  triton_mm_359 0.1481 ms 71.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1522 ms 69.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1667 ms 63.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1691 ms 62.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1783 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.1784 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1848 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_351 0.1848 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_345 0.2074 ms 50.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6883 seconds and 0.8211 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1061 ms 100.0% 
  triton_mm_359 0.1469 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1530 ms 69.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1673 ms 63.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1696 ms 62.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_365 0.1773 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1781 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_351 0.1848 ms 57.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1854 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2082 ms 50.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6845 seconds and 0.6492 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1036 ms 100.0% 
  triton_mm_359 0.1435 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1468 ms 70.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1632 ms 63.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1658 ms 62.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1762 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.1762 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_351 0.1781 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1830 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2015 ms 51.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6593 seconds and 0.5724 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1036 ms 100.0% 
  triton_mm_359 0.1435 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1477 ms 70.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1621 ms 63.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1656 ms 62.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_365 0.1757 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1758 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_351 0.1788 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1827 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2018 ms 51.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6296 seconds and 0.3589 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1049 ms 100.0% 
  triton_mm_359 0.1482 ms 70.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1515 ms 69.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1641 ms 63.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1646 ms 63.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_365 0.1751 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1765 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_351 0.1824 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1835 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2073 ms 50.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6594 seconds and 0.4463 seconds precompiling for 39 choices
Capturing batches (bs=16 avail_mem=48.62 GB):  90%| | 47/52 [04:23<04:16, 51.27s/it]Capturing batches (bs=12 avail_mem=48.02 GB):  90%| | 47/52 [04:23<04:16, 51.27s/it][rank6]:W1024 11:36:41.060000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:41.070000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:41.089000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:41.106000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:41.122000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:41.128000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:41.129000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:41.138000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:41.139000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:41.158000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:41.175000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:41.201000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:41.201000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:41.208000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:41.211000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:41.218000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:41.233000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:41.258000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:41.276000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:41.281000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:41.291000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:41.558000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:41.627000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:41.693000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:41.701000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:41.705000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:41.730000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:41.747000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:41.770000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:41.777000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:41.782000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:41.788000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:41.793000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:41.813000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:41.828000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:41.847000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:41.853000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:41.859000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:41.863000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:41.874000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:41.884000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:41.897000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:41.917000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:41.924000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:41.928000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:41.933000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:41.944000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:41.954000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:41.965000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:41.993000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:42.002000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:42.013000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:42.199000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:42.281000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:42.352000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:42.420000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:42.580000 821 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank1]:W1024 11:36:42.582000 816 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank5]:W1024 11:36:42.595000 820 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank7]:W1024 11:36:42.625000 822 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank3]:W1024 11:36:42.627000 818 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank0]:W1024 11:36:42.658000 815 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank2]:W1024 11:36:42.662000 817 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank4]:W1024 11:36:42.902000 819 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_395 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_394 0.0089 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_400 0.0090 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0090 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_397 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_392 0.0092 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_398 0.0092 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_403 0.0092 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_393 0.0092 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_396 0.0092 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8474 seconds and 0.1931 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0090 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_403 0.0091 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_393 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_402 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_396 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_392 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_398 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8959 seconds and 0.1546 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_401 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_395 0.0090 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0090 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_394 0.0090 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_392 0.0091 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_393 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_396 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_398 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_397 0.0092 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_399 0.0092 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7764 seconds and 0.1300 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0088 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_396 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_400 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0091 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_402 0.0091 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_393 0.0091 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_397 0.0091 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_403 0.0092 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_399 0.0092 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8671 seconds and 0.1946 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0090 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_396 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_392 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_397 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_402 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_403 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_398 0.0092 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9077 seconds and 0.1334 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_400 0.0090 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0090 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_395 0.0090 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_394 0.0090 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_396 0.0091 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_403 0.0091 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_397 0.0091 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_398 0.0091 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_399 0.0091 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_402 0.0091 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8731 seconds and 0.2031 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_395 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_394 0.0086 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_396 0.0088 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_400 0.0088 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0088 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_403 0.0088 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_397 0.0089 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_402 0.0089 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_398 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_399 0.0089 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8671 seconds and 0.1465 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0089 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_402 0.0091 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_403 0.0091 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_401 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_392 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_393 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_397 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_396 0.0091 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_400 0.0091 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8514 seconds and 0.1412 seconds precompiling for 25 choices
[rank3]:W1024 11:36:52.688000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:52.856000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:53.027000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:53.092000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:53.189000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:36:53.363000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:53.386000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:53.538000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:53.544000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:53.553000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:36:53.609000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:36:53.893000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:36:54.051000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:36:54.059000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:36:54.312000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:36:54.706000 818 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank6]:W1024 11:36:54.792000 821 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank4]:W1024 11:36:54.836000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:36:55.063000 815 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank2]:W1024 11:36:55.111000 817 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank1]:W1024 11:36:55.215000 816 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank5]:W1024 11:36:55.414000 820 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank7]:W1024 11:36:55.561000 822 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank4]:W1024 11:36:56.155000 819 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_415 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_414 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0090 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0090 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_427 0.0095 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_426 0.0095 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_422 0.0097 ms 86.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_423 0.0098 ms 86.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_409 0.0100 ms 84.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_424 0.0101 ms 83.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8573 seconds and 0.4271 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0086 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0091 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0091 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_408 0.0093 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_426 0.0094 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_427 0.0095 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_422 0.0097 ms 86.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0097 ms 86.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_423 0.0097 ms 86.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9342 seconds and 0.4411 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_415 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_414 0.0085 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_416 0.0088 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0089 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_427 0.0091 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_426 0.0091 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_422 0.0093 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_423 0.0093 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_424 0.0097 ms 85.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0098 ms 84.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8921 seconds and 0.4002 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_415 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_414 0.0085 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_416 0.0090 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0090 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_408 0.0096 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_427 0.0096 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_426 0.0097 ms 87.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_409 0.0097 ms 86.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_423 0.0098 ms 86.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0099 ms 85.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9336 seconds and 0.4156 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_415 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_414 0.0087 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_416 0.0091 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0091 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_427 0.0097 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_426 0.0097 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_422 0.0100 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_423 0.0100 ms 86.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_424 0.0102 ms 85.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_425 0.0102 ms 85.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9090 seconds and 0.4354 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0089 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_426 0.0095 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_427 0.0095 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_422 0.0095 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_423 0.0095 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_409 0.0098 ms 86.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_408 0.0099 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8898 seconds and 0.4209 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0086 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0091 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0091 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_408 0.0094 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_426 0.0095 ms 87.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_427 0.0096 ms 87.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_422 0.0097 ms 86.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_423 0.0097 ms 86.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_409 0.0098 ms 85.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9092 seconds and 0.4265 seconds precompiling for 25 choices
[rank6]:W1024 11:37:01.380000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:01.455000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0090 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0090 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_426 0.0092 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_427 0.0094 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_423 0.0095 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0095 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0096 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_408 0.0097 ms 87.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8620 seconds and 0.4164 seconds precompiling for 25 choices
[rank3]:W1024 11:37:01.528000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:01.554000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:01.605000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:01.689000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:01.705000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:01.768000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:01.778000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:01.854000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:01.869000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:01.955000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:01.959000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:01.969000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:02.035000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:02.047000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:02.135000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:02.148000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:02.432000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:02.510000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:02.610000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:02.933000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:03.010000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:03.110000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:06.121000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:06.199000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:06.302000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:06.310000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:06.386000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:06.465000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:06.497000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:06.513000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:06.543000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:06.589000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:06.630000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:06.645000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:06.691000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:06.707000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:06.741000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:06.808000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:06.820000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:06.923000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:06.924000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:07.002000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:07.104000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:07.173000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:07.251000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:07.353000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:07.939000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:07.950000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:07.967000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:07.976000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:08.005000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:08.016000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:08.026000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:08.043000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:08.055000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:08.082000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:08.112000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:08.118000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:08.128000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:08.144000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:08.157000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:08.183000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:08.190000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:08.291000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:08.306000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:08.385000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:08.554000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:08.633000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:08.737000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:08.794000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:09.374000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:09.384000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:09.396000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:09.406000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:09.418000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:09.427000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:09.457000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:09.708000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:09.863000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:09.872000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:09.883000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:09.894000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:09.904000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:09.971000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:09.984000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:09.995000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:10.345000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:10.356000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:10.364000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:10.376000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:10.385000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:10.394000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:10.417000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:10.454000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:11.997000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:12.016000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:12.034000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:12.045000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:12.053000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:12.067000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:12.074000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:12.075000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:12.094000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:12.107000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:12.112000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:12.122000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:12.124000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:12.130000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:12.144000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:12.145000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:12.152000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:12.162000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:12.171000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:12.179000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:12.185000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:12.200000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:12.202000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:12.247000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0112 ms 100.0% 
  triton_mm_431 0.0280 ms 40.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0420 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0454 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0492 ms 22.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0533 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0581 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0598 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0611 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0756 ms 14.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1994 seconds and 0.8382 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0113 ms 100.0% 
  triton_mm_431 0.0277 ms 40.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0418 ms 27.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 25.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0492 ms 23.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0531 ms 21.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0580 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0596 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0611 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0757 ms 15.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2390 seconds and 0.7584 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0125 ms 100.0% 
  triton_mm_431 0.0278 ms 45.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0419 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 27.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0492 ms 25.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0532 ms 23.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0582 ms 21.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0595 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0611 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0754 ms 16.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.3128 seconds and 0.7365 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0110 ms 100.0% 
  triton_mm_431 0.0279 ms 39.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0418 ms 26.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0452 ms 24.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0491 ms 22.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0531 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0581 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0597 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0610 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0754 ms 14.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2548 seconds and 0.7749 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0110 ms 100.0% 
  triton_mm_431 0.0277 ms 39.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0419 ms 26.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0493 ms 22.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0532 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0581 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0595 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0610 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0754 ms 14.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2567 seconds and 0.7102 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0109 ms 100.0% 
  triton_mm_431 0.0277 ms 39.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0421 ms 26.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0495 ms 22.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0532 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0583 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0596 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0612 ms 17.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0756 ms 14.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2074 seconds and 0.4284 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0113 ms 100.0% 
  triton_mm_431 0.0277 ms 41.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0419 ms 27.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 25.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0493 ms 23.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0531 ms 21.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0582 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0596 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0611 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0756 ms 15.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2277 seconds and 0.6949 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0112 ms 100.0% 
  triton_mm_431 0.0277 ms 40.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0418 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0452 ms 24.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0491 ms 22.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0531 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0581 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0594 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0609 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0754 ms 14.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2237 seconds and 0.7232 seconds precompiling for 39 choices
[rank2]:W1024 11:37:22.891000 817 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ece7b9f3630>
[rank2]:W1024 11:37:22.914000 817 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ece7b9f3750>
[rank5]:W1024 11:37:22.969000 820 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb395abfab0>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:37:22 DP2 TP2] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1024 11:37:22.992000 820 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb395abf750>
[rank3]:W1024 11:37:23.009000 818 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5b48347810>
[rank7]:W1024 11:37:23.029000 822 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dbb9180f0>
[rank3]:W1024 11:37:23.032000 818 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5b48347d80>
[rank7]:W1024 11:37:23.052000 822 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dbbbef990>
[rank1]:W1024 11:37:23.056000 816 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f43715c3ae0>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:37:23 DP5 TP5] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1024 11:37:23.078000 821 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f93d5183a20>
[rank1]:W1024 11:37:23.079000 816 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f43715c3ab0>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:37:23 DP3 TP3] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1024 11:37:23.101000 821 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f93d5183b70>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:37:23 DP7 TP7] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:37:23 DP1 TP1] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:37:23 DP6 TP6] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1024 11:37:23.344000 819 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eda859e7660>
[rank0]:W1024 11:37:23.352000 815 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fabda487ed0>
[rank4]:W1024 11:37:23.367000 819 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eda859e7690>
[rank0]:W1024 11:37:23.375000 815 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fabda4877b0>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:37:23 DP4 TP4] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-24 11:37:23 DP0 TP0] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1024 11:37:24.727000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:24.735000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:24.744000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:24.755000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:24.767000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:24.776000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:24.785000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:24.840000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:24.993000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:25.005000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:25.013000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:25.023000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:25.036000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:25.047000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:25.060000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:25.103000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:25.257000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:25.269000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:25.285000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:25.296000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:25.310000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:25.319000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:25.331000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:25.365000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:25.519000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:25.531000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:25.549000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:25.560000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:25.573000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:25.584000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:25.595000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:25.624000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:25.782000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:25.795000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:25.808000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:25.823000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:25.837000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:25.847000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:25.859000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:25.884000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:26.047000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:26.063000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:26.080000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:26.096000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:26.109000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:26.119000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:26.130000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:26.156000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:26.315000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:26.327000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:26.340000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:26.360000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:26.373000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:26.383000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:26.395000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:26.420000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:26.583000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:26.594000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:26.605000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:26.624000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:26.636000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:26.647000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:26.659000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:26.681000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:26.851000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:26.860000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:26.871000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:26.888000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:26.900000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:26.911000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:26.923000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:26.940000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:27.119000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:27.128000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:27.139000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:27.151000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:27.164000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:27.179000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:27.190000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:27.202000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:27.389000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:27.400000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:27.409000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:27.419000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:27.429000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:27.444000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:27.458000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:27.468000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:27.657000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:27.668000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:27.677000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:27.686000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:27.697000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:27.709000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:27.722000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:27.733000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:27.920000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:27.932000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:27.950000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:27.962000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:27.975000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:27.985000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:27.997000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:28.008000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:28.184000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:28.196000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:28.219000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:28.230000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:28.242000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:28.253000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:28.265000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:28.276000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:28.447000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:28.459000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:28.489000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:28.501000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:28.511000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:28.523000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:28.534000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:28.547000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:28.715000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:28.724000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:28.753000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:28.764000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:28.775000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:28.787000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:28.799000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:28.812000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:28.985000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:28.996000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:29.015000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:29.026000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:29.045000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:29.055000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:29.069000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:29.079000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:29.252000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:29.265000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:29.278000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:29.294000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:29.308000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:29.324000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:29.335000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:29.346000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:29.515000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:29.527000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:29.545000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:29.564000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:29.574000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:29.593000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:29.604000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:29.617000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:29.783000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:29.795000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:29.808000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:29.828000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:29.843000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:29.857000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:29.872000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:29.884000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:30.053000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:30.065000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:30.083000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:30.102000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:30.114000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:30.132000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:30.143000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:30.155000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:30.320000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:30.333000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:30.346000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:30.370000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:30.382000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:30.399000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:30.411000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:30.422000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:30.588000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:30.601000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:30.610000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:30.638000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:30.650000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:30.667000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:30.679000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:30.690000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:30.856000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:30.868000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:30.886000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:30.916000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:30.925000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:30.944000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:30.954000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:30.965000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:31.397000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:31.407000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:31.418000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:31.429000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:31.439000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:31.449000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:31.460000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:31.509000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:31.666000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:31.675000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:31.687000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:31.698000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:31.709000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:31.721000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:31.732000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:31.779000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:31.937000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:31.947000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:31.958000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:31.969000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:31.980000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:31.990000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:32.002000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:32.047000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:32.465000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:32.476000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:32.485000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:32.495000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:32.503000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:32.516000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:32.526000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:32.573000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:32.974000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:32.983000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:32.994000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:33.005000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:33.014000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:33.023000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:33.035000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:33.088000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:33.529000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:33.539000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:33.549000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:33.557000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:33.569000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:33.580000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:33.635000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:33.645000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:34.047000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:34.056000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:34.065000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:34.077000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:34.088000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:34.099000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:34.152000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:34.162000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:34.319000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:34.328000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:34.339000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:34.353000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:34.365000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:34.377000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:34.417000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:34.430000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:34.593000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:34.605000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:34.617000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:34.628000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:34.637000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:34.648000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:34.679000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:34.700000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:34.866000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:34.877000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:34.889000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:34.900000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:34.911000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:34.921000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:34.947000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:34.967000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:35.135000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:35.147000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:35.157000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:35.173000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:35.186000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:35.197000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:35.217000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:35.234000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:35.407000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:35.419000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:35.435000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:35.446000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:35.458000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:35.471000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:35.483000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:35.518000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:35.679000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:35.691000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:35.707000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:35.719000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:35.731000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:35.744000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:35.756000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:35.786000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:35.947000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:35.963000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:35.979000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:35.991000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:36.003000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:36.015000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:36.028000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:36.054000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:36.218000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:36.238000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:36.253000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:36.264000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:36.274000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:36.285000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:36.296000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:36.327000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:36.489000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:36.509000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:36.525000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:36.536000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:36.546000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:36.555000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:36.566000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:36.596000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:36.757000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:36.782000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:36.793000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:36.808000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:36.818000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:36.828000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:36.839000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:36.863000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:37.025000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:37.062000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:37.073000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:37.085000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:37.094000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:37.105000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:37.116000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:37.144000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:37.301000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:37.334000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:37.345000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:37.359000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:37.368000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:37.379000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:37.389000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:37.411000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:37.577000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:37.605000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:37.617000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:37.632000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:37.642000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:37.651000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:37.662000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:37.679000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:37.845000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:37.874000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:37.885000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:37.903000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:37.914000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:37.924000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:37.935000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:37.947000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:38.113000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:38.141000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:38.153000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:38.176000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:38.185000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:38.195000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:38.206000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:38.217000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:38.381000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:38.409000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:38.421000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:38.447000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:38.457000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:38.467000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:38.478000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:38.489000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:38.653000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:38.677000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:38.689000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:38.720000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:38.730000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:38.739000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:38.750000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:38.761000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:38.927000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:38.945000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:38.957000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:38.995000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:39.005000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:39.015000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:39.026000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:39.038000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:39.201000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:39.217000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:39.229000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:39.267000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:39.277000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:39.287000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:39.298000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:39.310000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:39.481000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:39.498000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:39.509000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:39.543000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:39.553000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:39.563000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:39.574000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:39.587000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:39.749000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:39.769000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:39.781000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:39.815000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:39.825000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:39.836000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:39.847000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:39.858000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:40.018000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:40.038000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:40.049000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:40.088000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:40.097000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:40.107000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:40.118000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:40.129000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:40.283000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:40.307000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:40.316000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:40.365000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:40.377000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:40.389000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:40.402000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:40.412000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:40.571000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:40.580000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:40.589000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:40.637000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:40.649000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:40.661000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:40.673000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:40.683000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:40.843000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:40.852000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:40.861000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:40.910000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:40.926000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:40.939000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:40.952000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:40.962000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:41.123000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:41.135000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:41.144000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:41.181000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:41.193000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:41.206000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:41.218000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:41.234000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:41.945000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:41.954000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:41.967000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:41.975000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:41.987000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:41.996000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:42.007000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:42.015000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0906 ms 100.0% 
  triton_mm_483 0.1468 ms 61.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1508 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1656 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1681 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1714 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1764 ms 51.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_489 0.1773 ms 51.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_475 0.1812 ms 50.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1836 ms 49.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.6910 seconds and 0.6473 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0910 ms 100.0% 
  triton_mm_483 0.1485 ms 61.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1529 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1656 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1688 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1714 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1773 ms 51.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_489 0.1775 ms 51.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_475 0.1826 ms 49.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1846 ms 49.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.7027 seconds and 0.7271 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0921 ms 100.0% 
  triton_mm_483 0.1510 ms 61.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1547 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1642 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1668 ms 55.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_489 0.1709 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1710 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1717 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_481 0.1780 ms 51.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_475 0.1822 ms 50.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.7277 seconds and 0.4548 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0895 ms 100.0% 
  triton_mm_483 0.1450 ms 61.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1471 ms 60.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1625 ms 55.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1651 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1679 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_489 0.1759 ms 50.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1759 ms 50.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_475 0.1787 ms 50.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1828 ms 49.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.7151 seconds and 0.7562 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0889 ms 100.0% 
  triton_mm_483 0.1434 ms 62.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1484 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1621 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1645 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1685 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1751 ms 50.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_489 0.1753 ms 50.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_475 0.1786 ms 49.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1827 ms 48.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.6587 seconds and 0.3835 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0918 ms 100.0% 
  triton_mm_483 0.1480 ms 62.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1534 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1672 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1689 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1741 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_489 0.1773 ms 51.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1779 ms 51.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_475 0.1836 ms 50.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1840 ms 49.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.7359 seconds and 0.6177 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0930 ms 100.0% 
  triton_mm_483 0.1503 ms 61.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1535 ms 60.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1667 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1693 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1730 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1777 ms 52.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_489 0.1777 ms 52.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_475 0.1832 ms 50.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1848 ms 50.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.7443 seconds and 0.6372 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0906 ms 100.0% 
  triton_mm_483 0.1442 ms 62.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1513 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1633 ms 55.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1645 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1716 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_489 0.1750 ms 51.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1755 ms 51.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_475 0.1807 ms 50.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1819 ms 49.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.7536 seconds and 0.3680 seconds precompiling for 39 choices
Capturing batches (bs=12 avail_mem=48.02 GB):  92%|| 48/52 [05:40<03:56, 59.05s/it]Capturing batches (bs=8 avail_mem=47.44 GB):  92%|| 48/52 [05:40<03:56, 59.05s/it] [rank1]:W1024 11:37:58.452000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:58.466000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:58.490000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:58.511000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:58.520000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:58.522000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:58.535000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:58.544000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:58.560000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:58.581000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:58.589000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:58.595000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:58.609000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:58.614000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:58.631000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:58.635000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:58.655000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:58.663000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:58.688000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:58.701000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:58.762000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:58.775000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:58.832000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:58.907000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:59.093000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:59.106000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:59.131000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:59.151000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:59.173000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:59.178000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:59.190000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:59.192000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:59.214000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:59.234000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:59.249000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:59.255000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:59.264000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:59.276000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:59.280000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:59.286000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:59.305000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:59.319000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:59.326000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:37:59.335000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:59.349000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:37:59.356000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:59.363000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:37:59.375000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:37:59.396000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:59.401000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:37:59.419000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:59.434000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:59.485000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:37:59.504000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:59.556000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:37:59.626000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:37:59.781000 816 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank2]:W1024 11:37:59.798000 817 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank7]:W1024 11:37:59.821000 822 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank6]:W1024 11:37:59.839000 821 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank5]:W1024 11:37:59.860000 820 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank0]:W1024 11:37:59.884000 815 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank4]:W1024 11:37:59.987000 819 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank3]:W1024 11:38:00.106000 818 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0090 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_527 0.0090 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_521 0.0091 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_522 0.0091 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_520 0.0091 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_523 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0092 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0092 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8693 seconds and 0.1636 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0086 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0086 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_520 0.0088 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_521 0.0088 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_522 0.0088 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_523 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_527 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8803 seconds and 0.1609 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_519 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_518 0.0086 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_516 0.0086 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0086 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_521 0.0087 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_520 0.0087 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_526 0.0087 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_527 0.0088 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_522 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_523 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9002 seconds and 0.1976 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0086 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0087 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_521 0.0087 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_520 0.0087 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_527 0.0088 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_522 0.0088 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_523 0.0088 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0088 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8722 seconds and 0.1587 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0086 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0087 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0087 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_521 0.0088 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_520 0.0088 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_524 0.0088 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_525 0.0088 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0088 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_527 0.0088 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9217 seconds and 0.1301 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0085 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0086 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0086 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_520 0.0088 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_521 0.0088 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_524 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_526 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_525 0.0089 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_512 0.0089 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9136 seconds and 0.1292 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0086 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0088 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0089 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_521 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_520 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_527 0.0090 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_523 0.0090 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0090 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_522 0.0090 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8940 seconds and 0.1523 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_517 0.0088 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0088 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_527 0.0089 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_520 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_521 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_526 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_524 0.0089 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_525 0.0090 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8916 seconds and 0.1668 seconds precompiling for 25 choices
[rank4]:W1024 11:38:10.315000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:38:10.377000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:38:10.707000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:38:10.796000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:38:10.821000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:38:10.881000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:38:10.910000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:38:10.987000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:38:11.033000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:38:11.209000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:38:11.210000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:38:11.305000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:38:11.424000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:38:11.496000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:38:11.536000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:38:11.705000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:38:12.052000 819 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank7]:W1024 11:38:12.433000 822 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank1]:W1024 11:38:12.521000 816 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank6]:W1024 11:38:12.582000 821 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank2]:W1024 11:38:12.748000 817 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank0]:W1024 11:38:12.828000 815 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank5]:W1024 11:38:12.963000 820 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank3]:W1024 11:38:13.142000 818 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0086 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_550 0.0092 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_551 0.0092 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_547 0.0094 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_546 0.0095 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_532 0.0098 ms 86.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0098 ms 86.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8910 seconds and 0.4019 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_539 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_538 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_551 0.0093 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_550 0.0093 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_547 0.0094 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_546 0.0094 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0097 ms 86.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_548 0.0099 ms 85.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8866 seconds and 0.4232 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_539 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_538 0.0087 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_540 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0090 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_550 0.0095 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_551 0.0095 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_546 0.0096 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_547 0.0097 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_533 0.0101 ms 84.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_549 0.0101 ms 84.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9650 seconds and 0.4306 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0089 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0089 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_550 0.0092 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_551 0.0092 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_546 0.0095 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_547 0.0096 ms 87.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0099 ms 84.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0099 ms 84.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9449 seconds and 0.4283 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0085 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0086 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_550 0.0090 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_551 0.0091 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_547 0.0091 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_546 0.0092 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_549 0.0097 ms 85.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_548 0.0098 ms 84.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9325 seconds and 0.4244 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_539 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_538 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0086 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0087 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_551 0.0090 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_550 0.0091 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_547 0.0093 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_546 0.0095 ms 87.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_532 0.0095 ms 86.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0096 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8851 seconds and 0.4114 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0089 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0090 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_551 0.0093 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_550 0.0094 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_546 0.0095 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_547 0.0097 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0097 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0099 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9596 seconds and 0.4061 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0085 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_541 0.0088 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0089 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_550 0.0094 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_551 0.0094 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_547 0.0097 ms 87.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0097 ms 87.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0098 ms 86.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_546 0.0099 ms 85.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9432 seconds and 0.4071 seconds precompiling for 25 choices
[rank4]:W1024 11:38:18.860000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:38:18.938000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:38:18.971000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:38:19.042000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:38:19.050000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:38:19.147000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:38:19.153000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:38:19.186000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:38:19.223000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:38:19.263000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:38:19.323000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:38:19.365000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:38:19.366000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:38:19.375000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:38:19.444000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:38:19.452000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:38:19.545000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:38:19.547000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:38:19.552000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:38:19.625000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:38:19.728000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:38:19.732000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:38:19.810000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:38:19.911000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:38:24.366000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:38:24.366000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:38:24.368000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:38:24.369000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:38:24.369000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:38:24.369000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:38:24.370000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:38:24.371000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:38:24.442000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:38:24.442000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:38:24.446000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:38:24.447000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:38:24.449000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:38:24.450000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:38:24.450000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:38:24.453000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:38:24.543000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:38:24.543000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:38:24.548000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:38:24.550000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:38:24.564000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:38:24.565000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:38:24.565000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:38:24.570000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:38:24 DP1 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:38:24 DP3 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:38:24 DP0 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:38:24 DP2 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:38:24 DP6 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:38:24 DP7 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:38:24 DP5 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:38:24 DP4 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1024 11:38:25.743000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:38:25.773000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:38:25.784000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:38:25.787000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:38:25.793000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:38:25.802000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:38:25.813000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:38:25.820000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:38:25.849000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:38:25.861000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:38:25.865000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:38:25.870000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:38:25.879000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:38:25.891000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:38:25.921000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:38:25.950000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:38:25.963000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:38:25.968000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:38:25.973000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:38:25 DP1 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank7]:W1024 11:38:25.982000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:38:25.994000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:38:26 DP3 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:38:26 DP6 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:38:26 DP2 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:38:26 DP5 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:38:26 DP7 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:38:26 DP4 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank0]:W1024 11:38:26.200000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:38:26.277000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:38:26.377000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-24 11:38:26 DP0 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank2]:W1024 11:38:26.949000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:38:26.959000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:38:26.968000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:38:26.980000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:38:26.991000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:38:27.003000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:38:27.015000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:38:27.032000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:38:27.221000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:38:27.232000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:38:27.243000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:38:27.254000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:38:27.268000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:38:27.279000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:38:27.290000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:38:27.303000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1024 11:38:27.501000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1024 11:38:27.511000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1024 11:38:27.521000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1024 11:38:27.532000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1024 11:38:27.544000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1024 11:38:27.555000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1024 11:38:27.566000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1024 11:38:27.579000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-24 11:38:27 DP0 TP0] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
/usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 8 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
