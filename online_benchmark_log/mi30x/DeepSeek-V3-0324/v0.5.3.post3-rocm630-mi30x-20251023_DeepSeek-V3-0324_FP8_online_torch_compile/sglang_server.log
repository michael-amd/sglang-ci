INFO 10-23 11:00:41 __init__.py:179] Automatically detected platform rocm.
WARNING 10-23 11:00:41 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 11:00:43] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-23 11:00:44] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.9, max_running_requests=1024, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', elastic_ep_backend=None, mooncake_ib_device=None, tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=953822991, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', enable_beta_spec=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 11:00:44] Using default HuggingFace chat template with detected content format: string
INFO 10-23 11:00:52 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 11:00:52 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 11:00:52 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 11:00:52 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 11:00:52 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 11:00:52 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 11:00:52 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 11:00:52 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 11:00:53 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 11:00:54] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-23 11:00:54 TP2] Process 285 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
`torch_dtype` is deprecated! Use `dtype` instead!
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 11:00:54] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 11:00:55] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 11:00:55 TP5] Process 288 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 11:00:55 TP2] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 11:00:55] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 11:00:55] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-23 11:00:55 TP3] Process 286 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 11:00:55 TP1] Process 284 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 11:00:55] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 11:00:55 TP4] Process 287 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 11:00:55 TP5] Init torch distributed begin.
[2025-10-23 11:00:55] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-23 11:00:55] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-23 11:00:55 TP0] Process 283 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 11:00:55 TP6] Process 289 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 11:00:55 TP3] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 11:00:55 TP1] Init torch distributed begin.
[2025-10-23 11:00:55 TP4] Init torch distributed begin.
[2025-10-23 11:00:55] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-23 11:00:55 TP7] Process 290 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 11:00:55 TP0] Init torch distributed begin.
[2025-10-23 11:00:55 TP6] Init torch distributed begin.
[2025-10-23 11:00:56 TP7] Init torch distributed begin.
[2025-10-23 11:00:57 TP0] sglang is using nccl==2.21.5
[2025-10-23 11:00:59 TP7] Init torch distributed ends. mem usage=3.92 GB
[2025-10-23 11:00:59 TP6] Init torch distributed ends. mem usage=3.93 GB
[2025-10-23 11:00:59 TP0] Init torch distributed ends. mem usage=3.63 GB
[2025-10-23 11:00:59 TP5] Init torch distributed ends. mem usage=3.91 GB
[2025-10-23 11:00:59 TP4] Init torch distributed ends. mem usage=3.99 GB
[2025-10-23 11:00:59 TP3] Init torch distributed ends. mem usage=4.04 GB
[2025-10-23 11:00:59 TP2] Init torch distributed ends. mem usage=4.05 GB
[2025-10-23 11:00:59 TP1] Init torch distributed ends. mem usage=4.05 GB
[2025-10-23 11:01:00 TP7] Load weight begin. avail mem=187.34 GB
[2025-10-23 11:01:00 TP6] Load weight begin. avail mem=187.33 GB
[2025-10-23 11:01:00 TP5] Load weight begin. avail mem=187.35 GB
[2025-10-23 11:01:00 TP2] Load weight begin. avail mem=187.21 GB
[2025-10-23 11:01:00 TP1] Load weight begin. avail mem=187.21 GB
[2025-10-23 11:01:00 TP4] Load weight begin. avail mem=187.27 GB
[2025-10-23 11:01:00 TP3] Load weight begin. avail mem=187.22 GB
[2025-10-23 11:01:00 TP0] Load weight begin. avail mem=187.63 GB
[2025-10-23 11:01:00 TP0] Detected fp8 checkpoint.
[2025-10-23 11:01:00 TP0] Only Deepseek V3/R1 on NV-platform with capability >= 80 can use shared experts fusion optimization. Shared experts fusion optimization is disabled.
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:18,  8.62it/s]
Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:00<00:26,  6.07it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:22,  7.23it/s]
Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:00<00:21,  7.38it/s]
Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:00<00:20,  7.59it/s]
Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:00<00:21,  7.25it/s]
Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:00<00:20,  7.47it/s]
Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:01<00:12, 12.09it/s]
Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:01<00:26,  5.79it/s]
Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:01<00:25,  5.90it/s]
Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:02<00:28,  5.22it/s]
Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:02<00:26,  5.50it/s]
Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:02<00:16,  8.80it/s]
Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:02<00:13, 10.57it/s]
Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:02<00:12, 10.86it/s]
Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:02<00:11, 12.49it/s]
Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:02<00:08, 15.51it/s]
Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:03<00:07, 17.44it/s]
Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:03<00:06, 20.17it/s]
Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:04<00:17,  7.45it/s]
Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:04<00:16,  7.73it/s]
Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:04<00:15,  7.70it/s]
Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:05<00:18,  6.68it/s]
Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:05<00:23,  5.12it/s]
Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:05<00:25,  4.63it/s]
Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:06<00:29,  4.04it/s]
Loading safetensors checkpoint shards:  28% Completed | 46/163 [00:06<00:29,  3.92it/s]
Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:07<00:41,  2.82it/s]
Loading safetensors checkpoint shards:  29% Completed | 48/163 [00:07<00:41,  2.76it/s]
Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:07<00:39,  2.85it/s]
Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:08<00:49,  2.28it/s]
Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:08<00:44,  2.52it/s]
Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:08<00:38,  2.91it/s]
Loading safetensors checkpoint shards:  33% Completed | 53/163 [00:09<00:39,  2.78it/s]
Loading safetensors checkpoint shards:  33% Completed | 54/163 [00:09<00:36,  2.96it/s]
Loading safetensors checkpoint shards:  34% Completed | 55/163 [00:09<00:35,  3.04it/s]
Loading safetensors checkpoint shards:  34% Completed | 56/163 [00:10<00:38,  2.81it/s]
Loading safetensors checkpoint shards:  35% Completed | 57/163 [00:10<00:37,  2.84it/s]
Loading safetensors checkpoint shards:  36% Completed | 59/163 [00:10<00:22,  4.70it/s]
Loading safetensors checkpoint shards:  38% Completed | 62/163 [00:10<00:13,  7.64it/s]
Loading safetensors checkpoint shards:  39% Completed | 64/163 [00:11<00:11,  8.69it/s]
Loading safetensors checkpoint shards:  40% Completed | 66/163 [00:11<00:09, 10.21it/s]
Loading safetensors checkpoint shards:  42% Completed | 69/163 [00:11<00:14,  6.39it/s]
Loading safetensors checkpoint shards:  44% Completed | 71/163 [00:12<00:11,  7.76it/s]
Loading safetensors checkpoint shards:  45% Completed | 73/163 [00:12<00:10,  8.21it/s]
Loading safetensors checkpoint shards:  46% Completed | 75/163 [00:12<00:09,  9.74it/s]
Loading safetensors checkpoint shards:  48% Completed | 78/163 [00:12<00:06, 12.74it/s]
Loading safetensors checkpoint shards:  49% Completed | 80/163 [00:12<00:06, 11.96it/s]
Loading safetensors checkpoint shards:  50% Completed | 82/163 [00:12<00:06, 11.94it/s]
Loading safetensors checkpoint shards:  52% Completed | 84/163 [00:13<00:06, 13.12it/s]
Loading safetensors checkpoint shards:  53% Completed | 86/163 [00:13<00:05, 14.43it/s]
Loading safetensors checkpoint shards:  55% Completed | 89/163 [00:13<00:04, 16.29it/s]
Loading safetensors checkpoint shards:  56% Completed | 91/163 [00:13<00:04, 15.09it/s]
Loading safetensors checkpoint shards:  57% Completed | 93/163 [00:13<00:04, 16.07it/s]
Loading safetensors checkpoint shards:  58% Completed | 95/163 [00:13<00:04, 13.79it/s]
Loading safetensors checkpoint shards:  60% Completed | 97/163 [00:13<00:04, 14.78it/s]
Loading safetensors checkpoint shards:  61% Completed | 99/163 [00:13<00:04, 14.99it/s]
Loading safetensors checkpoint shards:  62% Completed | 101/163 [00:14<00:03, 16.13it/s]
Loading safetensors checkpoint shards:  63% Completed | 103/163 [00:14<00:04, 13.90it/s]
Loading safetensors checkpoint shards:  65% Completed | 106/163 [00:15<00:09,  6.21it/s]
Loading safetensors checkpoint shards:  66% Completed | 108/163 [00:15<00:07,  7.46it/s]
Loading safetensors checkpoint shards:  68% Completed | 111/163 [00:15<00:05,  9.72it/s]
Loading safetensors checkpoint shards:  70% Completed | 114/163 [00:15<00:04, 12.06it/s]
Loading safetensors checkpoint shards:  71% Completed | 116/163 [00:15<00:03, 13.35it/s]
Loading safetensors checkpoint shards:  72% Completed | 118/163 [00:15<00:03, 11.57it/s]
Loading safetensors checkpoint shards:  74% Completed | 120/163 [00:16<00:03, 13.01it/s]
Loading safetensors checkpoint shards:  75% Completed | 123/163 [00:16<00:02, 15.24it/s]
Loading safetensors checkpoint shards:  77% Completed | 126/163 [00:16<00:02, 17.44it/s]
Loading safetensors checkpoint shards:  79% Completed | 129/163 [00:16<00:02, 15.77it/s]
Loading safetensors checkpoint shards:  80% Completed | 131/163 [00:16<00:01, 16.58it/s]
Loading safetensors checkpoint shards:  82% Completed | 133/163 [00:16<00:02, 12.90it/s]
Loading safetensors checkpoint shards:  83% Completed | 136/163 [00:17<00:01, 14.87it/s]
Loading safetensors checkpoint shards:  85% Completed | 138/163 [00:17<00:01, 15.84it/s]
Loading safetensors checkpoint shards:  86% Completed | 140/163 [00:17<00:01, 16.57it/s]
Loading safetensors checkpoint shards:  87% Completed | 142/163 [00:17<00:02, 10.38it/s]
Loading safetensors checkpoint shards:  89% Completed | 145/163 [00:17<00:01, 12.81it/s]
Loading safetensors checkpoint shards:  90% Completed | 147/163 [00:17<00:01, 13.89it/s]
Loading safetensors checkpoint shards:  91% Completed | 149/163 [00:17<00:00, 14.98it/s]
Loading safetensors checkpoint shards:  93% Completed | 151/163 [00:18<00:00, 15.89it/s]
Loading safetensors checkpoint shards:  94% Completed | 153/163 [00:18<00:01,  6.04it/s]
Loading safetensors checkpoint shards:  95% Completed | 155/163 [00:19<00:01,  7.46it/s]
Loading safetensors checkpoint shards:  96% Completed | 157/163 [00:19<00:00,  9.02it/s]
Loading safetensors checkpoint shards:  98% Completed | 159/163 [00:19<00:00,  9.30it/s]
Loading safetensors checkpoint shards:  99% Completed | 161/163 [00:19<00:00, 10.83it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:19<00:00,  8.35it/s]

[2025-10-23 11:01:48 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.65 GB, mem usage=79.56 GB.
[2025-10-23 11:01:48 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.64 GB, mem usage=79.56 GB.
[2025-10-23 11:01:48 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.65 GB, mem usage=79.56 GB.
[2025-10-23 11:01:48 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.06 GB, mem usage=79.56 GB.
[2025-10-23 11:01:53 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.77 GB, mem usage=79.56 GB.
[2025-10-23 11:01:54 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.79 GB, mem usage=79.56 GB.
[2025-10-23 11:01:55 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.78 GB, mem usage=79.56 GB.
[2025-10-23 11:01:58 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.70 GB, mem usage=79.56 GB.
[2025-10-23 11:01:58 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-23 11:01:58 TP5] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-23 11:01:58 TP5] Memory pool end. avail mem=43.54 GB
[2025-10-23 11:01:58 TP4] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-23 11:01:58 TP3] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-23 11:01:58 TP4] Memory pool end. avail mem=43.45 GB
[2025-10-23 11:01:58 TP3] Memory pool end. avail mem=43.40 GB
[2025-10-23 11:01:58 TP7] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-23 11:01:58 TP7] Memory pool end. avail mem=43.53 GB
[2025-10-23 11:01:58 TP0] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-23 11:01:58 TP0] Memory pool end. avail mem=43.81 GB
[2025-10-23 11:01:58 TP1] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-23 11:01:58 TP2] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-23 11:01:58 TP1] Memory pool end. avail mem=43.40 GB
[2025-10-23 11:01:58 TP2] Memory pool end. avail mem=43.39 GB
[2025-10-23 11:01:58 TP6] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-23 11:01:58 TP6] Memory pool end. avail mem=43.52 GB
[2025-10-23 11:02:00 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=43.32 GB
[2025-10-23 11:02:00 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=43.19 GB
[2025-10-23 11:02:00 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=43.25 GB
[2025-10-23 11:02:00 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=43.61 GB
[2025-10-23 11:02:00 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
[2025-10-23 11:02:00 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=43.19 GB
[2025-10-23 11:02:00 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=43.33 GB
[2025-10-23 11:02:00 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=43.33 GB
[2025-10-23 11:02:00 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=43.20 GB
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=42.97 GB):   0%|          | 0/52 [00:00<?, ?it/s][aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 11:02:03 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 11:02:03 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 11:02:03 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 11:02:03 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 11:02:03 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 11:02:03 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 11:02:03 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 11:02:03 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:03 TP7] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:03 TP4] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:03 TP6] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:03 TP0] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:03 TP1] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:03 TP2] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:03 TP3] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:03 TP5] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 11:02:04 TP7] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 11:02:04 TP6] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 11:02:04 TP5] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 11:02:04 TP3] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 11:02:04 TP4] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 11:02:04 TP2] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 11:02:05 TP0] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 11:02:05 TP1] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP7] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP7] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP7] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP7] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP7] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:05 TP7] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP6] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP6] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP6] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP5] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP5] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP5] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP6] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP6] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP5] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP5] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:05 TP6] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:05 TP5] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP4] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP4] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP4] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP4] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP4] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP3] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP3] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP3] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP3] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP3] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:05 TP4] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP2] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP2] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP2] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP2] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP2] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:05 TP3] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:05 TP2] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP0] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP0] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP0] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP0] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP0] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:05 TP0] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP1] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP1] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP1] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP1] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:05 TP1] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:05 TP1] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
Capturing batches (bs=512 avail_mem=42.97 GB):   2%|         | 1/52 [00:04<04:07,  4.86s/it]Capturing batches (bs=496 avail_mem=42.30 GB):   2%|         | 1/52 [00:04<04:07,  4.86s/it][aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:06 TP4] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:07 TP7] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:07 TP1] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:07 TP3] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:07 TP6] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:07 TP2] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:07 TP0] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:07 TP5] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=496 avail_mem=42.30 GB):   4%|         | 2/52 [00:06<02:29,  3.00s/it]Capturing batches (bs=480 avail_mem=42.28 GB):   4%|         | 2/52 [00:06<02:29,  3.00s/it][aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:07 TP0] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:07 TP2] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:07 TP1] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:07 TP3] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:07 TP7] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:07 TP4] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:07 TP6] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:07 TP5] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=480 avail_mem=42.28 GB):   6%|         | 3/52 [00:06<01:29,  1.82s/it]Capturing batches (bs=464 avail_mem=42.28 GB):   6%|         | 3/52 [00:06<01:29,  1.82s/it][aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP6] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP3] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP7] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP4] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP0] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP2] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP5] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP1] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=464 avail_mem=42.28 GB):   8%|         | 4/52 [00:07<01:01,  1.27s/it]Capturing batches (bs=448 avail_mem=42.27 GB):   8%|         | 4/52 [00:07<01:01,  1.27s/it][aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP1] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP2] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP4] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP7] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP0] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP5] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP6] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP3] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=448 avail_mem=42.27 GB):  10%|         | 5/52 [00:07<00:45,  1.04it/s]Capturing batches (bs=432 avail_mem=42.27 GB):  10%|         | 5/52 [00:07<00:45,  1.04it/s][aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP1] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP5] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP3] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP2] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP4] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP7] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP6] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:08 TP0] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=432 avail_mem=42.27 GB):  12%|        | 6/52 [00:08<00:35,  1.29it/s]Capturing batches (bs=416 avail_mem=42.26 GB):  12%|        | 6/52 [00:08<00:35,  1.29it/s][aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:09 TP4] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:09 TP7] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:09 TP6] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:09 TP5] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:09 TP2] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:09 TP0] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:09 TP3] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:09 TP1] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=416 avail_mem=42.26 GB):  13%|        | 7/52 [00:08<00:29,  1.51it/s]Capturing batches (bs=400 avail_mem=42.26 GB):  13%|        | 7/52 [00:08<00:29,  1.51it/s][aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:09 TP5] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:09 TP4] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:09 TP6] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:09 TP3] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:09 TP7] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:09 TP0] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:09 TP1] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:09 TP2] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=400 avail_mem=42.26 GB):  15%|        | 8/52 [00:09<00:25,  1.70it/s]Capturing batches (bs=384 avail_mem=42.25 GB):  15%|        | 8/52 [00:09<00:25,  1.70it/s][aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP4] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP7] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP1] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP6] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP5] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP0] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP2] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP3] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=384 avail_mem=42.25 GB):  17%|        | 9/52 [00:09<00:20,  2.05it/s]Capturing batches (bs=368 avail_mem=42.25 GB):  17%|        | 9/52 [00:09<00:20,  2.05it/s][aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP4] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP2] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP5] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP1] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP3] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP6] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP0] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP7] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=368 avail_mem=42.25 GB):  19%|        | 10/52 [00:09<00:19,  2.14it/s]Capturing batches (bs=352 avail_mem=42.25 GB):  19%|        | 10/52 [00:09<00:19,  2.14it/s][aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP3] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP2] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP6] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP5] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP7] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP1] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP4] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:10 TP0] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=352 avail_mem=42.25 GB):  21%|        | 11/52 [00:10<00:18,  2.21it/s]Capturing batches (bs=336 avail_mem=42.24 GB):  21%|        | 11/52 [00:10<00:18,  2.21it/s][aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:11 TP2] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:11 TP4] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:11 TP0] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:11 TP6] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:11 TP7] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:11 TP3] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:11 TP5] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:11 TP1] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=336 avail_mem=42.24 GB):  23%|       | 12/52 [00:10<00:17,  2.26it/s]Capturing batches (bs=320 avail_mem=42.24 GB):  23%|       | 12/52 [00:10<00:17,  2.26it/s][aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:11 TP4] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:11 TP7] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:11 TP1] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:11 TP6] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:11 TP0] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:11 TP2] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:11 TP3] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:11 TP5] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=320 avail_mem=42.24 GB):  25%|       | 13/52 [00:10<00:15,  2.54it/s]Capturing batches (bs=304 avail_mem=42.23 GB):  25%|       | 13/52 [00:10<00:15,  2.54it/s][aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:11 TP3] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:11 TP2] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:12 TP6] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:12 TP4] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:12 TP5] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:12 TP1] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:12 TP7] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:12 TP0] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=304 avail_mem=42.23 GB):  27%|       | 14/52 [00:11<00:15,  2.49it/s]Capturing batches (bs=288 avail_mem=42.23 GB):  27%|       | 14/52 [00:11<00:15,  2.49it/s][aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:12 TP4] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:12 TP7] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:12 TP1] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:12 TP0] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:12 TP2] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:12 TP5] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:12 TP3] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:12 TP6] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=288 avail_mem=42.23 GB):  29%|       | 15/52 [00:11<00:13,  2.75it/s]Capturing batches (bs=272 avail_mem=42.22 GB):  29%|       | 15/52 [00:11<00:13,  2.75it/s][aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:12 TP3] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:12 TP2] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:12 TP1] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:12 TP0] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:12 TP4] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:12 TP7] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:12 TP6] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:12 TP5] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=272 avail_mem=42.22 GB):  31%|       | 16/52 [00:12<00:13,  2.61it/s]Capturing batches (bs=256 avail_mem=42.22 GB):  31%|       | 16/52 [00:12<00:13,  2.61it/s][aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP4] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP0] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP6] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP5] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP1] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP2] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP3] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP7] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP4] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP4] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP4] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP4] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP6] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP6] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP6] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:13 TP4] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP4] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP6] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP7] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP7] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP7] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:13 TP6] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP6] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP7] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:13 TP2] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP7] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP2] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP2] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP7] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP2] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:13 TP2] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP1] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP2] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP1] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP1] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP1] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:13 TP1] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP0] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP0] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP1] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP0] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP0] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:13 TP0] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP0] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP5] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP5] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP5] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP5] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:13 TP5] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP3] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP5] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP3] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP3] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:13 TP3] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:13 TP3] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP3] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=256 avail_mem=42.22 GB):  33%|      | 17/52 [00:12<00:13,  2.53it/s]Capturing batches (bs=248 avail_mem=42.21 GB):  33%|      | 17/52 [00:12<00:13,  2.53it/s][aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP7] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP5] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP6] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP0] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP4] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP1] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP3] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP2] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=248 avail_mem=42.21 GB):  35%|      | 18/52 [00:12<00:13,  2.49it/s]Capturing batches (bs=240 avail_mem=42.21 GB):  35%|      | 18/52 [00:12<00:13,  2.49it/s][aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP3] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP0] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP5] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP6] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP7] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP4] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP1] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:13 TP2] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=240 avail_mem=42.21 GB):  37%|      | 19/52 [00:13<00:13,  2.43it/s]Capturing batches (bs=232 avail_mem=42.20 GB):  37%|      | 19/52 [00:13<00:13,  2.43it/s][aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:14 TP6] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:14 TP4] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:14 TP0] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:14 TP2] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:14 TP1] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:14 TP5] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:14 TP3] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:14 TP7] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=232 avail_mem=42.20 GB):  38%|      | 20/52 [00:13<00:13,  2.41it/s]Capturing batches (bs=224 avail_mem=42.20 GB):  38%|      | 20/52 [00:13<00:13,  2.41it/s][aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:14 TP5] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:14 TP6] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:14 TP4] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:14 TP2] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:14 TP7] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:14 TP1] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:14 TP3] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:14 TP0] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=224 avail_mem=42.20 GB):  40%|      | 21/52 [00:14<00:12,  2.40it/s]Capturing batches (bs=216 avail_mem=42.19 GB):  40%|      | 21/52 [00:14<00:12,  2.40it/s][aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP0] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP2] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP1] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP3] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP4] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP6] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP5] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP7] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=216 avail_mem=42.19 GB):  42%|     | 22/52 [00:14<00:12,  2.39it/s]Capturing batches (bs=208 avail_mem=42.19 GB):  42%|     | 22/52 [00:14<00:12,  2.39it/s][aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP4] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP1] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP0] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP2] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP6] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP7] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP3] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP5] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=208 avail_mem=42.19 GB):  44%|     | 23/52 [00:14<00:10,  2.67it/s]Capturing batches (bs=200 avail_mem=42.18 GB):  44%|     | 23/52 [00:14<00:10,  2.67it/s][aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP3] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP1] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP2] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP5] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP6] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP7] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP0] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:15 TP4] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=200 avail_mem=42.18 GB):  46%|     | 24/52 [00:15<00:10,  2.58it/s]Capturing batches (bs=192 avail_mem=42.18 GB):  46%|     | 24/52 [00:15<00:10,  2.58it/s][aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP4] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP1] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP0] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP7] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP2] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP6] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP3] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP5] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=192 avail_mem=42.18 GB):  48%|     | 25/52 [00:15<00:09,  2.80it/s]Capturing batches (bs=184 avail_mem=42.18 GB):  48%|     | 25/52 [00:15<00:09,  2.80it/s][aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP4] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP6] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP7] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP1] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP0] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP5] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP2] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP3] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=184 avail_mem=42.18 GB):  50%|     | 26/52 [00:15<00:08,  2.99it/s]Capturing batches (bs=176 avail_mem=42.17 GB):  50%|     | 26/52 [00:15<00:08,  2.99it/s][aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP4] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP1] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP0] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP6] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP2] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP5] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP7] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:16 TP3] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=176 avail_mem=42.17 GB):  52%|    | 27/52 [00:16<00:07,  3.17it/s]Capturing batches (bs=168 avail_mem=42.17 GB):  52%|    | 27/52 [00:16<00:07,  3.17it/s][aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP6] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP1] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP3] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP2] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP0] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP5] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP4] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP7] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=168 avail_mem=42.17 GB):  54%|    | 28/52 [00:16<00:08,  2.88it/s]Capturing batches (bs=160 avail_mem=42.17 GB):  54%|    | 28/52 [00:16<00:08,  2.88it/s][aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP7] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP4] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP1] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP0] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP6] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP2] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP5] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP3] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=160 avail_mem=42.17 GB):  56%|    | 29/52 [00:16<00:07,  3.08it/s]Capturing batches (bs=152 avail_mem=42.16 GB):  56%|    | 29/52 [00:16<00:07,  3.08it/s][aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP0] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP5] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP4] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP3] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP7] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP6] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP2] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:17 TP1] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=152 avail_mem=42.16 GB):  58%|    | 30/52 [00:17<00:07,  2.84it/s]Capturing batches (bs=144 avail_mem=42.16 GB):  58%|    | 30/52 [00:17<00:07,  2.84it/s][aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP4] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP0] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP1] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP2] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP6] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP7] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP5] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP3] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=144 avail_mem=42.16 GB):  60%|    | 31/52 [00:17<00:06,  3.02it/s]Capturing batches (bs=136 avail_mem=42.15 GB):  60%|    | 31/52 [00:17<00:06,  3.02it/s][aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP0] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP4] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP1] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP6] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP2] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP7] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP5] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP3] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=136 avail_mem=42.15 GB):  62%|   | 32/52 [00:17<00:06,  3.17it/s]Capturing batches (bs=128 avail_mem=42.15 GB):  62%|   | 32/52 [00:17<00:06,  3.17it/s][aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 11:02:18 TP7] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 11:02:18 TP4] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 11:02:18 TP5] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 11:02:18 TP6] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 11:02:18 TP0] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 11:02:18 TP1] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 11:02:18 TP2] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 11:02:18 TP3] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP4] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP7] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP5] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP0] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP2] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP6] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP1] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP3] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP4] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP7] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP5] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP0] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP2] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP6] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP1] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP3] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP4] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP7] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP5] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP0] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP2] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP6] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP1] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP3] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP4] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP7] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP0] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP2] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:18 TP1] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP5] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP4] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:18 TP6] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:18 TP3] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:18 TP7] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:18 TP0] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:18 TP2] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:18 TP1] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:18 TP5] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:18 TP6] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:18 TP3] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP4] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP7] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP0] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP2] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP1] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP5] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP6] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:18 TP3] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=128 avail_mem=42.15 GB):  63%|   | 33/52 [00:18<00:05,  3.28it/s]Capturing batches (bs=120 avail_mem=42.15 GB):  63%|   | 33/52 [00:18<00:05,  3.28it/s][aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP1] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP0] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP7] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP6] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP2] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP3] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP4] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP5] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=120 avail_mem=42.15 GB):  65%|   | 34/52 [00:18<00:06,  2.94it/s]Capturing batches (bs=112 avail_mem=42.14 GB):  65%|   | 34/52 [00:18<00:06,  2.94it/s][aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP0] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP1] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP2] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP4] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP7] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP6] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP3] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP5] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=112 avail_mem=42.14 GB):  67%|   | 35/52 [00:18<00:05,  3.13it/s]Capturing batches (bs=104 avail_mem=42.14 GB):  67%|   | 35/52 [00:18<00:05,  3.13it/s][aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP7] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP6] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP0] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP2] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP3] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP5] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP4] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:19 TP1] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=104 avail_mem=42.14 GB):  69%|   | 36/52 [00:19<00:05,  2.87it/s]Capturing batches (bs=96 avail_mem=42.14 GB):  69%|   | 36/52 [00:19<00:05,  2.87it/s] [aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP4] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP1] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP0] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP2] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP6] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP7] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP3] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP5] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=96 avail_mem=42.14 GB):  71%|   | 37/52 [00:19<00:04,  3.04it/s]Capturing batches (bs=88 avail_mem=42.13 GB):  71%|   | 37/52 [00:19<00:04,  3.04it/s][aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP1] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP5] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP7] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP4] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP3] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP6] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP0] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP2] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=88 avail_mem=42.13 GB):  73%|  | 38/52 [00:19<00:04,  2.81it/s]Capturing batches (bs=80 avail_mem=42.13 GB):  73%|  | 38/52 [00:19<00:04,  2.81it/s][aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP4] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP0] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP7] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP1] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP2] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP6] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP5] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:20 TP3] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=80 avail_mem=42.13 GB):  75%|  | 39/52 [00:20<00:04,  3.03it/s]Capturing batches (bs=72 avail_mem=42.12 GB):  75%|  | 39/52 [00:20<00:04,  3.03it/s][aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP6] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP7] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP3] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP1] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP4] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP0] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP2] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP5] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=72 avail_mem=42.12 GB):  77%|  | 40/52 [00:20<00:04,  2.79it/s]Capturing batches (bs=64 avail_mem=42.12 GB):  77%|  | 40/52 [00:20<00:04,  2.79it/s][aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP7] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP4] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP0] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP5] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP1] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP6] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP3] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP2] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 11:02:21 TP7] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 11:02:21 TP4] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 11:02:21 TP0] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 11:02:21 TP6] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 11:02:21 TP5] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 11:02:21 TP1] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 11:02:21 TP3] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 11:02:21 TP2] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP4] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP7] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP0] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:21 TP4] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP1] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:21 TP6] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP7] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:21 TP2] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP5] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:21 TP0] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:21 TP3] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:21 TP1] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:21 TP6] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:21 TP2] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:21 TP5] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 11:02:21 TP3] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP7] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP4] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP0] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP6] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP1] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP2] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP3] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP5] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=64 avail_mem=42.12 GB):  79%|  | 41/52 [00:20<00:03,  3.00it/s]Capturing batches (bs=56 avail_mem=42.11 GB):  79%|  | 41/52 [00:20<00:03,  3.00it/s][aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP0] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP4] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP7] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP5] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP1] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP6] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP3] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:21 TP2] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=56 avail_mem=42.11 GB):  81%|  | 42/52 [00:21<00:03,  2.79it/s]Capturing batches (bs=48 avail_mem=42.11 GB):  81%|  | 42/52 [00:21<00:03,  2.79it/s][aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:22 TP1] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:22 TP2] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:22 TP0] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:22 TP4] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:22 TP7] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:22 TP6] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:22 TP3] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:22 TP5] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=48 avail_mem=42.11 GB):  83%| | 43/52 [00:21<00:02,  3.00it/s]Capturing batches (bs=40 avail_mem=42.10 GB):  83%| | 43/52 [00:21<00:02,  3.00it/s][aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:22 TP2] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:22 TP5] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:22 TP4] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:22 TP6] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:22 TP0] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:22 TP7] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:22 TP3] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:02:22 TP1] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=40 avail_mem=42.10 GB):  85%| | 44/52 [00:21<00:02,  2.78it/s]Capturing batches (bs=32 avail_mem=42.10 GB):  85%| | 44/52 [00:21<00:02,  2.78it/s][rank1]:W1023 11:02:28.562000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1023 11:02:28.647000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:02:28.704000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:02:28.725000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1023 11:02:28.764000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:02:28.787000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:02:28.788000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1023 11:02:28.812000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:02:28.834000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:02:28.844000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:02:28.846000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1023 11:02:28.917000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:02:28.917000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:02:28.927000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:02:28.936000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:02:28.942000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1023 11:02:28.966000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:02:28.975000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1023 11:02:29.018000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:02:29.047000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1023 11:02:29.049000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:02:29.057000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1023 11:02:29.146000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:02:29.180000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1023 11:02:30.516000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:02:30.628000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:02:30.664000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:02:30.676000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:02:30.751000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:02:30.760000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:02:30.832000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:02:30.892000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:02:32.207000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:02:32.282000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:02:32.391000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:02:32.442000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:32 TP1] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1023 11:02:32.516000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:02:32.532000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:02:32.552000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:02:32.580000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:02:32.606000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:02:32.607000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:02:32.613000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:02:32.630000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:02:32.655000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:32 TP6] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1023 11:02:32.683000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:02:32.703000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:02:32.730000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:02:32.752000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:32 TP4] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1023 11:02:32.781000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:32 TP0] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1023 11:02:32.788000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:32 TP5] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1023 11:02:32.830000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:32 TP2] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1023 11:02:32.863000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:02:32.905000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:02:32.925000 284 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank7]:W1023 11:02:32.962000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:02:33.010000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:33 TP7] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:33 TP3] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1023 11:02:33.145000 289 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank4]:W1023 11:02:33.235000 287 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank0]:W1023 11:02:33.282000 283 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank5]:W1023 11:02:33.292000 288 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank2]:W1023 11:02:33.344000 285 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank7]:W1023 11:02:33.550000 290 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank3]:W1023 11:02:33.630000 286 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_25 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_13 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_3 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_11 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_2 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_6 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_7 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_1 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1956 seconds and 0.5018 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_13 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_3 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_5 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_11 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_2 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_7 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_1 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_25 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1900 seconds and 0.5128 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_13 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_3 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_11 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_5 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_23 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_1 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_6 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_7 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_2 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2446 seconds and 0.5401 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_13 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_11 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_1 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_3 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_6 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_7 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_2 0.0065 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_5 0.0065 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1954 seconds and 0.5591 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_11 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_3 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_13 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_6 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_1 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_2 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_5 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_7 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_9 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2959 seconds and 0.4867 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_15 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_13 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_6 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0063 ms 98.7% 
  triton_bmm_3 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_2 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_7 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_11 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_23 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2481 seconds and 0.5463 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_11 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_7 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_13 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_1 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_3 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_5 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_9 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_2 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_6 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3048 seconds and 0.4757 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_2 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_3 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_5 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_7 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_11 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_1 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_13 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_6 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2660 seconds and 0.4783 seconds precompiling for 27 choices
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1023 11:02:44.169000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:02:44.198000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:02:44.517000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:02:44.545000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:02:44.672000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:02:44.702000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:02:44.726000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:02:44.741000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:02:45.006000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:02:45.006000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:02:45.057000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:02:45.065000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:02:45.225000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:02:45.251000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:02:45.520000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:02:45.541000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1023 11:02:47.484000 284 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1023 11:02:48.102000 283 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
[rank2]:W1023 11:02:48.654000 285 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1023 11:02:49.434000 286 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1023 11:02:51.933000 289 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1023 11:02:52.824000 290 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1023 11:02:53.729000 287 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1023 11:02:55.019000 288 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0069 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0073 ms 95.0% 
  triton_bmm_41 0.0075 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0080 ms 86.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0085 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0087 ms 79.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0090 ms 76.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0091 ms 75.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0093 ms 73.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0095 ms 72.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 7.7657 seconds and 0.5248 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0069 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0071 ms 97.7% 
  triton_bmm_41 0.0076 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0079 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_33 0.0085 ms 81.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_39 0.0086 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_37 0.0089 ms 77.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0090 ms 77.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0090 ms 77.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_31 0.0094 ms 73.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 7.9153 seconds and 0.5164 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0073 ms 92.8% 
  triton_bmm_41 0.0076 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0080 ms 84.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_33 0.0084 ms 80.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_39 0.0085 ms 78.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_46 0.0089 ms 76.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0089 ms 76.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_47 0.0094 ms 71.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_37 0.0094 ms 71.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 7.8389 seconds and 0.5209 seconds precompiling for 27 choices
[rank1]:W1023 11:02:57.407000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:02:57.482000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:02:57.580000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:57 TP1] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1023 11:02:57.711000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:02:57.785000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0069 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_41 0.0074 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0074 ms 93.0% 
  triton_bmm_28 0.0080 ms 86.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0085 ms 81.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0086 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0089 ms 77.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0089 ms 77.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0090 ms 76.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_51 0.0094 ms 73.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 7.8398 seconds and 0.5188 seconds precompiling for 27 choices
[rank0]:W1023 11:02:57.882000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:57 TP0] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0068 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0071 ms 94.9% 
  triton_bmm_41 0.0076 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0079 ms 85.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0084 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0086 ms 79.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0090 ms 75.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0091 ms 74.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0093 ms 73.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_31 0.0094 ms 71.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.6171 seconds and 0.5416 seconds precompiling for 27 choices
[rank2]:W1023 11:02:58.637000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:02:58.713000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:02:58.812000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:58 TP2] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0066 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0071 ms 93.2% 
  triton_bmm_41 0.0073 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0077 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0084 ms 78.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0084 ms 78.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0085 ms 77.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0086 ms 77.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0087 ms 75.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0092 ms 72.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.7045 seconds and 0.5267 seconds precompiling for 27 choices
[rank6]:W1023 11:02:59.268000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:02:59.342000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:02:59.431000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:02:59.439000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:59 TP6] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1023 11:02:59.507000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:02:59.605000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:02:59 TP3] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0068 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0071 ms 94.9% 
  triton_bmm_41 0.0075 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0078 ms 86.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0084 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0085 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0089 ms 76.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0090 ms 75.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0090 ms 75.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_31 0.0093 ms 73.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.4529 seconds and 0.5339 seconds precompiling for 27 choices
[rank7]:W1023 11:03:00.217000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:00.293000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:00.394000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:00 TP7] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1023 11:03:00.884000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x32x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_29 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_41 0.0073 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0078 ms 83.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0084 ms 78.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0085 ms 76.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_37 0.0089 ms 73.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0090 ms 72.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0091 ms 72.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_51 0.0094 ms 69.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3194 seconds and 0.5155 seconds precompiling for 27 choices
[rank4]:W1023 11:03:00.960000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:01.058000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:01 TP4] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1023 11:03:01.917000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:01.993000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:02 TP1] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1023 11:03:02.091000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:02 TP5] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:02 TP0] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1023 11:03:03.167000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:03.242000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:03.341000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:03 TP1] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:03 TP2] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1023 11:03:03.508000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:03.583000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:03.682000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:03 TP0] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:03 TP6] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:04 TP3] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1023 11:03:04.535000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:04.610000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:04.708000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:04 TP2] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:04 TP4] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:04 TP7] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1023 11:03:05.136000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:05.212000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:05.252000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:05.311000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:05.325000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:05 TP6] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1023 11:03:05.422000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:05 TP3] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1023 11:03:06.071000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:06.145000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:06.243000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:06 TP4] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1023 11:03:06.466000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:06.550000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:06 TP5] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1023 11:03:06.698000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:06.826000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:06.961000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:06.975000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:07.034000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:07.063000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:07.131000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:07 TP7] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:07 TP0] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:07 TP1] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1023 11:03:07.491000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:07.732000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:07.801000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:07.875000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:07.972000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:08.014000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:08 TP5] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:08 TP2] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1023 11:03:08.514000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:08.574000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:08.598000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:08.650000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:08.668000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:08.743000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:08.750000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:08.752000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:08 TP0] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1023 11:03:08.840000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:08.882000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:08 TP1] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1023 11:03:09.033000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:09.119000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:09 TP6] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:09 TP3] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1023 11:03:09.778000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:09.779000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:09.853000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:09.951000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:10 TP2] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1023 11:03:10.010000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1023 11:03:10.242000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:10.343000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:10.579000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:10 TP4] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1023 11:03:10.644000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:10.722000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:10.746000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:10.820000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:10.821000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:10.860000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:10 TP6] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1023 11:03:10.918000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:10 TP3] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1023 11:03:11.181000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:11 TP7] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1023 11:03:11.423000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:11.710000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:11.832000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:11.906000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:12.002000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:12 TP4] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:12 TP5] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1023 11:03:12.471000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:12.549000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:12.651000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:12 TP7] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1023 11:03:13.321000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:13.396000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:13.493000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:03:13 TP5] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0101 ms 100.0% 
  triton_mm_55 0.0293 ms 34.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0398 ms 25.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0453 ms 22.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0489 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0582 ms 17.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0678 ms 14.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0683 ms 14.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0684 ms 14.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0706 ms 14.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5720 seconds and 0.3781 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0099 ms 100.0% 
  triton_mm_55 0.0331 ms 29.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0397 ms 24.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0454 ms 21.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0490 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_77 0.0683 ms 14.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0685 ms 14.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_74 0.0687 ms 14.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0705 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_75 0.0715 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6413 seconds and 0.3910 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0099 ms 100.0% 
  triton_mm_55 0.0331 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0407 ms 24.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0454 ms 21.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0530 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0691 ms 14.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_74 0.0692 ms 14.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0707 ms 14.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_75 0.0715 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_77 0.0730 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.6491 seconds and 0.3910 seconds precompiling for 27 choices
[rank0]:W1023 11:03:16.622000 283 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dfcdd5740>
[rank0]:W1023 11:03:16.653000 283 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dfcdd56b0>
[rank1]:W1023 11:03:16.796000 284 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec9a89bd830>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:03:16 TP0] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1023 11:03:16.828000 284 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec9a89bd6e0>
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:03:17 TP1] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_55 0.0290 ms 32.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0396 ms 23.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0452 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0489 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0580 ms 16.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0678 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0685 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0690 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0705 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5888 seconds and 0.3841 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_55 0.0292 ms 33.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0395 ms 24.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0452 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0487 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0580 ms 16.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0675 ms 14.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0679 ms 14.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0687 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0705 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6428 seconds and 0.3952 seconds precompiling for 27 choices
[rank2]:W1023 11:03:17.906000 285 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec47f4f18f0>
[rank2]:W1023 11:03:17.947000 285 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec47f4f1860>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:03:18 TP2] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1023 11:03:18.166000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1023 11:03:18.342000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_55 0.0304 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0414 ms 23.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0453 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0502 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0642 ms 14.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_74 0.0693 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0706 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_75 0.0714 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_77 0.0716 ms 13.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.5709 seconds and 0.3798 seconds precompiling for 27 choices
[rank0]:W1023 11:03:18.402000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:18.547000 289 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef416a31e00>
[rank1]:W1023 11:03:18.578000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:18.578000 289 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef416a319b0>
[rank0]:W1023 11:03:18.633000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:03:18 TP6] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1023 11:03:18.820000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:18.863000 286 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f078774df20>
[rank0]:W1023 11:03:18.865000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1023 11:03:18.895000 286 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f078c800ae0>
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_55 0.0285 ms 33.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0395 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0451 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0487 ms 19.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0578 ms 16.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0673 ms 14.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0682 ms 14.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0685 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0704 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5773 seconds and 0.3886 seconds precompiling for 27 choices
[rank1]:W1023 11:03:19.059000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:03:19 TP3] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1023 11:03:19.103000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1023 11:03:19.303000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:19.338000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:19.443000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:19.539000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:19.570000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:19.691000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:19.779000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:19.802000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_55 0.0291 ms 32.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0397 ms 23.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0453 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0490 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0584 ms 16.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0677 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0687 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0688 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0706 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5937 seconds and 0.3842 seconds precompiling for 27 choices
[rank2]:W1023 11:03:19.927000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:20.027000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:20.046000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:20.167000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:20.267000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:20.338000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:20.342000 287 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f607982de60>
[rank3]:W1023 11:03:20.408000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:20.415000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:20.425000 287 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f607982dfe0>
[rank7]:W1023 11:03:20.514000 290 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9d826e5aa0>
[rank6]:W1023 11:03:20.542000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:20.545000 290 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9d826e5a10>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:03:20 TP4] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1023 11:03:20.568000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:20.574000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:20.647000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:20.655000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:03:20 TP7] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1023 11:03:20.783000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:20.807000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:20.812000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1023 11:03:20.879000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:20.887000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:21.022000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:21.043000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:21.048000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:21.115000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:21.123000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:21.220000 288 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ecd0c7f9ef0>
[rank5]:W1023 11:03:21.252000 288 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ecd0c7fa130>
[rank6]:W1023 11:03:21.258000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:21.286000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:21.292000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:21.351000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:21.359000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:03:21 TP5] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1023 11:03:21.500000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:21.523000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:21.530000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1023 11:03:21.591000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:21.667000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:21.739000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:21.771000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:21.779000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:21.827000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:21.879000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:21.903000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:21.978000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:22.006000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:22.015000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:22.054000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:22.063000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:22.118000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:22.139000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:22.214000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:22.243000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:22.251000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:22.290000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:22.296000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:22.352000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:22.376000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:22.455000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:22.491000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:22.499000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:22.527000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:22.587000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:22.595000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:22.615000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:22.731000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:22.739000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:22.754000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:22.762000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:22.823000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:22.831000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:22.851000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:22.966000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:22.975000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:22.994000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:22.999000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:23.058000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:23.067000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:23.087000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:23.198000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:23.203000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:23.211000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:23.234000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:23.239000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:23.290000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:23.303000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:23.323000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:23.440000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:23.444000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:23.450000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:23.475000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:23.481000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:23.538000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:23.558000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:23.583000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:23.679000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:23.683000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:23.690000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:23.711000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:23.719000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:23.775000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:23.795000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:23.819000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:23.915000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:23.930000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:23.943000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:23.955000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:23.982000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:24.015000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:24.035000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:24.055000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:24.151000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:24.175000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:24.191000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:24.221000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:24.234000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:24.254000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:24.274000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:24.287000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:24.387000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:24.431000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:24.459000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:24.466000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:24.475000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:24.491000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:24.511000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:24.523000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:24.626000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:24.670000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:24.698000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:24.704000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:24.715000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:24.727000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:24.747000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:24.758000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:24.862000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:24.910000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:24.935000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:24.940000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:24.951000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:24.963000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:24.983000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:24.994000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:25.098000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:25.150000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:25.174000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:25.179000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:25.191000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:25.199000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:25.230000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:25.287000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:25.334000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:25.391000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:25.415000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:25.420000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:25.431000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:25.439000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:25.466000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:25.531000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:25.634000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:25.639000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:25.654000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:25.660000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:25.672000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:25.678000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:25.702000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:25.771000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:25.874000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:25.878000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:25.890000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:25.896000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:25.911000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:25.917000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:25.938000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:26.011000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:26.114000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:26.119000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:26.126000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:26.132000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:26.151000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:26.174000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:26.216000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:26.255000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:26.355000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:26.367000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:26.372000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:26.391000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:26.410000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:26.423000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:26.451000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:26.491000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:26.594000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:26.603000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:26.610000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:26.631000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:26.646000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:26.662000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:26.687000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:26.731000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:26.834000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:26.839000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:26.846000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:26.871000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:26.882000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:26.902000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:26.923000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:26.967000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:27.075000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:27.080000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:27.085000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:27.107000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:27.142000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:27.159000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:27.178000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:27.203000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:27.318000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:27.323000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:27.328000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:27.347000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:27.382000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:27.395000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:27.414000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:27.439000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:27.558000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:27.563000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:27.568000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:27.587000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:27.622000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:27.635000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:27.650000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:27.679000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:27.798000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:27.806000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:27.831000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:27.862000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:27.883000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:27.888000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:27.895000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:27.932000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:28.042000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:28.106000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:28.110000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:28.122000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:28.128000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:28.135000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:28.143000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:28.171000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:28.282000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:28.342000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:28.350000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:28.362000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:28.368000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:28.374000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:28.383000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:28.407000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:28.522000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:28.578000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:28.590000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:28.599000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:28.606000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:28.612000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:28.623000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:28.643000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:28.762000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:28.814000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:28.830000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:28.838000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:28.843000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:28.850000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:28.863000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:28.879000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:29.002000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:29.050000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:29.071000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:29.079000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:29.084000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:29.091000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:29.103000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:29.187000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:29.291000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:29.311000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:29.317000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:29.323000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:29.329000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:29.339000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:29.347000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:29.426000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:29.531000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:29.551000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:29.557000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:29.563000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:29.574000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:29.583000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:29.589000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:29.666000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:29.767000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:29.787000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:29.795000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:29.800000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:29.813000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:29.822000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:29.830000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:29.919000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:30.003000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:30.023000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:30.031000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:30.036000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:30.054000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:30.074000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:30.130000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:30.162000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:30.247000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:30.263000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:30.271000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:30.293000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:30.314000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:30.343000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:30.370000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:30.402000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:30.487000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:30.503000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:30.508000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:30.533000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:30.554000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:30.583000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:30.610000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:30.642000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:30.723000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:30.743000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:30.748000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:30.773000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:30.794000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:30.823000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:30.850000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:30.882000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:30.963000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:30.983000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:31.013000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:31.034000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:31.055000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:31.067000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:31.090000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:31.122000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:31.203000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:31.223000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:31.261000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:31.286000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:31.292000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:31.307000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:31.342000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:31.370000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:31.439000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:31.459000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:31.501000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:31.527000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:31.535000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:31.548000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:31.582000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:31.610000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:31.675000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:31.695000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:31.754000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:31.763000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:31.786000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:31.793000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:31.823000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:31.851000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:31.939000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:31.983000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:32.003000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:32.035000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:32.040000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:32.066000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:32.094000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:32.179000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:32.223000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:32.238000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:32.275000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:32.311000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:32.339000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:32.419000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:32.462000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:32.478000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:32.514000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:32.557000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:32.580000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:32.659000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:32.703000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:32.723000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:32.759000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:32.804000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:32.828000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:32.907000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:32.947000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:32.967000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:33.007000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:33.052000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:33.076000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:33.193000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:33.207000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:33.239000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:33.255000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:33.299000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:33.439000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:33.446000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:33.486000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:33.505000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:33.539000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:33.682000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:33.687000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:33.730000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:33.746000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:33.775000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:33.923000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:33.928000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:33.971000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:33.991000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:34.012000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:34.167000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:34.172000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:34.219000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:34.239000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:34.411000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:34.416000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:34.463000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:34.654000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:34.659000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:34.706000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:34.895000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:34.901000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:34.951000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:35.147000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:35.199000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:35.227000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:35.391000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:35.449000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:35.470000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:35.630000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:35.692000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:35.935000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:36.179000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:36.427000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:36.675000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:36.922000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0608 ms 100.0% 
  triton_mm_81 0.1041 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1055 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1103 ms 55.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1140 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1143 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1156 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1189 ms 51.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_90 0.1551 ms 39.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_99 0.1576 ms 38.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6228 seconds and 0.3358 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0609 ms 100.0% 
  triton_mm_81 0.1039 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1058 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1134 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1147 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1149 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1163 ms 52.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1209 ms 50.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1569 ms 38.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_98 0.1571 ms 38.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6802 seconds and 0.3455 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_81 0.1035 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1054 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1117 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_100 0.1137 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1137 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_91 0.1150 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1197 ms 50.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_90 0.1560 ms 38.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_99 0.1563 ms 38.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6477 seconds and 0.3268 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0609 ms 100.0% 
  triton_mm_81 0.1041 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1057 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1131 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_100 0.1153 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1156 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_91 0.1164 ms 52.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1204 ms 50.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1567 ms 38.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_98 0.1569 ms 38.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6305 seconds and 0.3345 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_81 0.1041 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1054 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1134 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_100 0.1158 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1159 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_91 0.1162 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1210 ms 50.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1569 ms 39.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_98 0.1570 ms 39.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6512 seconds and 0.3370 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0608 ms 100.0% 
  triton_mm_81 0.1037 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1058 ms 57.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1115 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1159 ms 52.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1160 ms 52.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1164 ms 52.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1204 ms 50.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1569 ms 38.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_98 0.1571 ms 38.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6129 seconds and 0.3390 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_81 0.1036 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1059 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1107 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1152 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1154 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1163 ms 52.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1191 ms 50.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_90 0.1563 ms 38.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_99 0.1567 ms 38.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5516 seconds and 0.3502 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_81 0.1041 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1053 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1152 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1171 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_101 0.1172 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1173 ms 52.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_92 0.1248 ms 49.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_98 0.1575 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1578 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6513 seconds and 0.3341 seconds precompiling for 27 choices
Capturing batches (bs=32 avail_mem=42.10 GB):  87%| | 45/52 [01:44<02:56, 25.15s/it]Capturing batches (bs=24 avail_mem=41.50 GB):  87%| | 45/52 [01:44<02:56, 25.15s/it][rank3]:W1023 11:03:49.981000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:50.055000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:50.140000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:50.157000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:50.215000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:50.316000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:50.345000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:50.420000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:50.502000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:50.523000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:50.577000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:50.680000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:50.793000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:50.804000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:50.811000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:50.865000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:50.879000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:50.887000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:50.966000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:50.981000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:50.993000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:51.141000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:51.215000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:51.318000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:51.666000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:51.837000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:52.034000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:52.181000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:52.460000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:52.488000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:52.504000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:52.813000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:53.230000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:53.308000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:53.408000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:53.566000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:53.641000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:53.721000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:53.739000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:53.797000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:03:53.878000 284 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank6]:W1023 11:03:53.895000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:03:54.212000 286 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank5]:W1023 11:03:54.302000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:54.376000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:03:54.376000 289 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank5]:W1023 11:03:54.384000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:54.454000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:54.495000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:03:54.559000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:54.585000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:54.604000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:54.664000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:54.680000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:54.765000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:03:54.778000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:54.882000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:03:54.957000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:03:54.971000 288 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank7]:W1023 11:03:55.026000 290 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank0]:W1023 11:03:55.057000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:03:55.235000 285 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank4]:W1023 11:03:55.244000 287 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank0]:W1023 11:03:55.538000 283 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_111 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_107 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_119 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_115 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_127 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_129 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0064 ms 98.1% 
  triton_bmm_126 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_105 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2408 seconds and 0.3714 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_111 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_115 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_107 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_119 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_110 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_126 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_106 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_120 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_129 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0064 ms 98.1% 
SingleProcess AUTOTUNE benchmarking takes 5.2705 seconds and 0.3787 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_117 0.0065 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_111 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_113 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_115 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_119 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_123 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_107 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_109 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_110 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3231 seconds and 0.3690 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_109 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_107 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_111 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_105 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_110 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_117 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_119 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_120 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_123 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_106 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3096 seconds and 0.3705 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_115 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_109 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_110 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_111 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_119 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_106 0.0063 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_107 0.0063 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_113 0.0063 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0063 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_121 0.0063 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2956 seconds and 0.3724 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_111 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_107 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_110 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_119 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_109 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_115 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_113 0.0063 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_120 0.0063 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_127 0.0063 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2900 seconds and 0.3694 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_109 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_119 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_115 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_127 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0065 ms 98.8% 
  triton_bmm_107 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_113 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_123 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_105 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.3113 seconds and 0.3777 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_111 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 100.0% 
  triton_bmm_110 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_109 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_115 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_119 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_107 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_105 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_106 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_117 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3279 seconds and 0.3772 seconds precompiling for 27 choices
[rank1]:W1023 11:04:05.012000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:05.510000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:05.514000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:06.007000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:06.030000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:06.458000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:06.494000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:06.524000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:06.530000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:06.864000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:06.952000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:06.984000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:07.024000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:07.359000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:07.507000 284 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank5]:W1023 11:04:07.536000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:08.059000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:08.364000 286 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank6]:W1023 11:04:08.814000 289 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank2]:W1023 11:04:10.084000 285 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank5]:W1023 11:04:10.488000 288 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank4]:W1023 11:04:10.956000 287 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank7]:W1023 11:04:10.959000 290 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank0]:W1023 11:04:11.607000 283 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_133 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0075 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0078 ms 83.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0084 ms 77.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0085 ms 76.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0087 ms 75.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_152 0.0088 ms 73.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0091 ms 71.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_153 0.0094 ms 69.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.1805 seconds and 0.5379 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0067 ms 100.0% 
  triton_bmm_133 0.0067 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0073 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0078 ms 85.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0082 ms 81.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0085 ms 78.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0087 ms 76.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_150 0.0088 ms 75.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0088 ms 75.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_155 0.0092 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1514 seconds and 0.5186 seconds precompiling for 27 choices
[rank1]:W1023 11:04:14.167000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:14.241000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:14.289000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_133 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0073 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0078 ms 82.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0084 ms 76.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0085 ms 76.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_141 0.0087 ms 74.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0088 ms 73.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_152 0.0089 ms 72.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_135 0.0093 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2168 seconds and 0.5434 seconds precompiling for 27 choices
[rank3]:W1023 11:04:14.989000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:15.064000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:15.113000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x24x512, 16x512x128)
  triton_bmm_133 0.0066 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0067 ms 98.8% 
  triton_bmm_145 0.0074 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0077 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_137 0.0083 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_143 0.0084 ms 79.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_152 0.0090 ms 73.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_150 0.0091 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_155 0.0093 ms 70.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_136 0.0095 ms 69.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.1841 seconds and 0.5250 seconds precompiling for 27 choices
[rank6]:W1023 11:04:15.981000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:16.056000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:16.104000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_133 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0074 ms 87.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0078 ms 83.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0082 ms 79.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0085 ms 76.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0086 ms 75.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0087 ms 74.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0089 ms 73.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_155 0.0093 ms 69.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1449 seconds and 0.5194 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x512, 16x512x128)
  triton_bmm_133 0.0069 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0070 ms 98.3% 
  triton_bmm_145 0.0073 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0079 ms 86.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0081 ms 84.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0084 ms 81.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_141 0.0087 ms 78.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0087 ms 78.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_152 0.0090 ms 76.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_155 0.0093 ms 73.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2402 seconds and 0.5345 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0067 ms 100.0% 
  triton_bmm_133 0.0067 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0072 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0077 ms 86.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0081 ms 81.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0084 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0085 ms 77.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0087 ms 76.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0089 ms 74.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_135 0.0092 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2695 seconds and 0.5434 seconds precompiling for 27 choices
[rank2]:W1023 11:04:16.860000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:16.936000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:16.985000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:17.084000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:17.159000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:17.207000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_133 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0072 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0078 ms 83.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0083 ms 78.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0085 ms 76.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_141 0.0088 ms 74.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0090 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_150 0.0090 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_135 0.0094 ms 69.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2326 seconds and 0.5136 seconds precompiling for 27 choices
[rank7]:W1023 11:04:17.730000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:17.804000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:17.852000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:18.248000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:18.323000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:18.372000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:18.744000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:18.819000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:18.868000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:20.068000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:20.143000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:20.167000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:20.242000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:20.243000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:20.345000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:21.914000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:21.990000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:22.091000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:22.929000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:23.006000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:23.107000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:23.117000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:23.192000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:23.291000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:23.406000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:23.483000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:23.558000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:23.659000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:23.662000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:23.715000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:23.789000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:23.814000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:23.887000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:23.906000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:24.058000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:24.298000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:24.762000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:24.837000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:24.863000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:24.949000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:25.099000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:25.335000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:25.516000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:25.591000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:25.689000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:25.781000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:25.857000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:25.956000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:26.044000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:26.292000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:26.297000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:26.416000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:26.550000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:26.555000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:26.660000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:26.802000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:26.903000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:26.932000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:27.007000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:27.051000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:27.110000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:27.295000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:27.536000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:28.038000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:28.102000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:28.114000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:28.213000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:28.276000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:28.341000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:28.352000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:28.451000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:28.502000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:28.578000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:28.578000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:28.681000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:29.144000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:29.221000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:29.320000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:30.186000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:30.262000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:30.361000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_159 0.0317 ms 29.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0399 ms 23.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0453 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0488 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0602 ms 15.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0680 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_178 0.0692 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0706 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_179 0.0708 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5450 seconds and 0.3784 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_159 0.0291 ms 32.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0403 ms 23.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0452 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0487 ms 19.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0587 ms 16.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0678 ms 14.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_162 0.0688 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.0693 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0706 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5304 seconds and 0.3778 seconds precompiling for 27 choices
[rank1]:W1023 11:04:33.149000 284 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec9a89bd830>
[rank1]:W1023 11:04:33.288000 284 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec9a89bd6e0>
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:04:33 TP1] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_159 0.0284 ms 33.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0402 ms 23.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0452 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0485 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0587 ms 16.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0677 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_162 0.0687 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.0691 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0706 ms 13.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5314 seconds and 0.3903 seconds precompiling for 27 choices
[rank3]:W1023 11:04:33.560000 286 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f078774df20>
[rank3]:W1023 11:04:33.583000 286 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f078c800ae0>
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:04:33 TP3] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_159 0.0294 ms 31.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0403 ms 23.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0453 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0488 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0590 ms 15.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0683 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_162 0.0690 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.0697 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0707 ms 13.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5677 seconds and 0.3713 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_159 0.0315 ms 30.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0409 ms 23.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0453 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0519 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0635 ms 15.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_178 0.0704 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0707 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_181 0.0710 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_179 0.0714 ms 13.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5475 seconds and 0.3799 seconds precompiling for 27 choices
[rank1]:W1023 11:04:34.782000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:34.990000 289 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef416a31e00>
[rank6]:W1023 11:04:35.013000 289 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef416a319b0>
[rank1]:W1023 11:04:35.030000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_159 0.0308 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0411 ms 22.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0451 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0522 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0642 ms 14.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_170 0.0707 ms 13.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.0716 ms 13.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_162 0.0725 ms 12.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_181 0.0727 ms 12.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.6020 seconds and 0.3723 seconds precompiling for 27 choices
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:04:35 TP6] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1023 11:04:35.286000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:35.475000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:35.530000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_159 0.0281 ms 33.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0402 ms 23.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0451 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0485 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0586 ms 16.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0681 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_162 0.0685 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.0695 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0704 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5161 seconds and 0.3933 seconds precompiling for 27 choices
[rank3]:W1023 11:04:35.722000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:35.774000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:35.957000 288 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ecd0c7f9ef0>
[rank3]:W1023 11:04:35.966000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:35.980000 288 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ecd0c7fa130>
[rank1]:W1023 11:04:36.018000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:04:36 TP5] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1023 11:04:36.184000 285 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec47f4f18f0>
[rank2]:W1023 11:04:36.207000 285 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec47f4f1860>
[rank3]:W1023 11:04:36.211000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:36.262000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:04:36 TP2] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1023 11:04:36.352000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:36.458000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:36.470000 287 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f607982de60>
[rank4]:W1023 11:04:36.493000 287 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f607982dfe0>
[rank1]:W1023 11:04:36.506000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:04:36 TP4] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1023 11:04:36.604000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_159 0.0290 ms 33.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0403 ms 23.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0452 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0488 ms 19.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0600 ms 16.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0681 ms 14.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_162 0.0689 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.0695 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0708 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5312 seconds and 0.3740 seconds precompiling for 27 choices
[rank3]:W1023 11:04:36.702000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:36.750000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:36.848000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:36.946000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:36.994000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:37.017000 290 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9d826e5aa0>
[rank7]:W1023 11:04:37.040000 290 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9d826e5a10>
[rank6]:W1023 11:04:37.095000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:04:37 TP7] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1023 11:04:37.204000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:37.240000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:37.295000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:37.339000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:37.448000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:37.495000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:37.543000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:37.564000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:37.591000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:37.692000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:37.739000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:37.787000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:37.812000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:37.835000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:37.936000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:37.975000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:37.984000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:38.031000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:38.051000 283 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dfcdd5740>
[rank2]:W1023 11:04:38.056000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:38.074000 283 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dfcdd56b0>
[rank6]:W1023 11:04:38.083000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:04:38 TP0] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1023 11:04:38.183000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:38.224000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:38.230000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:38.276000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:38.303000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:38.328000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:38.356000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:38.427000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:38.467000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:38.478000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:38.520000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:38.550000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:38.572000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:38.600000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:38.670000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:38.711000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:38.726000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:38.763000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:38.798000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:38.820000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:38.840000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:38.914000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:38.957000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:38.975000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:39.008000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:39.046000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:39.064000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:39.076000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:39.174000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:39.199000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:39.222000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:39.247000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:39.294000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:39.308000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:39.316000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:39.398000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:39.418000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:39.439000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:39.466000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:39.487000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:39.538000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:39.552000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:39.558000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:39.642000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:39.663000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:39.679000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:39.710000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:39.728000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:39.782000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:39.796000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:39.802000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:39.882000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:39.907000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:39.919000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:39.954000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:39.968000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:40.027000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:40.040000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:40.047000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:40.122000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:40.151000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:40.160000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:40.208000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:40.278000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:40.286000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:40.292000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:40.334000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:40.366000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:40.399000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:40.405000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:40.448000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:40.526000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:40.533000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:40.539000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:40.582000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:40.606000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:40.643000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:40.649000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:40.692000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:40.774000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:40.781000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:40.787000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:40.830000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:40.846000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:40.887000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:40.893000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:40.936000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:41.022000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:41.029000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:41.035000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:41.078000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:41.086000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:41.135000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:41.141000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:41.179000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:41.270000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:41.277000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:41.282000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:41.326000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:41.331000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:41.379000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:41.385000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:41.424000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:41.519000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:41.528000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:41.575000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:41.579000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:41.632000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:41.656000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:41.671000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:41.763000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:41.771000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:41.776000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:41.822000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:41.827000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:41.876000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:41.904000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:41.915000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:42.011000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:42.016000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:42.022000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:42.066000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:42.082000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:42.116000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:42.152000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:42.159000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:42.252000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:42.266000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:42.271000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:42.306000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:42.330000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:42.360000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:42.400000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:42.495000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:42.514000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:42.519000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:42.539000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:42.546000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:42.578000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:42.604000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:42.652000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:42.739000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:42.762000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:42.767000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:42.786000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:42.793000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:42.827000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:42.844000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:42.901000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:42.984000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:43.023000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:43.030000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:43.044000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:43.078000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:43.156000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:43.161000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:43.224000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:43.230000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:43.274000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:43.279000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:43.296000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:43.326000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:43.408000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:43.413000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:43.471000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:43.514000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:43.527000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:43.548000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:43.574000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:43.608000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:43.660000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:43.665000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:43.719000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:43.754000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:43.774000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:43.796000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:43.822000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:43.852000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:43.915000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:43.920000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:43.963000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:44.006000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:44.035000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:44.044000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:44.082000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:44.096000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:44.164000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:44.170000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:44.207000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:44.250000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:44.282000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:44.292000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:44.330000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:44.340000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:44.412000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:44.418000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:44.451000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:44.490000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:44.531000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:44.540000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:44.580000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:44.587000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:44.660000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:44.675000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:44.696000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:44.787000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:44.794000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:44.831000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:44.836000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:44.878000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:44.908000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:44.927000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:44.939000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:45.031000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:45.042000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:45.075000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:45.086000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:45.122000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:45.155000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:45.175000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:45.187000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:45.279000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:45.290000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:45.323000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:45.334000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:45.366000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:45.404000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:45.423000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:45.432000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:45.523000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:45.538000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:45.567000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:45.582000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:45.610000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:45.652000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:45.670000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:45.677000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:45.771000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:45.786000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:45.811000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:45.830000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:45.854000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:45.899000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:45.918000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:45.925000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:46.019000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:46.034000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:46.056000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:46.078000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:46.098000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:46.148000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:46.166000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:46.173000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:46.266000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:46.283000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:46.299000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:46.326000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:46.342000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:46.396000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:46.417000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:46.422000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:46.515000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:46.534000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:46.548000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:46.574000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:46.586000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:46.648000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:46.665000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:46.671000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:46.764000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:46.787000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:46.796000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:46.834000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:46.896000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:46.911000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:46.922000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:46.990000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:47.011000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:47.040000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:47.051000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:47.090000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:47.143000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:47.155000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:47.170000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:47.242000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:47.259000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:47.284000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:47.298000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:47.334000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:47.399000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:47.405000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:47.418000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:47.494000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:47.507000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:47.528000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:47.547000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:47.578000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:47.647000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:47.653000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:47.666000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:47.746000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:47.755000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:47.772000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:47.795000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:47.822000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:47.895000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:47.901000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:47.914000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:47.998000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:48.005000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:48.016000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:48.042000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:48.066000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:48.152000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:48.163000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:48.255000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:48.262000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:48.269000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:48.296000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:48.314000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:48.399000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:48.414000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:48.447000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:48.511000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:48.517000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:48.523000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:48.548000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:48.574000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:48.647000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:48.666000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:48.698000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:48.756000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:48.763000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:48.774000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:48.796000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:48.818000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:48.896000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:48.914000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:48.950000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:49.004000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:04:49.038000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:49.052000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:49.074000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:49.147000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:49.164000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:49.169000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:49.203000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:49.252000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:49.308000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:49.318000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:49.395000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:49.415000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:49.420000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:49.455000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:49.500000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:49.562000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:49.570000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:49.672000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:04:49.711000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:49.752000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:49.792000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:49.812000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:49.823000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:49.831000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:49.929000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:50.048000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:50.062000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:50.083000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:50.092000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:50.188000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:50.208000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:50.299000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:50.310000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:50.343000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:50.350000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:50.443000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:50.456000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:50.548000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:50.555000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:50.599000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:04:50.606000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:50.704000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:50.713000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:50.804000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:50.809000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:50.855000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:50.959000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:50.965000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:51.059000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:51.065000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:51.106000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:51.212000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:51.218000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:51.316000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:51.359000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:51.455000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:51.462000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:04:51.476000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:51.568000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:51.615000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:51.708000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:51.723000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:51.820000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:04:51.870000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:51.958000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:51.977000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:04:52.067000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:52.202000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:52.224000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:52.454000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:04:52.478000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:52.719000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:52.975000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:53.226000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:04:53.496000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0609 ms 100.0% 
  triton_mm_185 0.1029 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1059 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1134 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1143 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1151 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1158 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1207 ms 50.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1563 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1564 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6856 seconds and 0.3524 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0609 ms 100.0% 
  triton_mm_185 0.1027 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1055 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1127 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1142 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1147 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1155 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1211 ms 50.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1555 ms 39.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1557 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6346 seconds and 0.3697 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_185 0.1028 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1053 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1136 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1146 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1151 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1158 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1216 ms 50.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1562 ms 39.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1564 ms 39.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6385 seconds and 0.3532 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_185 0.1026 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1051 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_205 0.1148 ms 53.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1150 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_204 0.1152 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1156 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1203 ms 51.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1560 ms 39.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1563 ms 39.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6630 seconds and 0.3594 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_185 0.1024 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1056 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1116 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1141 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.1144 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_204 0.1145 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.1209 ms 50.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1560 ms 38.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1561 ms 38.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6417 seconds and 0.3694 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0612 ms 100.0% 
  triton_mm_185 0.1026 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1057 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1113 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1146 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1155 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1156 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1209 ms 50.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1561 ms 39.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1564 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6341 seconds and 0.3667 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0608 ms 100.0% 
  triton_mm_185 0.1025 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1055 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1106 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1145 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1147 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1152 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1188 ms 51.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1556 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_194 0.1559 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.6539 seconds and 0.3737 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0609 ms 100.0% 
  triton_mm_185 0.1028 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1053 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1106 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1155 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_205 0.1157 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1162 ms 52.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.1206 ms 50.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_194 0.1567 ms 38.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_203 0.1575 ms 38.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6271 seconds and 0.3521 seconds precompiling for 27 choices
Capturing batches (bs=24 avail_mem=41.50 GB):  88%| | 46/52 [03:01<04:03, 40.59s/it]Capturing batches (bs=16 avail_mem=40.86 GB):  88%| | 46/52 [03:01<04:03, 40.59s/it][rank0]:W1023 11:05:06.650000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:06.661000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:06.709000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:06.726000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:06.738000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:06.784000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:06.829000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:06.841000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:06.846000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:06.887000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:06.919000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:07.008000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:07.021000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:07.083000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:07.110000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:07.185000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:07.187000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:07.292000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:07.307000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:07.360000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:07.384000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:07.437000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:07.491000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:07.543000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:08.343000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:08.356000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:08.410000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:08.534000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:08.709000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:08.798000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:09.009000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:09.053000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:10.028000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:10.104000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:10.203000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:10 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1023 11:05:10.352000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:10.428000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:10.528000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:10 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1023 11:05:10.660000 283 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank7]:W1023 11:05:10.669000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:10.746000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:10.781000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:10.839000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:10.849000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:10.852000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:10.864000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:10 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1023 11:05:10.921000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:10.928000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:10.968000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:10.989000 287 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank2]:W1023 11:05:11.022000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:11 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1023 11:05:11.030000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:11.031000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:11 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:11 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1023 11:05:11.112000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:11.219000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:11 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1023 11:05:11.320000 290 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank3]:W1023 11:05:11.454000 286 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank6]:W1023 11:05:11.506000 289 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank2]:W1023 11:05:11.509000 285 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank5]:W1023 11:05:11.569000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:11.657000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:11.705000 284 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank5]:W1023 11:05:11.765000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:11 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1023 11:05:12.233000 288 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
AUTOTUNE bmm(16x16x128, 16x128x512)
  bmm 0.0061 ms 100.0% 
  triton_bmm_212 0.0064 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_219 0.0064 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_224 0.0064 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_230 0.0064 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_208 0.0064 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_213 0.0064 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_215 0.0064 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_221 0.0064 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_223 0.0064 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8159 seconds and 0.1452 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_230 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_231 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_219 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_220 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_225 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_228 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_218 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_221 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_222 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7679 seconds and 0.1510 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_219 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_228 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_229 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_213 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_214 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_218 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_224 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_215 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_220 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_221 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8665 seconds and 0.1395 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_220 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_221 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 99.4% 
  triton_bmm_229 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_230 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_231 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_210 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_211 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_213 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_215 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8412 seconds and 0.1501 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_208 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_226 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_228 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_211 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_212 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_215 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 99.4% 
  triton_bmm_210 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_214 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_218 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8451 seconds and 0.1528 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_219 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_220 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_221 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_224 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_229 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_230 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_228 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_231 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_226 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_227 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8526 seconds and 0.1489 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_220 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_229 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_218 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_230 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_219 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_228 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0063 ms 98.8% 
  triton_bmm_227 0.0063 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_215 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_226 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8763 seconds and 0.1522 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_213 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_219 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_211 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_212 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_221 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_209 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_214 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_223 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0064 ms 98.7% 
  triton_bmm_215 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8263 seconds and 0.1577 seconds precompiling for 25 choices
[rank4]:W1023 11:05:20.867000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:21.365000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:21.446000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:21.473000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:21.721000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:21.736000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:21.936000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:21.978000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:22.201000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:22.228000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:22.239000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:22.552000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:22.645000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:22.709000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:23.071000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:23.163000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:23.794000 287 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank0]:W1023 11:05:24.277000 283 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank7]:W1023 11:05:24.607000 290 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank1]:W1023 11:05:24.667000 284 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank6]:W1023 11:05:24.876000 289 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank5]:W1023 11:05:25.875000 288 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank3]:W1023 11:05:25.949000 286 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank2]:W1023 11:05:26.593000 285 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x16x512, 16x512x128)
  triton_bmm_235 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 99.4% 
  triton_bmm_234 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_245 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_244 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_242 0.0071 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0073 ms 88.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_238 0.0082 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0083 ms 78.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_236 0.0083 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8475 seconds and 0.4232 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x512, 16x512x128)
  triton_bmm_234 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_235 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 99.4% 
  triton_bmm_245 0.0068 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_244 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0072 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0072 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_238 0.0080 ms 81.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0080 ms 81.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_236 0.0084 ms 77.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7575 seconds and 0.4305 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_234 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_235 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_245 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_244 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_242 0.0075 ms 86.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0075 ms 86.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_238 0.0081 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0081 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_255 0.0085 ms 75.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7625 seconds and 0.3949 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x512, 16x512x128)
  triton_bmm_234 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_235 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 98.2% 
  triton_bmm_244 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_245 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0070 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0071 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_238 0.0081 ms 79.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0081 ms 79.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_254 0.0084 ms 76.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8713 seconds and 0.3995 seconds precompiling for 25 choices
[rank4]:W1023 11:05:30.130000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_235 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_234 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_244 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_245 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0073 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0073 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_238 0.0081 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0081 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_237 0.0084 ms 77.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8320 seconds and 0.3960 seconds precompiling for 25 choices
[rank4]:W1023 11:05:30.206000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:30.305000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:30 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1023 11:05:30.527000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:30.603000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:30.702000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:30 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1023 11:05:30.775000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:30.851000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:30.949000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:31 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1023 11:05:31.061000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:31.138000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_235 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_234 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_244 0.0069 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_245 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_243 0.0072 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0074 ms 88.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_239 0.0080 ms 81.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_238 0.0081 ms 80.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_237 0.0085 ms 76.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8029 seconds and 0.4202 seconds precompiling for 25 choices
[rank6]:W1023 11:05:31.237000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x16x512, 16x512x128)
  triton_bmm_234 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_235 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0067 ms 95.2% 
  triton_bmm_244 0.0070 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_245 0.0070 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_243 0.0073 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0073 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_239 0.0083 ms 77.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_238 0.0083 ms 76.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_254 0.0084 ms 76.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8404 seconds and 0.4114 seconds precompiling for 25 choices
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:31 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1023 11:05:31.400000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:31.477000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:31.576000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:31 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE bmm(16x16x512, 16x512x128)
  triton_bmm_235 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_234 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0066 ms 96.3% 
  triton_bmm_245 0.0069 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_244 0.0069 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_242 0.0076 ms 83.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0077 ms 82.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_238 0.0079 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0079 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_255 0.0085 ms 74.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8019 seconds and 0.4015 seconds precompiling for 25 choices
[rank5]:W1023 11:05:32.057000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:32.135000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:32.234000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:32 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1023 11:05:32.299000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:32.377000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:32.477000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:32 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1023 11:05:33.319000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:33.398000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:33.499000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:33 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:34 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:34 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:34 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:35 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1023 11:05:35.368000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:35.443000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:35.493000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:35 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1023 11:05:35.658000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:35.733000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:35.782000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:35 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:36 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:36 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1023 11:05:36.630000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:36.650000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:36 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1023 11:05:36.705000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:36.724000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:36.754000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:36.773000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:36 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:36 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1023 11:05:37.347000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:37.421000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:37.469000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:37 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1023 11:05:37.552000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:37.628000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:37.677000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:37 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1023 11:05:37.909000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:37.983000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:38 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1023 11:05:38.031000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:38 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1023 11:05:38.261000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:38.507000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:38.752000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:39 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1023 11:05:39.226000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:39.278000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:39.353000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:39.402000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:39 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1023 11:05:39.482000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:39.519000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:39.726000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:39.768000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:39.906000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:40.011000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:40 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1023 11:05:40.154000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:40.349000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:40 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1023 11:05:40.408000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:40.416000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:40.424000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:40.523000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:40 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1023 11:05:40.632000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:40.675000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:40.744000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:40 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1023 11:05:40.888000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:40.929000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:40.993000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:41.140000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:41.252000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:41 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1023 11:05:41.318000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:41.404000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:41.503000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:41 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:41 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:41 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1023 11:05:41.774000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:41.851000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:41.957000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:42 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1023 11:05:42.030000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:42.107000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:42.207000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:42 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1023 11:05:42.420000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:42.501000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:42.585000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:42.593000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:42.603000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:42 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1023 11:05:42.673000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:42.775000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:42 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1023 11:05:42.847000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:42.853000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:42.929000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:43.031000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:43 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1023 11:05:43.103000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:43 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1023 11:05:44.749000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:44.825000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:44.923000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:05:44 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_269 0.0331 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_259 0.0331 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_268 0.0331 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_258 0.0332 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.0498 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0503 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_276 0.0515 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0515 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_263 0.0599 ms 15.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1096 seconds and 0.2371 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_259 0.0296 ms 32.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_258 0.0308 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_269 0.0309 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_268 0.0309 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_267 0.0464 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0465 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0505 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0507 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_262 0.0550 ms 17.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9967 seconds and 0.2149 seconds precompiling for 25 choices
[rank4]:W1023 11:05:47.379000 287 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f607982de60>
[rank4]:W1023 11:05:47.402000 287 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f607982dfe0>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:05:47 TP4] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_269 0.0309 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_259 0.0311 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_268 0.0312 ms 30.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_258 0.0319 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.0466 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0467 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0518 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0518 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_262 0.0547 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0103 seconds and 0.2168 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_269 0.0312 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_268 0.0313 ms 30.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_259 0.0352 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_258 0.0357 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_277 0.0516 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0519 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_266 0.0522 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_267 0.0524 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_279 0.0623 ms 15.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0782 seconds and 0.2418 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0097 ms 100.0% 
  triton_mm_268 0.0306 ms 31.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_269 0.0306 ms 31.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_259 0.0311 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_258 0.0322 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.0464 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0465 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_276 0.0507 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0512 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_263 0.0538 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0555 seconds and 0.2179 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_259 0.0304 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_269 0.0307 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_268 0.0311 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_258 0.0318 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.0463 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0464 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0513 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0514 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_263 0.0541 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0604 seconds and 0.2265 seconds precompiling for 25 choices
[rank0]:W1023 11:05:48.658000 283 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dfcdd5740>
[rank0]:W1023 11:05:48.681000 283 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dfcdd56b0>
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_259 0.0299 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_268 0.0309 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_269 0.0309 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_258 0.0317 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.0463 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0463 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_276 0.0509 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0516 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_263 0.0540 ms 17.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0960 seconds and 0.2173 seconds precompiling for 25 choices
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:05:48 TP0] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1023 11:05:48.841000 289 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef416a31e00>
[rank6]:W1023 11:05:48.864000 289 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef416a319b0>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:05:49 TP6] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1023 11:05:49.193000 284 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec9a89bd830>
[rank1]:W1023 11:05:49.417000 284 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec9a89bd6e0>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:05:49 TP1] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1023 11:05:49.548000 286 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f078774df20>
[rank3]:W1023 11:05:49.571000 286 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f078c800ae0>
[rank7]:W1023 11:05:49.706000 290 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9d826e5aa0>
[rank7]:W1023 11:05:49.729000 290 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9d826e5a10>
[rank4]:W1023 11:05:49.792000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:05:49 TP3] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:05:50 TP7] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1023 11:05:50.035000 288 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ecd0c7f9ef0>
[rank4]:W1023 11:05:50.048000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:50.058000 288 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ecd0c7fa130>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:05:50 TP5] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1023 11:05:50.300000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:50.360000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:50.392000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:50.552000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0098 ms 100.0% 
  triton_mm_269 0.0325 ms 30.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_268 0.0325 ms 30.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_259 0.0341 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_258 0.0344 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_277 0.0495 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0497 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_267 0.0500 ms 19.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0502 ms 19.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_263 0.0591 ms 16.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0468 seconds and 0.2164 seconds precompiling for 25 choices
[rank0]:W1023 11:05:50.612000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:50.648000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:50.779000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:50.800000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:50.864000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:50.906000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:51.034000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:51.048000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:51.114000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:51.127000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:51.156000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:51.289000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:51.294000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:51.301000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:51.366000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:51.383000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:51.409000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:51.544000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:51.550000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:51.557000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:51.600000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:51.618000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:51.635000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:51.664000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:51.714000 285 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec47f4f18f0>
[rank2]:W1023 11:05:51.737000 285 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec47f4f1860>
[rank7]:W1023 11:05:51.796000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:05:51 TP2] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1023 11:05:51.806000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:51.813000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:51.852000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:51.866000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:51.887000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:51.916000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:52.044000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:52.059000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:52.065000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:52.100000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:52.114000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:52.139000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:52.168000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:52.300000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:52.310000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:52.317000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:52.352000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:52.362000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:52.392000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:52.421000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:52.552000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:52.568000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:52.574000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:52.604000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:52.614000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:52.647000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:52.672000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:52.800000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:52.816000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:52.830000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:52.856000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:52.872000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:52.899000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:52.920000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:53.048000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:53.063000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:53.083000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:53.108000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:53.122000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:53.151000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:53.170000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:53.296000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:53.311000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:53.334000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:53.356000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:53.370000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:53.403000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:53.420000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:53.544000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:53.564000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:53.591000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:53.608000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:53.622000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:53.655000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:53.672000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:53.731000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:53.792000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:53.811000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:53.843000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:53.860000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:53.874000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:53.907000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:53.924000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:53.988000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:54.039000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:54.059000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:54.096000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:54.112000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:54.135000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:54.172000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:54.180000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:54.252000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:54.291000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:54.311000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:54.348000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:54.367000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:54.383000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:54.424000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:54.435000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:54.504000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:54.543000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:54.563000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:54.599000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:54.623000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:54.631000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:54.676000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:54.692000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:54.756000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:54.791000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:54.815000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:54.852000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:54.875000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:54.881000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:54.940000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:54.947000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:55.019000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:55.040000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:55.068000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:55.103000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:55.128000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:55.133000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:55.191000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:55.200000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:55.271000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:55.288000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:55.320000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:55.355000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:55.380000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:55.385000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:55.443000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:55.452000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:55.523000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:55.536000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:55.567000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:55.608000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:55.631000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:55.637000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:55.696000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:55.704000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:55.777000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:55.783000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:55.815000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:55.864000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:55.883000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:55.889000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:55.948000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:55.959000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:56.027000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:56.036000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:56.068000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:56.119000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:56.138000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:56.146000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:56.199000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:56.216000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:56.280000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:56.288000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:56.320000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:56.371000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:56.391000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:56.400000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:56.451000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:56.468000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:56.531000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:56.540000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:56.576000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:56.628000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:56.643000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:56.655000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:56.709000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:56.723000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:56.785000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:56.792000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:56.827000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:56.896000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:56.907000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:56.914000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:56.964000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:56.979000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:57.036000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:57.045000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:57.080000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:57.147000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:57.154000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:57.172000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:57.215000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:57.236000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:57.287000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:57.296000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:57.332000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:57.403000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:57.408000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:57.424000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:57.471000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:57.488000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:57.539000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:57.546000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:57.584000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:57.670000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:57.676000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:57.683000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:57.735000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:57.741000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:57.791000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:57.798000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:57.832000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:57.926000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:57.932000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:57.940000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:57.992000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:57.997000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:58.045000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:58.051000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:58.079000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:58.184000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:58.191000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:58.199000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:58.245000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:58.252000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:58.296000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:58.305000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:58.437000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:58.444000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:58.460000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:58.504000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:58.548000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:58.560000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:58.572000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:58.687000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:58.695000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:58.720000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:58.744000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:58.755000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:58.800000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:58.813000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:58.829000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:58.976000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:59.000000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:59.019000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:59.059000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:59.067000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:59.080000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:59.215000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:59.220000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:59.228000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:59.256000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:59.279000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:59.317000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:59.323000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:59.332000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:59.481000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:59.487000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:59.493000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:59.514000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:59.581000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:59.588000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:59.739000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:05:59.756000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:05:59.764000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:05:59.776000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:05:59.782000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:05:59.789000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:05:59.836000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:05:59.844000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:05:59.995000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:00.012000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:00.020000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:00.033000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:00.040000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:00.046000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:00.088000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:00.099000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:00.251000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:00.268000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:00.276000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:00.289000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:00.299000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:00.305000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:00.340000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:00.351000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:00.507000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:00.524000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:00.532000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:00.544000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:00.555000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:00.561000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:00.592000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:00.604000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:00.780000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:00.788000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:00.800000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:00.815000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:00.821000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:00.844000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:00.859000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:01.019000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:01.036000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:01.044000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:01.056000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:01.071000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:01.079000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:01.096000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:01.115000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:01.279000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:01.292000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:01.301000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:01.312000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:01.323000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:01.335000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:01.348000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:01.367000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:01.539000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:01.556000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:01.563000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:01.570000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:01.577000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:01.591000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:01.600000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:01.619000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:01.799000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:01.807000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:01.820000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:01.827000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:01.834000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:01.847000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:01.854000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:01.871000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:02.059000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:02.065000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:02.076000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:02.084000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:02.090000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:02.104000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:02.111000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:02.123000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:02.319000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:02.325000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:02.332000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:02.340000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:02.346000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:02.356000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:02.369000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:02.375000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:02.580000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:02.586000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:02.594000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:02.601000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:02.607000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:02.627000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:02.633000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:02.840000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:02.847000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:02.853000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:02.861000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:02.868000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:02.874000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:02.887000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:02.893000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:03.096000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:03.104000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:03.115000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:03.125000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:03.131000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:03.137000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:03.147000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:03.153000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:03.352000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:03.360000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:03.375000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:03.382000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:03.389000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:03.395000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:03.407000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:03.413000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:03.608000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:03.616000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:03.636000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:03.642000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:03.649000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:03.656000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:03.665000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:03.670000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:03.864000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:03.872000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:03.896000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:03.902000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:03.909000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:03.915000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:03.924000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:03.930000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:04.120000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:04.128000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:04.152000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:04.163000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:04.168000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:04.176000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:04.184000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:04.190000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:04.376000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:04.384000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:04.404000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:04.415000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:04.430000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:04.436000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:04.445000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:04.632000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:04.640000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:04.655000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:04.667000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:04.687000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:04.694000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:04.702000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:04.888000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:04.896000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:04.908000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:04.924000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:04.944000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:04.951000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:04.960000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:05.165000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:05.172000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:05.183000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:05.209000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:05.224000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:05.425000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:05.432000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:05.439000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:05.467000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:05.480000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:05.685000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:05.696000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:05.728000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:05.737000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:05.953000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:05.992000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:06.212000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:06.252000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:06.477000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:06.744000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:07.004000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:07.264000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:07.532000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:07.778000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:08.026000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:08.270000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_282 0.1036 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1037 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_292 0.1047 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1047 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_301 0.1092 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1093 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1129 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1129 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_287 0.1481 ms 41.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0259 seconds and 0.1851 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0610 ms 100.0% 
  triton_mm_282 0.1036 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1036 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_293 0.1046 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_292 0.1047 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1094 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1095 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1128 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1129 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_286 0.1487 ms 41.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0865 seconds and 0.1838 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_283 0.1036 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_282 0.1036 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_292 0.1044 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1044 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_301 0.1095 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1096 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_290 0.1133 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1135 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_287 0.1482 ms 41.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0378 seconds and 0.1816 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_283 0.1038 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_282 0.1039 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_292 0.1049 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1049 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_301 0.1096 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1097 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1132 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1134 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_286 0.1484 ms 41.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0475 seconds and 0.1880 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_282 0.1035 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1036 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_292 0.1045 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1046 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1090 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1091 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_291 0.1125 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1125 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_287 0.1476 ms 41.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0495 seconds and 0.1838 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0616 ms 100.0% 
  triton_mm_282 0.1036 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1037 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_293 0.1045 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_292 0.1047 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_300 0.1095 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1095 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1130 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1131 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_287 0.1483 ms 41.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0443 seconds and 0.1819 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_282 0.1036 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1037 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_292 0.1045 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1045 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1096 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1097 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_291 0.1131 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1133 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_286 0.1482 ms 41.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0166 seconds and 0.1842 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0610 ms 100.0% 
  triton_mm_282 0.1036 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1037 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_292 0.1046 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1046 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1098 ms 55.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1099 ms 55.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1129 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1129 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_287 0.1474 ms 41.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0359 seconds and 0.1943 seconds precompiling for 25 choices
Capturing batches (bs=16 avail_mem=40.86 GB):  90%| | 47/52 [04:16<04:13, 50.77s/it]Capturing batches (bs=12 avail_mem=40.27 GB):  90%| | 47/52 [04:16<04:13, 50.77s/it][rank5]:W1023 11:06:21.354000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:21.367000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:21.432000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:21.443000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:21.538000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:21.541000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:21.548000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:21.617000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:21.689000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:21.719000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:21.721000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:21.765000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:21.795000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:21.852000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:21.870000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:21.898000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:21.900000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:21.930000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:21.975000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:22.039000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:22.068000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:22.080000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:22.145000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:22.250000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:23.058000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:23.088000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:23.244000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:23.406000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:23.429000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:23.598000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:23.606000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:23.765000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:24.601000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:24.679000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:24.781000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:24.899000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:24.924000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:24.976000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:25.001000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:25.076000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:25.101000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:25.118000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:25.196000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:25.285000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:25.297000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:25.363000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:25.442000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:25.463000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:25.465000 288 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank6]:W1023 11:06:25.520000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:25.538000 283 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank7]:W1023 11:06:25.562000 290 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank3]:W1023 11:06:25.617000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:25.632000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:25.706000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:25.772000 284 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank3]:W1023 11:06:25.814000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:25.947000 285 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank6]:W1023 11:06:26.111000 289 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank3]:W1023 11:06:26.297000 286 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank4]:W1023 11:06:26.439000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:26.518000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:26.618000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:27.076000 287 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_315 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_316 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_317 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_320 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_321 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_323 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_325 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_307 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_326 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_314 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8842 seconds and 0.1339 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_314 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0063 ms 100.0% 
  triton_bmm_307 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_315 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_326 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_327 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_311 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_316 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_318 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_321 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8218 seconds and 0.1483 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_320 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_322 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_323 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_324 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_326 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_318 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_321 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_325 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_306 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_315 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8978 seconds and 0.1282 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_325 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_324 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_326 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_327 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_307 0.0063 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_315 0.0063 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_317 0.0063 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_306 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_314 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_318 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8396 seconds and 0.1308 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_314 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_325 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_324 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_311 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_315 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_316 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_317 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_318 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_319 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_322 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8454 seconds and 0.1289 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_315 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_322 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_317 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_320 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  bmm 0.0064 ms 99.4% 
  triton_bmm_314 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_316 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_318 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_319 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_321 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9269 seconds and 0.1296 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_310 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_315 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_320 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_314 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_316 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_323 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_308 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_309 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_311 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_319 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8520 seconds and 0.1403 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_310 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_306 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_307 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_311 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_316 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_308 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_309 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_314 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_315 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_318 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8802 seconds and 0.1432 seconds precompiling for 25 choices
[rank0]:W1023 11:06:35.575000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:36.085000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:36.230000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:36.305000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:36.602000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:36.618000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:36.724000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:36.809000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:36.818000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:37.103000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:37.106000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:37.114000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:37.318000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:37.355000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:37.604000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:37.857000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:38.373000 283 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank1]:W1023 11:06:39.044000 284 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank7]:W1023 11:06:39.129000 290 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank2]:W1023 11:06:39.684000 285 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank6]:W1023 11:06:39.804000 289 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank3]:W1023 11:06:40.889000 286 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank5]:W1023 11:06:41.547000 288 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank4]:W1023 11:06:43.157000 287 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_330 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_341 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_340 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0071 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0072 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_334 0.0080 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0081 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_351 0.0083 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7969 seconds and 0.4316 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_330 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_340 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0073 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0073 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_334 0.0081 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0081 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_350 0.0085 ms 77.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8083 seconds and 0.4116 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  triton_bmm_330 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 97.5% 
  triton_bmm_341 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_340 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_338 0.0069 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0069 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_335 0.0079 ms 79.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_334 0.0081 ms 78.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_350 0.0082 ms 77.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8334 seconds and 0.4224 seconds precompiling for 25 choices
[rank0]:W1023 11:06:44.556000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:44.632000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:44.681000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x12x512, 16x512x128)
  triton_bmm_330 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 96.9% 
  triton_bmm_340 0.0069 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0069 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0076 ms 83.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0076 ms 83.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_334 0.0079 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0079 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_350 0.0085 ms 74.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8077 seconds and 0.4117 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_330 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_340 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0069 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_339 0.0071 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0071 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_334 0.0081 ms 80.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0081 ms 80.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_350 0.0084 ms 77.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8624 seconds and 0.4193 seconds precompiling for 25 choices
[rank7]:W1023 11:06:45.451000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:45.526000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:45.575000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:45.654000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:45.731000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:45.780000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:45.973000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:46.050000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:46.100000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_330 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_340 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0071 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0073 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_334 0.0082 ms 79.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0083 ms 78.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_350 0.0083 ms 77.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7763 seconds and 0.4051 seconds precompiling for 25 choices
[rank6]:W1023 11:06:46.183000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:46.259000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:46.307000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_331 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_330 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_340 0.0070 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0070 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_339 0.0073 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0073 ms 88.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_334 0.0081 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0081 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_351 0.0085 ms 76.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8934 seconds and 0.4144 seconds precompiling for 25 choices
[rank3]:W1023 11:06:47.151000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:47.237000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:47.287000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:47.811000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:47.887000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:47.937000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_330 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_341 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_340 0.0069 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_338 0.0072 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0073 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_334 0.0083 ms 78.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0083 ms 78.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_350 0.0083 ms 78.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8688 seconds and 0.3934 seconds precompiling for 25 choices
[rank4]:W1023 11:06:49.369000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:49.445000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:49.493000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:49.960000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:50.039000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:50.144000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:50.866000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:50.944000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:51.047000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:51.181000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:51.259000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:51.373000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:51.438000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:51.515000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:51.618000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:52.285000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:52.288000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:52.363000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:52.366000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:52.472000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:52.758000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:53.084000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:53.162000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:53.263000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:53.608000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:53.862000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:54.118000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:54.520000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:54.782000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:54.856000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:55.040000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:55.107000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:55.115000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:55.374000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:55.379000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:55.454000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:55.533000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:55.636000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:55.639000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:55.685000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:55.699000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:55.776000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:06:55.876000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:55.949000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:56.208000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:56.247000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:56.512000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:56.662000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:56.737000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:56.769000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:56.791000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:06:56.836000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:56.864000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:56.952000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:57.028000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:06:57.054000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:57.285000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:57.291000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:57.373000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:06:57.483000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:57.690000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:57.767000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:06:57.866000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:58.450000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:58.528000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:06:58.630000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:58.778000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:58.808000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:58.855000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:06:58.954000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:59.065000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:06:59.328000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:00.947000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:01.023000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:01.121000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0097 ms 100.0% 
  triton_mm_365 0.0306 ms 31.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.0307 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_355 0.0328 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_354 0.0334 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_362 0.0461 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0464 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0512 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0513 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_359 0.0543 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0454 seconds and 0.2285 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_355 0.0302 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_364 0.0303 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.0303 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_354 0.0322 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_362 0.0459 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0461 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_373 0.0518 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0519 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_359 0.0538 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0956 seconds and 0.2273 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_365 0.0306 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.0306 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_355 0.0352 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_354 0.0357 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_363 0.0464 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_362 0.0471 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_372 0.0503 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0509 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_375 0.0621 ms 15.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1139 seconds and 0.2268 seconds precompiling for 25 choices
[rank0]:W1023 11:07:03.055000 283 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dfcdd5740>
[rank0]:W1023 11:07:03.078000 283 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dfcdd56b0>
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:07:03 TP0] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0100 ms 100.0% 
  triton_mm_364 0.0324 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_355 0.0325 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_365 0.0327 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_354 0.0336 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_362 0.0487 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0496 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_373 0.0505 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0508 ms 19.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_359 0.0596 ms 16.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0787 seconds and 0.2316 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_355 0.0303 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_365 0.0304 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.0304 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_354 0.0317 ms 29.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_362 0.0461 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0461 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0516 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0516 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.0541 ms 17.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0978 seconds and 0.2397 seconds precompiling for 25 choices
[rank7]:W1023 11:07:03.681000 290 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9d826e5aa0>
[rank7]:W1023 11:07:03.704000 290 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9d826e5a10>
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:07:03 TP7] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_365 0.0304 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_355 0.0305 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_364 0.0306 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_354 0.0325 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_362 0.0462 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0469 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_373 0.0513 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0514 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_358 0.0538 ms 17.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9597 seconds and 0.2312 seconds precompiling for 25 choices
[rank1]:W1023 11:07:04.356000 284 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec9a89bd830>
[rank1]:W1023 11:07:04.379000 284 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec9a89bd6e0>
[rank0]:W1023 11:07:04.419000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:07:04 TP1] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1023 11:07:04.473000 285 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec47f4f18f0>
[rank2]:W1023 11:07:04.496000 285 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec47f4f1860>
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:07:04 TP2] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_364 0.0306 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_355 0.0307 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_365 0.0307 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_354 0.0322 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_362 0.0461 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0462 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_373 0.0508 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0512 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_359 0.0540 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1049 seconds and 0.2208 seconds precompiling for 25 choices
[rank0]:W1023 11:07:04.674000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:04.897000 289 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef416a31e00>
[rank6]:W1023 11:07:04.920000 289 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef416a319b0>
[rank0]:W1023 11:07:04.930000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:07:04 TP6] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1023 11:07:05.188000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:05.446000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:05.511000 286 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f078774df20>
[rank3]:W1023 11:07:05.533000 286 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f078c800ae0>
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:07:05 TP3] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1023 11:07:05.640000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:05.710000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:05.787000 288 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ecd0c7f9ef0>
[rank5]:W1023 11:07:05.810000 288 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ecd0c7fa130>
[rank2]:W1023 11:07:05.835000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:07:05 TP5] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1023 11:07:05.883000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:05.896000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:05.966000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:06.099000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:06.147000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:06.154000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:06.223000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:06.359000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:06.407000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:06.414000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:06.483000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:06.619000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:06.668000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:06.673000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:06.692000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:06.738000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_355 0.0323 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_365 0.0324 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.0325 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_354 0.0330 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_363 0.0493 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_362 0.0496 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0508 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0515 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_358 0.0605 ms 15.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1249 seconds and 0.2268 seconds precompiling for 25 choices
[rank3]:W1023 11:07:06.843000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:06.879000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:06.925000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:06.935000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:06.953000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:06.999000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:07.103000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:07.152000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:07.158000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:07.180000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:07.199000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:07.213000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:07.258000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:07.367000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:07.415000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:07.422000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:07.436000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:07.459000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:07.468000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:07.521000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:07.627000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:07.675000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:07.682000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:07.692000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:07.719000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:07.726000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:07.778000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:07.887000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:07.939000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:07.945000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:07.953000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:07.979000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:07.993000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:08.034000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:08.142000 287 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f607982de60>
[rank3]:W1023 11:07:08.155000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:08.164000 287 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f607982dfe0>
[rank2]:W1023 11:07:08.199000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:08.208000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:08.216000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:07:08 TP4] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1023 11:07:08.244000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:08.253000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:08.290000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:08.415000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:08.459000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:08.469000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:08.475000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:08.508000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:08.514000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:08.554000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:08.691000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:08.720000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:08.732000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:08.742000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:08.770000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:08.775000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:08.810000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:08.967000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:08.988000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:08.993000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:09.001000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:09.024000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:09.035000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:09.066000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:09.231000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:09.249000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:09.255000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:09.262000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:09.284000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:09.295000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:09.322000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:09.495000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:09.508000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:09.515000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:09.522000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:09.529000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:09.544000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:09.559000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:09.578000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:09.760000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:09.767000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:09.777000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:09.784000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:09.792000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:09.805000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:09.823000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:10.024000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:10.030000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:10.038000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:10.049000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:10.054000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:10.065000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:10.087000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:10.166000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:10.281000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:10.296000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:10.302000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:10.310000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:10.320000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:10.328000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:10.349000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:10.426000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:10.536000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:10.556000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:10.568000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:10.579000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:10.591000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:10.599000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:10.611000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:10.686000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:10.797000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:10.825000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:10.843000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:10.855000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:10.865000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:10.875000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:10.947000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:11.060000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:11.092000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:11.111000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:11.119000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:11.133000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:11.140000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:11.148000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:11.206000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:11.321000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:11.353000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:11.376000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:11.384000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:11.393000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:11.405000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:11.471000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:11.581000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:11.617000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:11.644000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:11.670000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:11.739000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:11.768000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:11.845000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:11.884000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:11.911000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:11.936000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:11.968000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:11.987000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:12.003000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:12.035000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:12.104000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:12.148000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:12.175000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:12.200000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:12.228000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:12.251000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:12.262000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:12.299000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:12.360000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:12.409000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:12.444000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:12.461000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:12.492000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:12.516000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:12.522000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:12.575000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:12.616000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:12.669000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:12.715000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:12.722000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:12.757000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:12.783000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:12.788000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:12.839000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:12.873000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:12.983000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:12.991000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:13.029000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:13.047000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:13.053000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:13.104000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:13.133000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:13.257000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:13.273000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:13.296000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:13.315000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:13.321000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:13.371000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:13.393000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:13.521000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:13.536000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:13.560000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:13.583000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:13.589000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:13.596000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:13.639000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:13.648000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:13.780000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:13.800000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:13.824000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:13.851000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:13.857000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:13.864000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:13.904000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:13.911000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:14.040000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:14.064000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:14.088000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:14.115000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:14.123000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:14.130000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:14.160000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:14.175000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:14.300000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:14.328000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:14.352000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:14.379000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:14.387000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:14.395000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:14.416000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:14.439000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:14.560000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:14.592000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:14.612000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:14.643000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:14.651000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:14.659000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:14.672000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:14.703000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:14.820000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:14.856000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:14.872000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:14.907000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:14.915000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:14.923000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:14.930000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:14.967000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:15.081000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:15.121000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:15.133000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:15.170000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:15.179000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:15.187000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:15.231000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:15.349000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:15.393000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:15.400000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:15.431000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:15.443000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:15.451000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:15.495000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:15.503000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:15.613000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:15.660000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:15.667000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:15.695000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:15.707000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:15.715000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:15.760000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:15.769000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:15.877000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:15.928000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:15.935000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:15.971000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:15.983000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:15.991000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:16.027000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:16.035000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:16.141000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:16.196000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:16.203000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:16.234000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:16.247000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:16.259000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:16.291000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:16.299000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:16.401000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:16.460000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:16.467000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:16.499000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:16.511000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:16.523000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:16.555000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:16.563000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:16.665000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:16.724000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:16.731000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:16.763000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:16.775000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:16.787000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:16.819000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:16.827000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:16.925000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:16.988000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:16.995000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:17.026000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:17.039000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:17.051000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:17.083000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:17.091000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:17.188000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:17.252000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:17.259000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:17.291000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:17.303000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:17.315000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:17.348000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:17.354000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:17.453000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:17.516000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:17.523000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:17.555000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:17.567000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:17.579000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:17.608000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:17.619000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:17.712000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:17.780000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:17.788000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:17.819000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:17.831000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:17.847000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:17.869000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:17.895000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:17.976000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:18.044000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:18.051000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:18.084000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:18.096000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:18.113000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:18.127000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:18.164000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:18.239000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:18.307000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:18.315000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:18.348000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:18.360000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:18.377000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:18.387000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:18.428000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:18.508000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:18.573000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:18.581000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:18.607000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:18.623000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:18.641000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:18.649000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:18.691000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:18.773000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:18.837000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:18.844000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:18.871000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:18.887000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:18.907000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:18.915000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:18.961000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:19.040000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:19.100000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:19.107000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:19.136000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:19.152000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:19.172000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:19.179000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:19.228000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:19.304000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:19.363000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:19.371000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:19.400000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:19.416000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:19.437000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:19.443000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:19.492000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:19.569000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:19.628000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:19.640000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:19.691000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:19.704000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:19.712000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:19.767000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:19.833000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:19.892000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:19.904000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:19.959000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:19.966000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:19.979000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:20.035000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:20.097000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:20.156000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:20.168000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:20.224000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:20.230000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:20.247000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:20.303000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:20.361000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:20.425000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:20.434000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:20.489000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:20.504000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:20.520000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:20.582000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:20.632000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:20.699000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:20.705000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:20.751000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:20.776000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:20.792000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:20.852000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:20.967000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:20.973000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:21.012000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:21.039000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:21.055000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:21.115000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:21.236000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:21.243000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:21.272000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:21.319000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:21.511000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:21.518000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:21.536000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:21.600000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:21.783000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:21.790000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:21.801000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:21.868000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:22.064000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:22.070000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:22.145000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:22.335000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:22.341000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:22.608000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:22.871000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:23.140000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:23.407000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:23.671000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:23.938000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:24.202000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:24.462000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_379 0.1044 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_378 0.1044 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_388 0.1045 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1045 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1092 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1093 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_387 0.1125 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1126 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_383 0.1455 ms 42.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0062 seconds and 0.1936 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0612 ms 100.0% 
  triton_mm_379 0.1042 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_378 0.1043 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_388 0.1044 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1045 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1088 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1089 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1123 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1125 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_382 0.1445 ms 42.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0174 seconds and 0.1883 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_379 0.1042 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_378 0.1043 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_389 0.1043 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_388 0.1044 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1088 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1089 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_386 0.1122 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1122 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_382 0.1441 ms 42.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1003 seconds and 0.2038 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_379 0.1045 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_378 0.1045 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_388 0.1049 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1049 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1094 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1095 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_387 0.1129 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1130 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_382 0.1452 ms 42.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1074 seconds and 0.2056 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_379 0.1043 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_388 0.1043 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_378 0.1044 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_389 0.1045 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_397 0.1096 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1097 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1130 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1130 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_382 0.1452 ms 42.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0669 seconds and 0.2024 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_379 0.1044 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_378 0.1045 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_388 0.1045 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1045 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1096 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1098 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_387 0.1128 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1130 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_383 0.1449 ms 42.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0828 seconds and 0.2044 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_378 0.1043 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_379 0.1044 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_388 0.1045 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1045 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1097 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1098 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1128 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1130 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_382 0.1450 ms 42.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0608 seconds and 0.1914 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_379 0.1045 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_378 0.1045 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_389 0.1045 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_388 0.1046 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1102 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1102 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1134 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1135 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_382 0.1452 ms 42.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0859 seconds and 0.1976 seconds precompiling for 25 choices
Capturing batches (bs=12 avail_mem=40.27 GB):  92%|| 48/52 [05:31<03:53, 58.32s/it]Capturing batches (bs=8 avail_mem=39.69 GB):  92%|| 48/52 [05:31<03:53, 58.32s/it] [rank7]:W1023 11:07:37.357000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:37.433000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:37.436000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:37.513000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:37.538000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:37.559000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:37.619000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:37.638000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:37.745000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:37.865000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:37.942000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:37.957000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:38.019000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:38.036000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:38.050000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:38.096000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:38.143000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:38.145000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:38.203000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:38.225000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:38.334000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:38.479000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:38.558000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:38.668000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:39.071000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:39.144000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:39.285000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:39.577000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:39.683000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:39.738000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:39.866000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:40.201000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:40.832000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:40.856000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:40.911000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:40.935000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:41.015000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:41.037000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:41.223000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:41.304000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:41.407000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:41.483000 283 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank7]:W1023 11:07:41.502000 290 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank2]:W1023 11:07:41.534000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:41.622000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:41.734000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:41.780000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:41.859000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:41.892000 286 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank5]:W1023 11:07:41.963000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:42.059000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:42.096000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:42.141000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:42.186000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:42.223000 285 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank6]:W1023 11:07:42.244000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:42.289000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:42.443000 288 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank6]:W1023 11:07:42.727000 289 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank1]:W1023 11:07:42.772000 284 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank4]:W1023 11:07:43.032000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:43.110000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:43.209000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:43.667000 287 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_423 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0063 ms 100.0% 
  triton_bmm_412 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_413 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_415 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_419 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_420 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_421 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_401 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8602 seconds and 0.1364 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_421 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_405 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_406 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_412 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_418 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_423 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_403 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_415 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_402 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8703 seconds and 0.1579 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_411 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_412 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_420 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_410 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_421 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_413 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_418 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_406 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_415 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8421 seconds and 0.1469 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_418 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_419 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_411 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_412 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_410 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_403 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_406 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_416 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_404 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8155 seconds and 0.1387 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_421 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_405 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_413 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_412 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_423 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_402 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_403 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0064 ms 98.7% 
  triton_bmm_401 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8842 seconds and 0.1499 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_416 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_411 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_419 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_420 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0064 ms 99.4% 
  triton_bmm_406 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_410 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_401 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_415 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8450 seconds and 0.1551 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_404 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_407 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_410 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_411 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_413 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_419 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_402 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_412 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_422 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8122 seconds and 0.1486 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_406 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_407 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_415 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_402 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_410 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_412 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_414 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_421 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8844 seconds and 0.1429 seconds precompiling for 25 choices
[rank0]:W1023 11:07:51.511000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:51.886000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:52.017000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:52.199000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:52.390000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:52.659000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:07:52.719000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:52.923000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:07:53.160000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:53.415000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:07:53.431000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:07:53.576000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:53.708000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:07:53.926000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:07:53.938000 283 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank3]:W1023 11:07:54.091000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:07:54.211000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:07:54.792000 290 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank2]:W1023 11:07:55.038000 285 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank6]:W1023 11:07:55.796000 289 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank3]:W1023 11:07:56.426000 286 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank4]:W1023 11:07:56.501000 287 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank5]:W1023 11:07:57.165000 288 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank1]:W1023 11:07:57.488000 284 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x8x512, 16x512x128)
  triton_bmm_427 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_426 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0065 ms 96.9% 
  triton_bmm_437 0.0068 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_436 0.0069 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0071 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0072 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_430 0.0080 ms 79.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0080 ms 79.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_447 0.0082 ms 77.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8687 seconds and 0.4420 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x512, 16x512x128)
  triton_bmm_427 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_426 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0065 ms 97.5% 
  triton_bmm_437 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_436 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_434 0.0069 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0069 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_431 0.0078 ms 81.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_430 0.0078 ms 81.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_429 0.0081 ms 78.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8465 seconds and 0.3956 seconds precompiling for 25 choices
[rank0]:W1023 11:08:00.316000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x8x512, 16x512x128)
  triton_bmm_426 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_427 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 97.5% 
  triton_bmm_437 0.0069 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_436 0.0069 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_430 0.0078 ms 81.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0078 ms 81.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_434 0.0079 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0079 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_447 0.0083 ms 76.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8416 seconds and 0.4023 seconds precompiling for 25 choices
[rank0]:W1023 11:08:00.394000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:00.495000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:01.104000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_426 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_427 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_437 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_436 0.0069 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_434 0.0073 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0073 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_431 0.0079 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_430 0.0080 ms 80.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_428 0.0084 ms 76.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8827 seconds and 0.3942 seconds precompiling for 25 choices
[rank7]:W1023 11:08:01.181000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:01.281000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x8x512, 16x512x128)
  triton_bmm_426 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_427 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 96.3% 
  triton_bmm_436 0.0067 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0067 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0070 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0071 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_430 0.0080 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0081 ms 78.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_447 0.0082 ms 77.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8071 seconds and 0.4003 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_427 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_426 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_437 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_436 0.0069 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_434 0.0072 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0073 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_430 0.0081 ms 79.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0081 ms 79.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_447 0.0083 ms 77.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7266 seconds and 0.4196 seconds precompiling for 25 choices
[rank2]:W1023 11:08:01.798000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:01.876000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:01.977000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:02.028000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:02.106000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:02.206000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_427 0.0065 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_426 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_437 0.0067 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_436 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0070 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0071 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_430 0.0079 ms 81.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0079 ms 81.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_428 0.0082 ms 77.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8120 seconds and 0.3982 seconds precompiling for 25 choices
[rank4]:W1023 11:08:02.597000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:02.675000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:02.711000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_426 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_427 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_436 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0069 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0072 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0073 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_430 0.0081 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0081 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_447 0.0082 ms 79.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7803 seconds and 0.3926 seconds precompiling for 25 choices
[rank4]:W1023 11:08:02.776000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:02.790000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:02.891000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:03.587000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:03.665000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:03.767000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:03.785000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:03.862000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:03.962000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:06.278000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:06.356000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:06.406000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:06.974000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:07.051000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:07.064000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:07.101000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:07.141000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:07.201000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:07.890000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:07.913000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:07.965000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:07.992000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:08.014000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:08.042000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:08.421000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:08.496000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:08.546000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:09.430000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:09.507000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:09.557000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:09.578000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:09.616000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:09.693000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:09.743000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:09.843000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:09.877000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:10.106000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:10.146000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:10.404000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:10.656000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:10.799000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:10.932000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:11.066000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:11.199000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:11.329000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:11.507000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:11.696000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:11.776000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:11.961000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:12.039000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:12.053000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:12.120000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:12.149000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:12.231000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:12.234000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:12.324000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:12.340000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:12.596000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:12.633000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:12.868000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:12.937000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:13.215000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:13.392000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:13.471000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:13.486000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:13.576000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:13.865000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:14.041000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:14.128000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:14.235000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:14.380000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:14.482000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:14.596000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:14.695000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:14.807000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:14.966000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:15.459000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:15.529000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:15.538000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:15.633000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:15.640000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_451 0.0295 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_461 0.0300 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0301 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_450 0.0309 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_458 0.0458 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.0459 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.0507 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0508 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_455 0.0540 ms 17.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0100 seconds and 0.2284 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_460 0.0299 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_461 0.0299 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_451 0.0301 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_450 0.0315 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_458 0.0455 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.0458 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0503 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0503 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_455 0.0538 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0612 seconds and 0.2359 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_461 0.0328 ms 28.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0329 ms 28.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_451 0.0332 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_450 0.0339 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_469 0.0488 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_459 0.0492 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.0497 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_468 0.0497 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_455 0.0635 ms 14.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0298 seconds and 0.2136 seconds precompiling for 25 choices
[rank0]:W1023 11:08:19.417000 283 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dfcdd5740>
[rank0]:W1023 11:08:19.440000 283 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dfcdd56b0>
[rank7]:W1023 11:08:19.480000 290 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9d826e5aa0>
[rank7]:W1023 11:08:19.503000 290 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9d826e5a10>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:08:19 TP0] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:08:19 TP7] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_451 0.0295 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_461 0.0299 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0301 ms 30.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_450 0.0309 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_458 0.0458 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.0460 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0510 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0511 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_455 0.0542 ms 16.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0643 seconds and 0.2373 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_461 0.0315 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_451 0.0319 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_460 0.0321 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_450 0.0323 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_459 0.0492 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.0495 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0496 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_458 0.0499 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_455 0.0597 ms 15.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0940 seconds and 0.2134 seconds precompiling for 25 choices
[rank2]:W1023 11:08:20.511000 285 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec47f4f18f0>
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_461 0.0302 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0303 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_451 0.0309 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_450 0.0325 ms 28.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_459 0.0459 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.0461 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_468 0.0508 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0509 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_454 0.0546 ms 16.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0774 seconds and 0.2281 seconds precompiling for 25 choices
[rank2]:W1023 11:08:20.534000 285 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec47f4f1860>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:08:20 TP2] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1023 11:08:21.004000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:21.071000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:21.269000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_451 0.0295 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_460 0.0300 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_461 0.0301 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_450 0.0314 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_459 0.0459 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.0460 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0505 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0506 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_455 0.0540 ms 16.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0756 seconds and 0.2134 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_461 0.0300 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0303 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_451 0.0317 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_450 0.0330 ms 27.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_458 0.0459 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.0464 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0511 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0511 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_455 0.0541 ms 17.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0936 seconds and 0.2114 seconds precompiling for 25 choices
[rank0]:W1023 11:08:21.335000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:21.386000 289 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef416a31e00>
[rank6]:W1023 11:08:21.409000 289 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef416a319b0>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:08:21 TP6] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1023 11:08:21.489000 287 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f607982de60>
[rank4]:W1023 11:08:21.512000 287 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f607982dfe0>
[rank7]:W1023 11:08:21.533000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:08:21 TP4] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1023 11:08:21.603000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:21.797000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:21.833000 286 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f078774df20>
[rank3]:W1023 11:08:21.856000 286 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f078c800ae0>
[rank0]:W1023 11:08:21.867000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:08:21 TP3] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1023 11:08:22.036000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:22.068000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:22.131000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:22.319000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:22.333000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:22.399000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:22.587000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:22.597000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:22.630000 288 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ecd0c7f9ef0>
[rank5]:W1023 11:08:22.653000 288 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ecd0c7fa130>
[rank0]:W1023 11:08:22.663000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:08:22 TP5] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1023 11:08:22.861000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:22.868000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:22.909000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:22.936000 284 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec9a89bd830>
[rank0]:W1023 11:08:22.939000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:22.960000 284 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec9a89bd6e0>
[rank4]:W1023 11:08:22.981000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:08:23 TP1] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1023 11:08:23.128000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:23.137000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:23.176000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:23.204000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:23.248000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:23.377000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:23.396000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:23.411000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:23.440000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:23.471000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:23.513000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:23.647000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:23.665000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:23.675000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:23.709000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:23.741000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:23.781000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:23.915000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:23.937000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:23.943000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:23.977000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:24.007000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:24.046000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:24.156000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:24.185000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:24.205000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:24.213000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:24.248000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:24.276000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:24.320000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:24.428000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:24.457000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:24.476000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:24.485000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:24.516000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:24.552000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:24.588000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:24.597000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:24.696000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:24.725000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:24.744000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:24.753000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:24.784000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:24.816000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:24.852000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:24.869000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:24.964000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:25.001000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:25.012000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:25.029000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:25.052000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:25.092000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:25.116000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:25.149000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:25.232000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:25.269000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:25.280000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:25.297000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:25.320000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:25.356000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:25.380000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:25.417000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:25.505000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:25.536000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:25.549000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:25.563000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:25.589000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:25.619000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:25.645000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:25.684000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:25.781000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:25.803000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:25.817000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:25.831000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:25.857000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:25.887000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:25.909000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:25.955000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:26.053000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:26.071000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:26.085000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:26.099000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:26.124000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:26.157000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:26.172000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:26.229000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:26.328000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:26.341000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:26.352000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:26.369000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:26.392000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:26.424000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:26.440000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:26.501000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:26.602000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:26.608000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:26.621000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:26.636000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:26.661000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:26.696000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:26.705000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:26.780000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:26.872000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:26.881000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:26.888000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:26.909000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:26.928000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:26.964000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:26.971000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:27.065000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:27.144000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:27.157000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:27.163000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:27.181000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:27.196000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:27.228000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:27.236000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:27.337000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:27.416000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:27.425000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:27.432000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:27.449000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:27.464000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:27.492000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:27.504000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:27.604000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:27.689000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:27.695000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:27.702000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:27.715000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:27.733000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:27.755000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:27.769000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:27.875000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:27.957000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:27.963000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:27.971000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:27.987000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:28.001000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:28.023000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:28.033000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:28.148000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:28.224000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:28.237000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:28.244000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:28.261000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:28.272000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:28.294000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:28.302000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:28.433000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:28.500000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:28.516000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:28.523000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:28.541000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:28.548000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:28.561000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:28.569000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:28.717000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:28.772000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:28.784000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:28.801000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:28.821000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:28.828000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:28.838000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:28.845000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:28.989000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:29.044000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:29.051000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:29.069000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:29.089000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:29.100000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:29.107000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:29.115000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:29.265000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:29.315000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:29.321000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:29.337000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:29.357000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:29.372000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:29.379000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:29.387000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:29.541000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:29.587000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:29.593000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:29.605000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:29.625000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:29.644000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:29.651000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:29.660000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:29.813000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:29.860000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:29.866000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:29.873000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:29.893000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:29.916000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:29.923000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:29.932000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:30.086000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:30.132000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:30.138000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:30.145000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:30.161000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:30.188000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:30.196000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:30.204000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:30.361000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:30.404000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:30.410000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:30.417000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:30.429000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:30.460000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:30.467000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:30.476000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:30.640000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:30.681000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:30.688000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:30.695000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:30.707000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:30.733000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:30.743000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:30.752000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:30.912000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:30.949000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:30.957000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:30.967000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:30.979000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:31.001000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:31.011000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:31.019000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:31.187000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:31.217000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:31.225000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:31.239000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:31.251000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:31.269000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:31.279000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:31.287000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:31.463000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:31.489000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:31.497000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:31.511000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:31.523000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:31.537000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:31.547000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:31.555000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:31.739000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:31.761000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:31.769000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:31.783000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:31.795000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:31.810000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:31.816000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:31.826000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:32.015000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:32.033000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:32.040000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:32.055000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:32.067000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:32.084000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:32.093000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:32.101000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:32.304000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:32.310000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:32.317000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:32.339000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:32.351000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:32.359000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:32.367000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:32.376000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:32.574000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:32.589000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:32.596000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:32.612000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:32.624000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:32.632000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:32.641000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:32.845000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:32.861000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:32.884000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:32.901000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:32.910000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:32.916000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:32.924000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:33.111000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:33.119000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:33.129000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:33.159000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:33.172000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:33.179000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:33.188000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:33.197000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:33.383000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:33.405000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:33.431000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:33.444000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:33.454000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:33.460000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:33.470000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:33.655000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:33.681000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:33.704000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:33.716000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:33.729000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:33.735000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:33.745000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:33.838000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:33.927000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:33.957000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:33.975000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:33.988000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:34.002000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:34.009000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:34.017000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:34.110000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:34.200000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:34.229000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:34.252000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:34.260000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:34.274000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:34.285000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:34.382000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:34.479000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:34.501000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:34.528000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:34.536000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:34.557000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:34.662000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:34.767000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:34.774000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:34.781000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:34.811000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:34.820000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:34.829000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:34.937000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:35.005000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:35.039000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:35.047000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:35.057000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:35.083000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:35.092000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:35.218000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:35.285000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:35.323000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:35.335000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:35.343000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:35.367000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:35.376000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:35.493000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:35.557000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:35.565000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:35.595000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:35.612000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:35.620000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:35.644000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:35.770000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:35.829000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:35.838000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:35.871000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:35.892000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:35.920000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:36.049000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:36.105000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:36.117000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:36.123000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:36.143000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:36.171000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:36.195000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:36.329000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:36.349000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:36.380000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:36.393000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:36.400000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:36.415000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:36.447000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:36.467000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:36.605000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:36.621000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:36.652000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:36.665000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:36.675000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:36.687000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:36.723000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:36.739000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:36.897000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:36.925000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:36.941000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:36.956000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:37.000000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:37.177000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:37.197000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:37.217000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:37.248000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:37.292000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:37.459000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:37.469000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:37.494000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:37.505000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:37.533000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:37.582000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:37.745000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:37.753000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:37.777000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:37.803000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:37.823000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:37.863000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:38.025000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:38.032000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:38.052000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:38.079000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:38.095000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:38.292000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:38.305000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:38.321000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:38.351000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:38.367000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:38.561000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:38.577000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:38.589000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:38.628000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:38.640000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:38.853000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:38.911000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:38.920000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:39.133000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:39.200000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:39.206000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:39.406000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:39.482000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:39.696000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:39.767000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:39.979000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:40.036000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:40.319000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:40.597000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0608 ms 100.0% 
  triton_mm_475 0.1041 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_474 0.1042 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_484 0.1043 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_485 0.1044 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1083 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1084 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_483 0.1117 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1119 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_479 0.1399 ms 43.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1335 seconds and 0.2045 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0607 ms 100.0% 
  triton_mm_475 0.1041 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_484 0.1042 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_474 0.1042 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_485 0.1043 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_493 0.1087 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1087 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1121 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1122 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_478 0.1406 ms 43.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0504 seconds and 0.2040 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_475 0.1039 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_474 0.1043 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_485 0.1043 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_484 0.1044 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_492 0.1084 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1086 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_483 0.1118 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1119 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_478 0.1395 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0214 seconds and 0.1879 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_474 0.1043 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_475 0.1043 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_485 0.1046 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_484 0.1047 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1088 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1088 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_482 0.1124 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1126 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_478 0.1404 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0811 seconds and 0.2026 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_475 0.1040 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_474 0.1041 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_485 0.1041 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_484 0.1043 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_492 0.1090 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1092 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1126 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1127 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_478 0.1406 ms 43.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0806 seconds and 0.2086 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_475 0.1043 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_474 0.1043 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_485 0.1044 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_484 0.1045 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_492 0.1091 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1091 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1124 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1125 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_478 0.1406 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0893 seconds and 0.1826 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_475 0.1039 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_484 0.1040 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_474 0.1041 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_485 0.1041 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1093 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1095 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1127 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1128 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_479 0.1405 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0923 seconds and 0.1986 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0610 ms 100.0% 
  triton_mm_475 0.1044 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_484 0.1045 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_474 0.1045 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_485 0.1045 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_493 0.1098 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1099 ms 55.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_482 0.1129 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1131 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_478 0.1408 ms 43.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0341 seconds and 0.1929 seconds precompiling for 25 choices
Capturing batches (bs=8 avail_mem=39.69 GB):  94%|| 49/52 [06:48<03:11, 63.82s/it]Capturing batches (bs=4 avail_mem=39.09 GB):  94%|| 49/52 [06:48<03:11, 63.82s/it][rank0]:W1023 11:08:54.067000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:54.079000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:54.085000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:54.139000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:54.146000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:54.156000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:54.163000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:54.218000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:54.253000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:54.262000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:54.268000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:54.325000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:54.333000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:54.410000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:54.475000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:54.516000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:54.564000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:54.618000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:54.672000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:54.691000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:54.698000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:54.772000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:54.808000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:54.879000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:55.776000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:55.785000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:55.797000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:55.855000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:56.022000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:56.221000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:56.328000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:56.413000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:57.580000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:57.591000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:57.659000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:57.669000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:57.696000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:57.710000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:08:57.719000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:57.774000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:08:57.825000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:57.923000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:57.994000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:58.001000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:08:58.051000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:58.075000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:58.126000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:58.156000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:58.235000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:58.286000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:08:58.398000 289 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank4]:W1023 11:08:58.408000 287 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank7]:W1023 11:08:58.452000 290 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank0]:W1023 11:08:58.520000 283 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank1]:W1023 11:08:58.557000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:08:58.596000 288 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank1]:W1023 11:08:58.648000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:58.708000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:58.746000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:08:58.777000 286 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank2]:W1023 11:08:58.830000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:08:58.887000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:08:59.194000 284 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank2]:W1023 11:08:59.365000 285 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
AUTOTUNE bmm(16x4x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_517 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_506 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_507 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_508 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_496 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_509 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_511 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_512 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8762 seconds and 0.1521 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_507 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_510 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_511 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_518 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_519 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_499 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_501 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_506 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_508 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9114 seconds and 0.1545 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_506 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_508 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_512 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_514 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_518 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_511 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_515 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_499 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_500 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8766 seconds and 0.1655 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_501 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_496 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_502 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_499 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_500 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_516 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_507 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_511 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_514 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8352 seconds and 0.1374 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_509 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_507 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_511 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_518 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_506 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_508 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_510 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_513 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8778 seconds and 0.1609 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_519 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_514 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_518 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_513 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_508 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_510 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_509 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_515 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_511 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_517 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8088 seconds and 0.1445 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_515 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_509 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_511 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_517 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 99.4% 
  triton_bmm_497 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_507 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_508 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_510 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8014 seconds and 0.1440 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_507 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_510 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_512 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_508 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_509 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_511 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_506 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_513 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_514 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_516 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8489 seconds and 0.1504 seconds precompiling for 25 choices
[rank4]:W1023 11:09:08.399000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:08.529000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:08.630000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:08.912000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:09.045000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:09.127000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:09.401000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:09.505000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:09.567000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:09.630000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:09.906000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:10.014000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:10.066000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:10.139000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:10.371000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:10.902000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:11.716000 283 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank4]:W1023 11:09:11.730000 287 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank6]:W1023 11:09:11.732000 289 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank3]:W1023 11:09:12.635000 286 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank1]:W1023 11:09:12.925000 284 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank2]:W1023 11:09:13.507000 285 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank7]:W1023 11:09:13.549000 290 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank5]:W1023 11:09:15.665000 288 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x4x512, 16x512x128)
  triton_bmm_522 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_523 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0064 ms 98.8% 
  triton_bmm_532 0.0068 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0068 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0072 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0072 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0079 ms 80.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0079 ms 80.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_524 0.0083 ms 76.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8363 seconds and 0.4214 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_523 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_522 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_533 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_530 0.0071 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0072 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_527 0.0079 ms 81.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_526 0.0080 ms 80.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_543 0.0083 ms 77.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8896 seconds and 0.4228 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_523 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_522 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_532 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0071 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0071 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0080 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0080 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_543 0.0083 ms 77.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8983 seconds and 0.4283 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  triton_bmm_523 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_522 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0065 ms 98.8% 
  triton_bmm_533 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0069 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0071 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_526 0.0081 ms 78.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0082 ms 78.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_543 0.0083 ms 77.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8602 seconds and 0.4151 seconds precompiling for 25 choices
[rank4]:W1023 11:09:17.994000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:18.014000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:18.057000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:18.072000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:18.092000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:18.122000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:18.134000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:18.142000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:18.185000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x4x512, 16x512x128)
  triton_bmm_523 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0064 ms 100.0% 
  triton_bmm_522 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_532 0.0069 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0069 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_531 0.0073 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0076 ms 84.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_526 0.0080 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0081 ms 79.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_542 0.0084 ms 76.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8629 seconds and 0.4193 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  triton_bmm_522 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_523 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 96.9% 
  triton_bmm_532 0.0069 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0070 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0078 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0079 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_530 0.0079 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0079 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_543 0.0084 ms 74.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8441 seconds and 0.4150 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  triton_bmm_522 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_523 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 98.8% 
  triton_bmm_532 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0070 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0070 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0079 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0080 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_542 0.0082 ms 78.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8732 seconds and 0.4238 seconds precompiling for 25 choices
[rank3]:W1023 11:09:19.001000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:19.079000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:19.129000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:19.294000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:19.372000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:19.422000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:19.843000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:19.897000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:19.922000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:19.972000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:19.974000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:20.025000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_522 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_523 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_532 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_531 0.0071 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0073 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_526 0.0080 ms 80.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0080 ms 80.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_542 0.0083 ms 76.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.6577 seconds and 0.4149 seconds precompiling for 25 choices
[rank5]:W1023 11:09:21.689000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:21.768000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:21.819000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:23.714000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:23.793000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:23.798000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:23.820000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:23.876000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:23.897000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:23.899000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:23.981000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:24.006000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:24.811000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:24.889000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:24.992000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:25.001000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:25.079000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:25.085000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:25.163000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:25.180000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:25.275000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:25.686000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:25.764000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:25.866000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:26.672000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:26.893000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:26.940000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:26.971000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:27.074000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:27.175000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:27.187000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:27.204000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:27.447000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:27.461000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:27.719000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:27.736000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:28.205000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:28.461000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:28.487000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:28.659000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:28.737000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:28.757000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:28.846000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:28.924000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:28.932000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:29.011000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:29.036000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:29.089000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:29.204000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:29.358000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:29.366000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:29.385000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:29.437000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:29.463000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:29.538000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:29.564000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:29.642000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:30.435000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:30.513000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:30.572000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:30.614000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:30.848000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:30.867000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:30.946000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:31.051000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:31.129000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:31.141000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:31.220000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:31.304000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:31.323000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:31.382000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:31.489000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:32.645000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:32.723000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:32.825000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_557 0.0298 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_556 0.0299 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_547 0.0317 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_546 0.0322 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_554 0.0453 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_555 0.0455 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0482 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0483 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_551 0.0539 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0534 seconds and 0.2187 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_547 0.0289 ms 31.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_556 0.0297 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_557 0.0298 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_546 0.0298 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_555 0.0453 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0453 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_564 0.0482 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0482 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_551 0.0546 ms 16.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9920 seconds and 0.2268 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_547 0.0314 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_557 0.0317 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_556 0.0318 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_546 0.0325 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_564 0.0482 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0485 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0487 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_555 0.0489 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_551 0.0599 ms 15.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0824 seconds and 0.2221 seconds precompiling for 25 choices
[rank0]:W1023 11:09:36.217000 283 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dfcdd5740>
[rank0]:W1023 11:09:36.240000 283 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dfcdd56b0>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:09:36 TP0] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_547 0.0291 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_556 0.0299 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_557 0.0299 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_546 0.0315 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_554 0.0453 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_555 0.0455 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_565 0.0479 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0479 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_551 0.0539 ms 17.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0926 seconds and 0.2423 seconds precompiling for 25 choices
[rank6]:W1023 11:09:36.475000 289 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef416a31e00>
[rank6]:W1023 11:09:36.498000 289 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef416a319b0>
[rank4]:W1023 11:09:36.522000 287 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f607982de60>
[rank4]:W1023 11:09:36.545000 287 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f607982dfe0>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:09:36 TP4] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_547 0.0292 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_557 0.0294 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_556 0.0294 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_546 0.0327 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_555 0.0447 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0447 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_564 0.0479 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0479 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_551 0.0536 ms 17.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0530 seconds and 0.2204 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_557 0.0296 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_556 0.0298 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_547 0.0323 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_546 0.0334 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_554 0.0450 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_555 0.0450 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0481 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0481 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_551 0.0559 ms 16.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0889 seconds and 0.2251 seconds precompiling for 25 choices
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:09:37 TP6] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_557 0.0318 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_556 0.0320 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_547 0.0342 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_546 0.0344 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_564 0.0483 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0487 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0514 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_555 0.0514 ms 17.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0617 ms 14.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1398 seconds and 0.2365 seconds precompiling for 25 choices
[rank3]:W1023 11:09:37.629000 286 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f078774df20>
[rank3]:W1023 11:09:37.652000 286 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f078c800ae0>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:09:37 TP3] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1023 11:09:38.007000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:38.016000 290 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9d826e5aa0>
[rank7]:W1023 11:09:38.039000 290 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9d826e5a10>
[rank0]:W1023 11:09:38.132000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:38.280000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:38.413000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:38.424000 285 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec47f4f18f0>
[rank2]:W1023 11:09:38.447000 285 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec47f4f1860>
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_556 0.0294 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_557 0.0294 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_547 0.0300 ms 30.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_546 0.0313 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_555 0.0449 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0450 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_564 0.0480 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0481 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_551 0.0539 ms 16.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0577 seconds and 0.2227 seconds precompiling for 25 choices
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:09:38 TP2] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1023 11:09:38.569000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:38.626000 284 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec9a89bd830>
[rank1]:W1023 11:09:38.650000 284 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec9a89bd6e0>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:09:38 TP7] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1023 11:09:38.693000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:38.854000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:38.976000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:39.114000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:39.134000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:39.270000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:09:39 TP1] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1023 11:09:39.388000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:39.412000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:39.563000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:39.664000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:39.692000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:39.702000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:39.800000 288 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ecd0c7f9ef0>
[rank5]:W1023 11:09:39.823000 288 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ecd0c7fa130>
[rank0]:W1023 11:09:39.841000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:09:39 TP5] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1023 11:09:39.940000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:39.972000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:39.989000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:40.122000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:40.129000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:40.216000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:40.252000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:40.280000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:40.401000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:40.409000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:40.499000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:40.532000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:40.550000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:40.560000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:40.681000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:40.688000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:40.770000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:40.777000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:40.812000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:40.831000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:40.840000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:40.961000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:40.968000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:41.056000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:41.070000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:41.092000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:41.122000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:41.131000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:41.237000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:41.246000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:41.334000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:41.348000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:41.374000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:41.396000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:41.408000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:41.490000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:41.526000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:41.532000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:41.609000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:41.628000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:41.653000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:41.676000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:41.688000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:41.770000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:41.802000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:41.809000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:41.885000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:41.908000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:41.930000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:41.960000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:41.972000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:42.082000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:42.089000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:42.165000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:42.192000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:42.214000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:42.240000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:42.256000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:42.361000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:42.369000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:42.440000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:42.474000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:42.492000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:42.521000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:42.537000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:42.581000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:42.637000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:42.645000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:42.716000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:42.754000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:42.772000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:42.802000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:42.817000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:42.861000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:42.916000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:42.924000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:42.992000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:43.033000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:43.052000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:43.086000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:43.098000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:43.141000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:43.192000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:43.201000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:43.268000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:43.326000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:43.334000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:43.373000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:43.385000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:43.425000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:43.469000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:43.477000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:43.548000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:43.606000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:43.616000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:43.654000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:43.663000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:43.704000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:43.744000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:43.753000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:43.824000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:43.886000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:43.896000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:43.933000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:43.943000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:43.984000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:44.020000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:44.029000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:44.100000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:44.166000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:44.176000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:44.209000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:44.221000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:44.264000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:44.296000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:44.303000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:44.376000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:44.445000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:44.456000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:44.486000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:44.502000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:44.544000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:44.572000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:44.579000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:44.652000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:44.726000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:44.736000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:44.761000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:44.777000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:44.824000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:44.848000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:44.856000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:44.928000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:45.006000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:45.016000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:45.037000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:45.058000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:45.104000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:45.124000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:45.133000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:45.204000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:45.290000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:45.298000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:45.317000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:45.338000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:45.384000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:45.401000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:45.409000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:45.484000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:45.570000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:45.580000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:45.593000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:45.613000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:45.664000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:45.676000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:45.683000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:45.760000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:45.849000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:45.860000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:45.869000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:45.889000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:45.944000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:45.953000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:45.960000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:46.036000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:46.130000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:46.140000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:46.148000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:46.166000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:46.224000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:46.232000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:46.239000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:46.312000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:46.410000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:46.424000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:46.432000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:46.441000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:46.504000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:46.512000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:46.519000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:46.588000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:46.689000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:46.704000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:46.714000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:46.722000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:46.784000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:46.792000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:46.799000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:46.864000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:46.978000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:46.988000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:46.996000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:47.006000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:47.064000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:47.072000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:47.079000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:47.140000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:47.262000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:47.269000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:47.278000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:47.287000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:47.344000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:47.351000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:47.359000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:47.416000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:47.542000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:47.552000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:47.560000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:47.569000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:47.624000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:47.632000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:47.639000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:47.692000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:47.822000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:47.838000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:47.845000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:47.853000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:47.904000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:47.912000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:47.919000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:47.968000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:48.101000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:48.117000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:48.128000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:48.136000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:48.184000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:48.192000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:48.199000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:48.244000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:48.382000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:48.398000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:48.412000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:48.420000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:48.464000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:48.472000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:48.479000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:48.520000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:48.662000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:48.677000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:48.692000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:48.700000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:48.744000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:48.751000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:48.765000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:48.796000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:48.942000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:48.958000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:48.978000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:48.984000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:49.024000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:49.031000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:49.041000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:49.072000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:49.222000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:49.238000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:49.258000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:49.268000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:49.304000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:49.311000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:49.320000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:49.348000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:49.506000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:49.522000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:49.550000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:49.556000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:49.584000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:49.591000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:49.609000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:49.624000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:49.786000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:49.802000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:49.830000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:49.840000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:49.864000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:49.871000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:49.884000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:49.900000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:50.066000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:50.081000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:50.110000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:50.120000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:50.144000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:50.151000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:50.161000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:50.176000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:50.346000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:50.362000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:50.390000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:50.404000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:50.429000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:50.436000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:50.443000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:50.456000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:50.626000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:50.642000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:50.670000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:50.688000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:50.708000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:50.715000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:50.724000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:50.733000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:50.906000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:50.922000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:50.950000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:50.972000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:50.988000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:50.996000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:51.003000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:51.012000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:51.198000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:51.213000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:51.242000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:51.256000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:51.268000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:51.275000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:51.288000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:51.296000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:51.490000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:51.505000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:51.534000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:51.540000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:51.552000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:51.559000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:51.566000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:51.584000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:51.774000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:51.785000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:51.814000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:51.824000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:51.831000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:51.840000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:51.847000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:51.860000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:52.053000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:52.066000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:52.094000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:52.108000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:52.115000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:52.124000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:52.131000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:52.139000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:52.334000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:52.345000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:52.374000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:52.393000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:52.399000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:52.406000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:52.415000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:52.422000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:52.618000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:52.626000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:52.654000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:52.676000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:52.683000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:52.691000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:52.700000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:52.709000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:52.914000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:52.922000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:52.946000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:52.956000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:52.965000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:52.971000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:52.980000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:52.996000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:53.206000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:53.214000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:53.236000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:53.244000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:53.253000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:53.260000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:53.268000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:53.284000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:53.490000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:53.498000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:53.516000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:53.526000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:53.536000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:53.543000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:53.551000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:53.564000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:53.778000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:53.786000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:53.796000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:53.810000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:53.820000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:53.827000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:53.835000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:09:53.844000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:54.062000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:54.070000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:54.077000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:54.094000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:09:54.102000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:54.120000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:54.127000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:54.342000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:54.350000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:54.358000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:54.374000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:09:54.402000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:54.409000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:54.623000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:54.631000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:54.639000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:54.655000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:54.688000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:54.915000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:54.924000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:54.931000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:54.942000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:54.977000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:55.217000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:55.226000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:55.235000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:55.246000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:55.261000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:55.500000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:55.518000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:55.526000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:09:55.535000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:55.551000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:09:55.781000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:55.802000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:55.810000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:55.833000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:56.084000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:56.092000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:56.105000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:56.370000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:09:56.380000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:56.389000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:09:56.662000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:56.677000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:56.960000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:57.248000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:57.541000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:09:57.833000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0603 ms 100.0% 
  triton_mm_571 0.1042 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_580 0.1042 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_581 0.1042 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_570 0.1043 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_589 0.1082 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_588 0.1082 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1116 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1117 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_575 0.1334 ms 45.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1530 seconds and 0.2050 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0605 ms 100.0% 
  triton_mm_580 0.1043 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_581 0.1043 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_570 0.1044 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_571 0.1045 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_589 0.1082 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_588 0.1082 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_578 0.1115 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1118 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_575 0.1332 ms 45.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1102 seconds and 0.2184 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0607 ms 100.0% 
  triton_mm_570 0.1041 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_571 0.1041 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_581 0.1041 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_580 0.1042 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_588 0.1084 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1085 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_579 0.1120 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1120 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_574 0.1334 ms 45.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0779 seconds and 0.2013 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_580 0.1042 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_581 0.1042 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_570 0.1042 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_571 0.1042 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_588 0.1086 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1087 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1117 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1120 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_575 0.1333 ms 45.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1206 seconds and 0.1865 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_571 0.1041 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_570 0.1041 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_581 0.1041 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_580 0.1043 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1082 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_588 0.1082 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_578 0.1113 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1114 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_575 0.1328 ms 45.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1092 seconds and 0.1863 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0602 ms 100.0% 
  triton_mm_571 0.1041 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_580 0.1042 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_581 0.1042 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_570 0.1043 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_588 0.1082 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1083 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1112 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1114 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_575 0.1322 ms 45.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0915 seconds and 0.1902 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0605 ms 100.0% 
  triton_mm_571 0.1044 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_570 0.1045 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_580 0.1045 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_581 0.1045 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_588 0.1090 ms 55.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1091 ms 55.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1120 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1121 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_574 0.1336 ms 45.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0013 seconds and 0.1940 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_581 0.1041 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_580 0.1041 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_571 0.1042 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_570 0.1043 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_589 0.1093 ms 55.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_588 0.1094 ms 55.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1125 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1126 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_574 0.1334 ms 45.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0701 seconds and 0.1954 seconds precompiling for 25 choices
Capturing batches (bs=4 avail_mem=39.09 GB):  96%|| 50/52 [08:05<02:15, 67.84s/it]Capturing batches (bs=2 avail_mem=38.51 GB):  96%|| 50/52 [08:05<02:15, 67.84s/it][rank3]:W1023 11:10:11.159000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:11.250000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:11.361000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:11.727000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:11.734000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:11.806000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:11.812000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:11.844000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:11.914000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:11.919000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:11.923000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:11.968000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:11.979000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:12.029000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:12.047000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:12.057000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:12.058000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:12.138000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:12.154000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:12.166000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:12.246000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:12.412000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:12.491000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:12.603000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:12.908000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:13.436000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:13.461000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:13.570000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:13.681000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:13.694000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:13.779000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:14.127000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:14.457000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:14.541000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:14.594000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:15.107000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:15.187000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:15.239000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:15.247000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:15.303000 286 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank4]:W1023 11:10:15.327000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:15.378000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:15.380000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:15.395000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:15.462000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:15.463000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:15.482000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:15.515000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:15.537000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:15.547000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:15.600000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:15.673000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:15.726000 288 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank1]:W1023 11:10:15.754000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:15.808000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:15.867000 287 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank0]:W1023 11:10:16.014000 283 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank6]:W1023 11:10:16.031000 289 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank7]:W1023 11:10:16.079000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:16.104000 285 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank7]:W1023 11:10:16.167000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:16.224000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:16.343000 284 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank7]:W1023 11:10:16.709000 290 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
AUTOTUNE bmm(16x2x128, 16x128x512)
  bmm 0.0062 ms 100.0% 
  triton_bmm_607 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_598 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_603 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_612 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_606 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_611 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_605 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_613 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_599 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8784 seconds and 0.1572 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_612 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_613 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_615 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_608 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_609 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_611 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_614 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_603 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_607 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8303 seconds and 0.1643 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_610 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_613 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 99.4% 
  triton_bmm_611 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_603 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_606 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_607 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_608 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_609 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_612 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8902 seconds and 0.1416 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_612 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_613 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_599 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_610 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_611 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_614 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_607 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_609 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_598 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9090 seconds and 0.1345 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_603 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_602 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_596 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_609 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_598 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_605 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_608 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  bmm 0.0064 ms 98.7% 
  triton_bmm_592 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_594 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9096 seconds and 0.1561 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_602 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_594 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_595 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_596 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_597 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_598 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_603 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_605 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_608 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_611 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8709 seconds and 0.1600 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_602 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_612 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0063 ms 99.4% 
  triton_bmm_604 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_608 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_609 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_610 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_593 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_595 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_611 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8710 seconds and 0.1296 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_595 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_608 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_610 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_594 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_596 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_602 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_603 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_604 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_605 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_606 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8870 seconds and 0.1456 seconds precompiling for 25 choices
[rank3]:W1023 11:10:25.590000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:25.759000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:25.774000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:25.808000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:26.101000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:26.261000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:26.285000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:26.314000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:26.315000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:26.722000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:26.825000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:27.231000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:27.299000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:27.337000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:27.805000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:27.869000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:28.490000 285 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank6]:W1023 11:10:28.631000 289 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank4]:W1023 11:10:29.045000 287 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank3]:W1023 11:10:29.098000 286 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank1]:W1023 11:10:29.907000 284 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank0]:W1023 11:10:29.933000 283 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank7]:W1023 11:10:30.023000 290 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank5]:W1023 11:10:31.073000 288 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_619 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_618 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0064 ms 96.8% 
  triton_bmm_629 0.0068 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0069 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0076 ms 81.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_622 0.0076 ms 81.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0076 ms 81.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_626 0.0076 ms 81.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_638 0.0083 ms 74.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8520 seconds and 0.4268 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_618 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0065 ms 99.4% 
  triton_bmm_619 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_628 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_627 0.0070 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0071 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_623 0.0079 ms 81.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_622 0.0079 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_638 0.0082 ms 78.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7885 seconds and 0.4199 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  bmm 0.0063 ms 100.0% 
  triton_bmm_618 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_619 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_629 0.0067 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0067 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_626 0.0071 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0071 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_622 0.0079 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0079 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_638 0.0083 ms 76.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8626 seconds and 0.4297 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_618 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_619 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_629 0.0067 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0067 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0067 ms 93.5% 
  triton_bmm_627 0.0070 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0070 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_622 0.0079 ms 79.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0080 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_620 0.0081 ms 78.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8870 seconds and 0.4128 seconds precompiling for 25 choices
[rank2]:W1023 11:10:34.862000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:34.914000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:34.941000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:34.991000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:34.994000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:35.045000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_618 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0064 ms 99.4% 
  triton_bmm_619 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_628 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0071 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0071 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_622 0.0079 ms 80.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0079 ms 80.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_638 0.0082 ms 77.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8442 seconds and 0.4263 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_618 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_619 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0064 ms 98.7% 
  triton_bmm_629 0.0067 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0067 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0069 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0069 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_622 0.0079 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0079 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_638 0.0081 ms 77.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8577 seconds and 0.4104 seconds precompiling for 25 choices
[rank4]:W1023 11:10:35.298000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:35.377000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_619 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_618 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0064 ms 98.7% 
  triton_bmm_628 0.0067 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0067 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0070 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0070 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_622 0.0079 ms 79.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0079 ms 79.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_638 0.0081 ms 77.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8969 seconds and 0.4169 seconds precompiling for 25 choices
[rank4]:W1023 11:10:35.428000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:35.633000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:35.713000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:35.776000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:36.260000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:36.281000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:36.340000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:36.360000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:36.390000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:36.410000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_619 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_618 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0064 ms 98.7% 
  triton_bmm_628 0.0067 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0067 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_627 0.0071 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0071 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_622 0.0078 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0078 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_639 0.0083 ms 76.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8751 seconds and 0.3945 seconds precompiling for 25 choices
[rank7]:W1023 11:10:36.418000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:36.496000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:36.547000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:37.422000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:37.502000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:37.553000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:40.431000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:40.510000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:40.615000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:40.714000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:40.741000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:40.754000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:40.792000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:40.821000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:40.832000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:40.895000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:40.925000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:40.934000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:42.094000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:42.120000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:42.173000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:42.199000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:42.264000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:42.287000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:42.314000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:42.342000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:42.445000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:43.285000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:43.363000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:43.465000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:44.083000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:44.129000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:44.141000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:44.368000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:44.405000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:44.419000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:44.436000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:44.648000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:44.688000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:44.697000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:44.727000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:44.968000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:45.099000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:45.380000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:45.512000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:45.654000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:45.661000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:45.800000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:45.938000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:46.084000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:46.162000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:46.202000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:46.213000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:46.214000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:46.242000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:46.281000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:46.292000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:46.347000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:46.383000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:46.394000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:46.642000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:46.697000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:46.722000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:46.824000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:46.983000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:47.278000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:47.280000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:47.363000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:47.476000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:47.737000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:47.776000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:47.816000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:47.856000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:47.924000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:47.969000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:49.113000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:49.193000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:49.300000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_653 0.0311 ms 29.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0317 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_643 0.0350 ms 26.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0351 ms 26.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_661 0.0464 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0465 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_650 0.0485 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_651 0.0486 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_663 0.0600 ms 15.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1718 seconds and 0.2271 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0089 ms 100.0% 
  triton_mm_653 0.0310 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0317 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_643 0.0320 ms 27.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0325 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_660 0.0466 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0467 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_651 0.0483 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_650 0.0499 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_647 0.0596 ms 14.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1426 seconds and 0.2122 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0089 ms 100.0% 
  triton_mm_652 0.0291 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_653 0.0291 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_643 0.0292 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0311 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_650 0.0439 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_651 0.0439 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_661 0.0443 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0445 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_647 0.0539 ms 16.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1483 seconds and 0.2230 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_652 0.0291 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_653 0.0291 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_643 0.0296 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0319 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_651 0.0441 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_650 0.0441 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_660 0.0441 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0442 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_647 0.0542 ms 16.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1318 seconds and 0.2180 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_652 0.0292 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_653 0.0293 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_643 0.0297 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0311 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_651 0.0440 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_650 0.0442 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0446 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0449 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_647 0.0539 ms 16.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1134 seconds and 0.2205 seconds precompiling for 25 choices
[rank2]:W1023 11:10:53.526000 285 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec47f4f18f0>
[rank2]:W1023 11:10:53.549000 285 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec47f4f1860>
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0090 ms 100.0% 
  triton_mm_653 0.0293 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0295 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_643 0.0348 ms 25.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0352 ms 25.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_661 0.0448 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_651 0.0450 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0450 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_650 0.0457 ms 19.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_662 0.0593 ms 15.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0361 seconds and 0.2221 seconds precompiling for 25 choices
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:10:53 TP2] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0090 ms 100.0% 
  triton_mm_652 0.0290 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_653 0.0291 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_643 0.0293 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0318 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_651 0.0444 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_650 0.0445 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_660 0.0445 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0446 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_647 0.0539 ms 16.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1011 seconds and 0.2303 seconds precompiling for 25 choices
[rank4]:W1023 11:10:53.660000 287 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f607982de60>
[rank4]:W1023 11:10:53.683000 287 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f607982dfe0>
[rank3]:W1023 11:10:53.723000 286 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f078774df20>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:10:53 TP4] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1023 11:10:53.758000 286 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f078c800ae0>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:10:53 TP3] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1023 11:10:53.834000 289 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef416a31e00>
[rank6]:W1023 11:10:53.858000 289 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef416a319b0>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:10:53 TP6] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1023 11:10:54.349000 283 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dfcdd5740>
[rank0]:W1023 11:10:54.372000 283 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dfcdd56b0>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:10:54 TP0] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1023 11:10:54.799000 284 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec9a89bd830>
[rank7]:W1023 11:10:54.803000 290 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9d826e5aa0>
[rank1]:W1023 11:10:54.822000 284 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec9a89bd6e0>
[rank7]:W1023 11:10:54.826000 290 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9d826e5a10>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:10:54 TP1] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:10:54 TP7] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0089 ms 100.0% 
  triton_mm_643 0.0283 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_653 0.0294 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0301 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_642 0.0311 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_650 0.0439 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_651 0.0439 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_661 0.0443 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0443 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_646 0.0538 ms 16.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1187 seconds and 0.2146 seconds precompiling for 25 choices
[rank4]:W1023 11:10:55.024000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:55.070000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:55.282000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:55.304000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:55.354000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:55.362000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:55.578000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:55.586000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:55.634000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:55.649000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:55.721000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:55.862000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:55.869000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:55.922000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:55.933000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:56.004000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:56.152000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:56.160000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:56.188000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:56.204000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:56.223000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:56.284000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:56.326000 288 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ecd0c7f9ef0>
[rank5]:W1023 11:10:56.349000 288 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ecd0c7fa130>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:10:56 TP5] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1023 11:10:56.440000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:56.448000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:56.476000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:56.492000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:56.511000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:56.564000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:56.732000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:56.738000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:56.753000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:56.762000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:56.778000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:56.800000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:56.853000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:57.020000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:57.028000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:57.037000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:57.046000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:57.062000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:57.085000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:57.133000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:57.300000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:57.314000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:57.321000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:57.330000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:57.350000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:57.373000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:57.413000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:57.584000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:57.604000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:57.612000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:57.625000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:57.636000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:57.661000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:57.693000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:57.868000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:57.875000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:57.884000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:57.897000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:57.910000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:57.920000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:57.953000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:57.973000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:58.157000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:58.165000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:58.172000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:58.188000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:58.197000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:58.258000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:58.458000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:58.465000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:58.483000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:58.492000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:58.749000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:58.756000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:58.787000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:58.907000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:58.957000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:59.033000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:59.044000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:10:59.082000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:59.113000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:59.202000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:59.242000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:59.250000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:10:59.317000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:59.332000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:59.397000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:59.439000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:59.499000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:59.534000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:59.543000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:59.622000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:59.685000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:10:59.734000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:10:59.802000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:10:59.833000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:10:59.842000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:10:59.909000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:10:59.972000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:00.026000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:00.038000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:00.094000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:00.117000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:00.133000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:00.197000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:00.256000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:00.273000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:00.314000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:00.326000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:00.382000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:00.401000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:00.425000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:00.481000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:00.540000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:00.557000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:00.602000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:00.614000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:00.670000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:00.686000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:00.718000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:00.830000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:00.847000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:00.888000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:00.900000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:00.957000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:00.968000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:01.015000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:01.122000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:01.134000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:01.183000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:01.192000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:01.247000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:01.257000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:01.309000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:01.408000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:01.417000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:01.425000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:01.474000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:01.483000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:01.538000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:01.546000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:01.601000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:01.696000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:01.705000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:01.713000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:01.764000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:01.772000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:01.824000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:01.833000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:01.898000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:01.986000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:01.996000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:02.005000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:02.056000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:02.063000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:02.116000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:02.123000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:02.189000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:02.268000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:02.285000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:02.292000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:02.350000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:02.358000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:02.410000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:02.418000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:02.481000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:02.556000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:02.573000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:02.580000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:02.638000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:02.648000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:02.698000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:02.706000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:02.773000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:02.846000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:02.863000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:02.872000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:02.925000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:02.934000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:02.984000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:02.992000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:03.066000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:03.138000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:03.158000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:03.167000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:03.227000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:03.236000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:03.274000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:03.282000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:03.357000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:03.425000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:03.445000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:03.453000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:03.530000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:03.540000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:03.574000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:03.583000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:03.649000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:03.709000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:03.737000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:03.744000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:03.817000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:03.824000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:03.860000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:03.867000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:03.942000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:03.994000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:04.030000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:04.039000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:04.108000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:04.115000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:04.148000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:04.155000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:04.234000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:04.278000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:04.318000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:04.327000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:04.400000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:04.407000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:04.436000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:04.443000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:04.522000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:04.562000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:04.606000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:04.615000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:04.692000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:04.699000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:04.724000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:04.730000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:04.810000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:04.846000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:04.894000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:04.903000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:04.984000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:04.991000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:05.012000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:05.019000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:05.098000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:05.130000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:05.182000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:05.191000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:05.287000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:05.296000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:05.314000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:05.322000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:05.385000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:05.413000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:05.469000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:05.477000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:05.594000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:05.603000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:05.614000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:05.622000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:05.677000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:05.697000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:05.757000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:05.764000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:05.886000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:05.895000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:05.905000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:05.913000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:05.969000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:05.981000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:06.045000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:06.053000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:06.178000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:06.187000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:06.198000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:06.206000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:06.265000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:06.272000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:06.337000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:06.344000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:06.466000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:06.476000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:06.486000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:06.494000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:06.557000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:06.564000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:06.625000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:06.633000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:06.754000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:06.764000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:06.774000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:06.783000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:06.845000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:06.853000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:06.913000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:06.921000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:07.046000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:07.055000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:07.066000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:07.074000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:07.133000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:07.141000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:07.201000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:07.209000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:07.336000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:07.344000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:07.355000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:07.363000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:07.421000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:07.429000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:07.489000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:07.497000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:07.630000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:07.640000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:07.650000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:07.658000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:07.705000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:07.717000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:07.777000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:07.785000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:07.919000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:07.930000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:07.941000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:07.949000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:07.993000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:08.005000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:08.065000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:08.073000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:08.210000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:08.220000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:08.230000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:08.238000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:08.281000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:08.297000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:08.357000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:08.364000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:08.502000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:08.512000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:08.522000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:08.530000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:08.569000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:08.585000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:08.641000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:08.653000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:08.795000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:08.804000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:08.814000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:08.822000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:08.857000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:08.877000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:08.925000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:08.941000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:09.086000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:09.096000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:09.106000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:09.114000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:09.149000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:09.173000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:09.213000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:09.233000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:09.376000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:09.384000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:09.393000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:09.400000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:09.438000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:09.466000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:09.502000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:09.522000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:09.668000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:09.677000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:09.684000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:09.691000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:09.726000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:09.754000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:09.786000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:09.810000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:09.962000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:09.972000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:09.981000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:09.989000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:10.013000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:10.041000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:10.069000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:10.097000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:10.266000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:10.275000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:10.284000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:10.293000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:10.302000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:10.333000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:10.353000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:10.386000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:10.556000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:10.564000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:10.572000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:10.578000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:10.590000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:10.626000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:10.638000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:10.674000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:10.856000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:10.864000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:10.872000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:10.878000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:10.887000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:10.914000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:10.924000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:10.958000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:11.150000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:11.160000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:11.168000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:11.178000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:11.185000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:11.202000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:11.209000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:11.241000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:11.440000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:11.448000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:11.454000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:11.465000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:11.474000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:11.494000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:11.503000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:11.530000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:11.732000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:11.740000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:11.746000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:11.756000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:11.766000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:11.786000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:11.795000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:11.822000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:12.030000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:12.046000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:12.054000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:12.064000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:12.077000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:12.084000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:12.105000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:12.322000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:12.341000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:12.349000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:12.369000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:12.376000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:12.397000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:12.611000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:12.631000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:12.662000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:12.686000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:12.905000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:12.956000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:12.983000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:13.205000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:13.257000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:13.279000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:13.551000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:13.579000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:13.874000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:14.171000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:14.474000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:14.768000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_666 0.1044 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_676 0.1044 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_677 0.1044 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_667 0.1045 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_684 0.1067 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1068 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1116 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1117 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_670 0.1289 ms 46.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1439 seconds and 0.1968 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0600 ms 100.0% 
  triton_mm_676 0.1038 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_677 0.1039 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_666 0.1039 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1040 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_684 0.1063 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1063 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1109 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1110 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_670 0.1280 ms 46.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0716 seconds and 0.1954 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_676 0.1041 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_666 0.1042 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1042 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_677 0.1043 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_685 0.1067 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_684 0.1067 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1116 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1117 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_671 0.1294 ms 46.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0424 seconds and 0.1945 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_677 0.1039 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_676 0.1040 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_667 0.1042 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_666 0.1042 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_685 0.1066 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_684 0.1067 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1116 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1118 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_670 0.1293 ms 46.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1067 seconds and 0.1829 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0602 ms 100.0% 
  triton_mm_676 0.1041 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_666 0.1043 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1043 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_677 0.1043 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_685 0.1067 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_684 0.1068 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1112 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1113 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_670 0.1290 ms 46.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0925 seconds and 0.2032 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0605 ms 100.0% 
  triton_mm_677 0.1044 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_666 0.1044 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_676 0.1044 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_667 0.1044 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_684 0.1070 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1073 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1116 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1118 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_671 0.1294 ms 46.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1431 seconds and 0.2084 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0600 ms 100.0% 
  triton_mm_677 0.1041 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_666 0.1041 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1041 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_676 0.1041 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1067 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_684 0.1067 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1111 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1111 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_670 0.1284 ms 46.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0796 seconds and 0.1959 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0603 ms 100.0% 
  triton_mm_677 0.1040 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_676 0.1040 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_667 0.1042 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_666 0.1042 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_685 0.1077 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_684 0.1078 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1115 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1116 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_671 0.1290 ms 46.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0992 seconds and 0.2002 seconds precompiling for 25 choices
Capturing batches (bs=2 avail_mem=38.51 GB):  98%|| 51/52 [09:22<01:10, 70.54s/it]Capturing batches (bs=1 avail_mem=37.92 GB):  98%|| 51/52 [09:22<01:10, 70.54s/it][rank7]:W1023 11:11:27.286000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:27.365000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:27.474000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:27.542000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:27.623000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:27.733000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:27.936000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:28.016000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:28.066000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:28.075000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:28.125000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:28.130000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:28.146000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:28.155000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:28.176000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:28.209000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:28.227000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:28.254000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:28.256000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:28.263000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:28.308000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:28.318000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:28.365000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:28.418000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:28.940000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:29.211000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:29.630000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:29.744000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:29.758000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:29.805000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:29.819000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:29.887000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:30.634000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:30.715000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:30.818000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:30.897000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:30.979000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:31.085000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:31.136000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:31.215000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:31.218000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:31.225000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:31.296000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:31.300000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:31.308000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:31.323000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:31.345000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:31.382000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:31.402000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:31.418000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:31.426000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:31.440000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:31.487000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:31.525000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:31.530000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:31.631000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:36.488000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:36.697000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:36.781000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:36.938000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:36.989000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:37.048000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:37.199000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:37.280000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:37.445000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:37.545000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:37.591000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:37.777000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:37.860000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:38.116000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:38.318000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:38.389000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:40.234000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:40.259000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:40.313000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:40.340000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:40.364000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:40.395000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:40.664000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:40.718000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:40.747000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:40.796000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:40.799000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:40.847000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:41.692000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:41.772000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:41.823000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:42.297000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:42.378000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:42.404000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:42.430000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:42.484000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:42.514000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:42.536000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:42.594000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:42.646000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:44.675000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:44.755000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:44.861000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:45.224000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:45.304000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:45.357000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:45.409000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:45.425000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:45.437000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:45.504000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:45.542000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:45.608000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:46.299000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:46.343000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:46.379000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:46.422000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:46.484000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:46.525000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:46.788000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:46.867000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:46.887000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:46.966000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:46.971000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:47.069000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:47.621000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:47.634000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:47.911000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:47.925000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:48.191000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:48.205000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:48.575000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:48.769000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:48.872000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:48.964000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:49.061000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:49.166000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:49.252000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:49.307000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:49.366000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:49.473000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:49.537000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:49.553000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:49.598000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:49.765000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:49.845000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:49.881000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:49.893000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:49.960000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:50.011000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:50.059000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:50.063000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:50.090000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:50.139000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:50.192000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:50.867000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:50.949000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:51.052000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:51.227000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:51.307000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:51.422000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:51.524000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:51.577000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:51.605000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:51.659000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:51.709000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:51.732000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:51.771000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:51.803000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:51.814000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:51.885000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:51.920000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:51.991000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:54.059000 287 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f607982de60>
[rank4]:W1023 11:11:54.082000 287 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f607982dfe0>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:11:54 TP4] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1023 11:11:54.244000 290 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9d826e5aa0>
[rank7]:W1023 11:11:54.267000 290 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9d826e5a10>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:11:54 TP7] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1023 11:11:54.837000 285 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec47f4f18f0>
[rank2]:W1023 11:11:54.860000 285 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec47f4f1860>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:11:54 TP2] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1023 11:11:55.455000 284 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec9a89bd830>
[rank0]:W1023 11:11:55.467000 283 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dfcdd5740>
[rank1]:W1023 11:11:55.478000 284 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec9a89bd6e0>
[rank0]:W1023 11:11:55.490000 283 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1dfcdd56b0>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:11:55 TP1] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:11:55 TP0] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1023 11:11:55.593000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:55.621000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:55.857000 289 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef416a31e00>
[rank6]:W1023 11:11:55.880000 289 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef416a319b0>
[rank4]:W1023 11:11:55.884000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:55.913000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:55.928000 288 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ecd0c7f9ef0>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:11:55 TP6] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1023 11:11:55.952000 288 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ecd0c7fa130>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:11:56 TP5] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1023 11:11:56.057000 286 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f078774df20>
[rank3]:W1023 11:11:56.081000 286 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f078c800ae0>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:11:56 TP3] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1023 11:11:56.173000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:56.201000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:56.382000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:56.460000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:56.488000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:56.678000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:56.754000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:56.779000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:56.973000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:56.984000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:57.005000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:57.051000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:57.071000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:57.277000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:57.284000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:57.300000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:57.342000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:57.359000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:57.473000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:57.570000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:57.579000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:57.588000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:57.605000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:57.629000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:57.645000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:57.769000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:57.862000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:57.871000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:57.880000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:57.889000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:57.899000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:57.917000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:57.934000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:58.061000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:58.148000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:58.156000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:58.182000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:58.189000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:58.198000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:58.210000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:58.227000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:58.355000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:58.440000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:58.448000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:58.479000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:58.487000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:58.494000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:58.505000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:58.516000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:58.650000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:58.732000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:58.739000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:58.770000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:58.781000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:58.789000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:58.799000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:58.810000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:58.942000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:59.024000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:59.030000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:59.062000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:59.076000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:59.085000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:59.094000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:59.106000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:59.234000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:59.320000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:59.327000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:59.358000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:59.373000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:59.382000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:59.393000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:59.402000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:59.525000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:59.618000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:59.627000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:59.649000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:59.670000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:59.682000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:59.691000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:59.699000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:11:59.817000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:11:59.910000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:11:59.919000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:11:59.945000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:11:59.962000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:11:59.975000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:11:59.983000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:11:59.993000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:00.113000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:00.208000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:00.217000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:00.243000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:00.264000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:00.276000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:00.285000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:00.294000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:00.406000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:00.500000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:00.508000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:00.538000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:00.560000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:00.570000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:00.578000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:00.590000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:00.698000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:00.792000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:00.800000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:00.834000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:00.856000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:00.869000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:00.881000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:00.890000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:00.990000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:01.096000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:01.104000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:01.130000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:01.161000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:01.171000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:01.183000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:01.191000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:01.281000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:01.390000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:01.399000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:01.425000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:01.453000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:01.470000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:01.482000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:01.492000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:01.579000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:01.688000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:01.695000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:01.726000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:01.750000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:01.773000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:01.780000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:01.791000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:01.878000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:01.984000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:01.992000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:02.022000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:02.042000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:02.068000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:02.078000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:02.087000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:02.171000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:02.280000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:02.288000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:02.318000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:02.334000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:02.365000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:02.376000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:02.386000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:02.463000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:02.580000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:02.587000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:02.614000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:02.626000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:02.661000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:02.672000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:02.682000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:02.759000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:02.872000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:02.884000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:02.911000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:02.919000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:02.964000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:02.972000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:02.983000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:03.051000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:03.168000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:03.180000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:03.210000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:03.218000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:03.261000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:03.270000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:03.279000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:03.343000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:03.468000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:03.476000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:03.506000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:03.515000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:03.556000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:03.570000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:03.578000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:03.637000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:03.766000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:03.778000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:03.801000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:03.807000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:03.862000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:03.871000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:03.886000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:03.933000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:04.062000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:04.075000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:04.100000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:04.108000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:04.159000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:04.167000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:04.182000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:04.233000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:04.358000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:04.370000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:04.397000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:04.409000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:04.455000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:04.463000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:04.478000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:04.533000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:04.648000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:04.661000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:04.694000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:04.706000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:04.749000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:04.759000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:04.773000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:04.831000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:04.944000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:04.957000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:04.990000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:05.002000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:05.044000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:05.054000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:05.068000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:05.127000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:05.240000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:05.252000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:05.287000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:05.298000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:05.346000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:05.355000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:05.377000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:05.423000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:05.536000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:05.549000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:05.604000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:05.652000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:05.679000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:05.734000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:05.842000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:05.859000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:05.909000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:05.951000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:05.979000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:06.033000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:06.138000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:06.155000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:06.213000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:06.246000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:06.273000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:06.335000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:06.398000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:06.428000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:06.449000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:06.458000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:06.514000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:06.540000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:06.569000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:06.634000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:06.694000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:06.724000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:06.745000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:06.754000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:06.811000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:06.841000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:06.931000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:06.991000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:07.051000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:07.059000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:07.112000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:07.291000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:07.355000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:07.377000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:07.419000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:07.591000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:07.655000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:07.689000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:07.709000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:07.723000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:07.848000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:07.891000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:07.951000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:07.961000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:08.015000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:08.024000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:08.034000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:08.152000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:08.191000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:08.251000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:08.265000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:08.319000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:08.329000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:08.452000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:08.495000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:08.555000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:08.569000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:08.627000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:08.636000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:08.750000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:08.783000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:08.793000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:08.853000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:08.871000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:08.929000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:08.940000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:09.050000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:09.078000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:09.093000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:09.141000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:09.149000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:09.170000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:09.229000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:09.240000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:09.346000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:09.379000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:09.393000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:09.441000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:09.449000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:09.482000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:09.533000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:09.550000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:09.642000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:09.679000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:09.693000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:09.742000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:09.749000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:09.781000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:09.835000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:09.849000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:09.936000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:09.981000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:09.994000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:10.043000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:10.052000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:10.083000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:10.138000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:10.153000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:10.244000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:10.293000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:10.302000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:10.346000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:10.355000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:10.387000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:10.437000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:10.459000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:10.542000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:10.595000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:10.603000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:10.645000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:10.654000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:10.690000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:10.737000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:10.762000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:10.836000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:10.893000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:10.906000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:10.950000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:10.959000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:10.989000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:11.042000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:11.065000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:11.132000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:11.193000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:11.207000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:11.255000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:11.265000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:11.293000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:11.347000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:11.369000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:11.434000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:11.499000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:11.513000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:11.561000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:11.568000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:11.598000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:11.649000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:11.674000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:11.734000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:11.803000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:11.813000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:11.865000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:11.873000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:11.899000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:11.953000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:11.979000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:12.046000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:12.113000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:12.123000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:12.165000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:12.177000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:12.206000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:12.257000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:12.282000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:12.346000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:12.413000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:12.426000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:12.465000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:12.481000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:12.507000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:12.561000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:12.582000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:12.646000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:12.713000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:12.726000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:12.765000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:12.785000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:12.818000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:12.867000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:12.893000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:12.952000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1023 11:12:13.010000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:13.025000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1023 11:12:13.067000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:13.090000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:13.117000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:13.170000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:13.193000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:13.248000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:13.330000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:13.390000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:13.421000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:13.469000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:13.499000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:13.553000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:13.635000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:13.693000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:13.727000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:13.769000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:13.803000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:13.854000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:13.939000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:14.005000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:14.032000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:14.077000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1023 11:12:14.107000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:14.150000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:14.243000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:14.308000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:14.334000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:14.376000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1023 11:12:14.445000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:14.542000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:14.608000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1023 11:12:14.630000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:14.673000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:14.839000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:14.911000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1023 11:12:14.971000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1023 11:12:15.142000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:15.215000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1023 11:12:15.514000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
Capturing batches (bs=1 avail_mem=37.92 GB): 100%|| 52/52 [10:20<00:00, 66.67s/it]Capturing batches (bs=1 avail_mem=37.92 GB): 100%|| 52/52 [10:20<00:00, 11.93s/it]
[2025-10-23 11:12:21 TP0] Registering 6396 cuda graph addresses
[2025-10-23 11:12:23 TP4] Capture cuda graph end. Time elapsed: 622.44 s. mem usage=6.13 GB. avail mem=37.12 GB.
[2025-10-23 11:12:23 TP0] Capture cuda graph end. Time elapsed: 622.46 s. mem usage=6.13 GB. avail mem=37.48 GB.
[2025-10-23 11:12:23 TP7] Capture cuda graph end. Time elapsed: 622.44 s. mem usage=6.15 GB. avail mem=37.17 GB.
[2025-10-23 11:12:23 TP6] Capture cuda graph end. Time elapsed: 622.62 s. mem usage=6.14 GB. avail mem=37.18 GB.
[2025-10-23 11:12:23 TP3] Capture cuda graph end. Time elapsed: 622.48 s. mem usage=6.15 GB. avail mem=37.05 GB.
[2025-10-23 11:12:23 TP1] Capture cuda graph end. Time elapsed: 622.53 s. mem usage=6.18 GB. avail mem=37.02 GB.
[2025-10-23 11:12:23 TP5] Capture cuda graph end. Time elapsed: 622.53 s. mem usage=6.20 GB. avail mem=37.13 GB.
[2025-10-23 11:12:23 TP2] Capture cuda graph end. Time elapsed: 622.57 s. mem usage=6.14 GB. avail mem=37.05 GB.
[2025-10-23 11:12:23 TP0] max_total_num_tokens=971748, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=1024, context_len=163840, available_gpu_mem=37.48 GB
[2025-10-23 11:12:23] INFO:     Started server process [43]
[2025-10-23 11:12:23] INFO:     Waiting for application startup.
[2025-10-23 11:12:23] INFO:     Application startup complete.
[2025-10-23 11:12:23] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-10-23 11:12:24] INFO:     127.0.0.1:56038 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-23 11:12:24 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:26 TP4] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:26 TP1] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:26 TP0] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:26 TP5] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:26 TP2] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:26 TP7] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:26 TP6] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:26 TP3] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:26] INFO:     127.0.0.1:56054 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-23 11:12:29] INFO:     127.0.0.1:56046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:29] The server is fired up and ready to roll!
[2025-10-23 11:12:34] INFO:     127.0.0.1:56056 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-23 11:12:34 TP0] Prefill batch. #new-seq: 1, #new-token: 666, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP4] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP6] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP7] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP5] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP0] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP2] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP1] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP3] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34] INFO:     127.0.0.1:56064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:34 TP0] Prefill batch. #new-seq: 1, #new-token: 67, #cached-token: 667, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP4] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP1] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP6] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP7] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP0] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP3] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP5] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP2] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP0] Prefill batch. #new-seq: 45, #new-token: 2713, #cached-token: 30015, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP0] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP6] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP4] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP7] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP1] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP2] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP5] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:34 TP3] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:35 TP0] Prefill batch. #new-seq: 125, #new-token: 7653, #cached-token: 83626, token usage: 0.00, #running-req: 46, #queue-req: 0, 
[2025-10-23 11:12:35 TP0] Prefill batch. #new-seq: 166, #new-token: 9774, #cached-token: 111116, token usage: 0.01, #running-req: 171, #queue-req: 0, 
[2025-10-23 11:12:36 TP0] Prefill batch. #new-seq: 276, #new-token: 16369, #cached-token: 184774, token usage: 0.02, #running-req: 337, #queue-req: 37, 
[2025-10-23 11:12:37 TP0] Prefill batch. #new-seq: 277, #new-token: 16382, #cached-token: 185510, token usage: 0.04, #running-req: 613, #queue-req: 193, 
[2025-10-23 11:12:39 TP0] Prefill batch. #new-seq: 134, #new-token: 8391, #cached-token: 89757, token usage: 0.05, #running-req: 890, #queue-req: 295, 
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:40 TP0] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:40 TP0] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:40 TP0] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:40 TP0] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:40 TP0] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:40 TP0] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:40 TP3] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:40 TP3] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:40 TP3] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:40 TP3] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:40 TP3] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:40 TP3] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:40 TP1] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:40 TP1] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:40 TP1] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:40 TP1] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:40 TP1] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:40 TP1] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:40 TP2] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:40 TP2] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:41 TP2] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:41 TP2] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:41 TP2] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:41 TP2] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:41 TP5] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:41 TP5] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:41 TP5] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:41 TP5] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:41 TP5] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:41 TP5] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:41 TP6] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:41 TP6] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:41 TP6] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:41 TP6] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:41 TP6] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:41 TP6] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:41 TP4] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:41 TP4] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:41 TP4] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:41 TP4] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:41 TP4] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:41 TP4] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:41 TP7] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:41 TP7] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:41 TP7] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 11:12:41 TP7] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:41 TP7] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:41 TP7] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 11:12:45 TP0] Decode batch. #running-req: 1024, #token: 95338, token usage: 0.10, cuda graph: False, gen throughput (token/s): 1479.12, #queue-req: 295, 
[2025-10-23 11:12:45] INFO:     127.0.0.1:54540 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:45 TP3] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:45 TP4] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:45 TP7] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:45 TP5] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:45 TP1] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:45 TP0] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:45 TP2] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:45 TP6] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:45 TP0] Prefill batch. #new-seq: 1, #new-token: 80, #cached-token: 669, token usage: 0.10, #running-req: 1023, #queue-req: 294, 
[2025-10-23 11:12:45] INFO:     127.0.0.1:56848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:46 TP0] Prefill batch. #new-seq: 1, #new-token: 138, #cached-token: 671, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[aiter] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP7] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP6] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP4] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP0] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP2] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP5] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP1] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP3] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46] INFO:     127.0.0.1:54154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:46] INFO:     127.0.0.1:57550 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP0] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP4] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP1] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP5] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP3] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP7] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP2] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP6] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46] INFO:     127.0.0.1:53970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:46] INFO:     127.0.0.1:55104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:46] INFO:     127.0.0.1:58312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:46] INFO:     127.0.0.1:34084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:46 TP0] Prefill batch. #new-seq: 2, #new-token: 137, #cached-token: 1341, token usage: 0.11, #running-req: 1022, #queue-req: 291, 
[aiter] [fused_moe] using default for (137, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP4] [fused_moe] using default for (137, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (137, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP0] [fused_moe] using default for (137, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (137, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (137, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP5] [fused_moe] using default for (137, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP1] [fused_moe] using default for (137, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (137, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (137, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP7] [fused_moe] using default for (137, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP3] [fused_moe] using default for (137, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (137, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP2] [fused_moe] using default for (137, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (137, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP6] [fused_moe] using default for (137, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46] INFO:     127.0.0.1:54848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:46] INFO:     127.0.0.1:56000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:46] INFO:     127.0.0.1:56682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:46] INFO:     127.0.0.1:59542 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP4] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP5] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP3] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP1] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP0] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP7] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP2] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:46 TP6] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP0] Prefill batch. #new-seq: 8, #new-token: 626, #cached-token: 5358, token usage: 0.11, #running-req: 1016, #queue-req: 283, 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP2] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP3] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP4] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP7] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP0] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP1] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP5] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP6] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47] INFO:     127.0.0.1:54888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:47] INFO:     127.0.0.1:56830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:47] INFO:     127.0.0.1:58394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:47] INFO:     127.0.0.1:59564 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP3] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP0] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP4] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP7] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP1] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP5] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP2] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP6] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP0] Prefill batch. #new-seq: 4, #new-token: 247, #cached-token: 2683, token usage: 0.11, #running-req: 1020, #queue-req: 279, 
[aiter] [fused_moe] using default for (247, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP0] [fused_moe] using default for (247, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (247, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (247, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP7] [fused_moe] using default for (247, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (247, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP4] [fused_moe] using default for (247, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (247, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP3] [fused_moe] using default for (247, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP1] [fused_moe] using default for (247, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (247, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP5] [fused_moe] using default for (247, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (247, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP2] [fused_moe] using default for (247, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (247, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP6] [fused_moe] using default for (247, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47] INFO:     127.0.0.1:54434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:47] INFO:     127.0.0.1:54612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:47] INFO:     127.0.0.1:54726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:47] INFO:     127.0.0.1:56240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:47] INFO:     127.0.0.1:58238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:47] INFO:     127.0.0.1:58270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:47] INFO:     127.0.0.1:58366 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP3] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP7] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP0] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP4] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP2] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP1] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP5] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP6] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP0] Prefill batch. #new-seq: 7, #new-token: 376, #cached-token: 4693, token usage: 0.11, #running-req: 1017, #queue-req: 272, 
[aiter] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP0] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP1] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP3] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP7] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP4] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP5] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP2] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP6] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47] INFO:     127.0.0.1:54566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:47] INFO:     127.0.0.1:55514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:47] INFO:     127.0.0.1:57256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:47] INFO:     127.0.0.1:58124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:47] INFO:     127.0.0.1:58940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:47] INFO:     127.0.0.1:34212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:47] INFO:     127.0.0.1:34216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:47] INFO:     127.0.0.1:34322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:47 TP0] Prefill batch. #new-seq: 8, #new-token: 449, #cached-token: 5360, token usage: 0.11, #running-req: 1016, #queue-req: 264, 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP0] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP7] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP1] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP3] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP5] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP4] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP2] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP6] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47] INFO:     127.0.0.1:57122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:47] INFO:     127.0.0.1:57700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:47] INFO:     127.0.0.1:59096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:47] INFO:     127.0.0.1:33536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:47 TP0] Prefill batch. #new-seq: 4, #new-token: 196, #cached-token: 2680, token usage: 0.11, #running-req: 1020, #queue-req: 260, 
[aiter] [fused_moe] using default for (196, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (196, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP0] [fused_moe] using default for (196, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP7] [fused_moe] using default for (196, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (196, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (196, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (196, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP1] [fused_moe] using default for (196, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (196, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP3] [fused_moe] using default for (196, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP5] [fused_moe] using default for (196, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP4] [fused_moe] using default for (196, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (196, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP2] [fused_moe] using default for (196, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (196, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47 TP6] [fused_moe] using default for (196, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:47] INFO:     127.0.0.1:59808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48 TP0] Prefill batch. #new-seq: 1, #new-token: 36, #cached-token: 669, token usage: 0.11, #running-req: 1023, #queue-req: 259, 
[aiter] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP2] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP0] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP7] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP6] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP3] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP5] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP4] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP1] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48] INFO:     127.0.0.1:54534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:58222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:58794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:59622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:33138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:33358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:33656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:34262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48 TP0] Prefill batch. #new-seq: 8, #new-token: 412, #cached-token: 5361, token usage: 0.11, #running-req: 1016, #queue-req: 251, 
[aiter] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP0] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP4] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP6] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP7] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP2] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP3] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP5] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP1] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48] INFO:     127.0.0.1:53944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:56442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:56584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:56802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:59036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:59784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:60594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:32904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:33216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:33490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:33746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:34642 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP2] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP3] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP4] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP1] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP5] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP7] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP0] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP6] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP0] Prefill batch. #new-seq: 12, #new-token: 901, #cached-token: 8039, token usage: 0.11, #running-req: 1012, #queue-req: 239, 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP0] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP3] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP7] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP1] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP5] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP4] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP2] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP6] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48] INFO:     127.0.0.1:54426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:54448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:54874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:57400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:58744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:60930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:33188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48 TP0] Prefill batch. #new-seq: 7, #new-token: 379, #cached-token: 4690, token usage: 0.11, #running-req: 1017, #queue-req: 232, 
[aiter] [fused_moe] using default for (379, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP0] [fused_moe] using default for (379, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (379, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP4] [fused_moe] using default for (379, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (379, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP3] [fused_moe] using default for (379, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (379, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP7] [fused_moe] using default for (379, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (379, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP1] [fused_moe] using default for (379, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (379, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP5] [fused_moe] using default for (379, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (379, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP2] [fused_moe] using default for (379, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (379, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP6] [fused_moe] using default for (379, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48] INFO:     127.0.0.1:55078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:55090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:55246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:58236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:58754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:59440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48] INFO:     127.0.0.1:34546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:48 TP0] Prefill batch. #new-seq: 7, #new-token: 456, #cached-token: 4692, token usage: 0.11, #running-req: 1017, #queue-req: 225, 
[aiter] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP4] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP2] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP6] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP3] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP0] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP1] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP7] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:48 TP5] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49] INFO:     127.0.0.1:55902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:58700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:59306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:59998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:33396 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP0] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP4] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP3] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP2] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP7] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP6] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP1] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP5] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP0] Prefill batch. #new-seq: 5, #new-token: 315, #cached-token: 3351, token usage: 0.12, #running-req: 1019, #queue-req: 220, 
[aiter] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP3] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP0] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP7] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP4] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP2] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP6] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP1] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP5] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49] INFO:     127.0.0.1:55056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:55116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:55274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:55620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:56046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:57854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:59102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:60016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:60162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:33500 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP3] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP7] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP0] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP4] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP2] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP6] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP1] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP5] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP0] Prefill batch. #new-seq: 10, #new-token: 518, #cached-token: 6695, token usage: 0.12, #running-req: 1014, #queue-req: 210, 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP0] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP7] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP3] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP4] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP2] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP6] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP5] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP1] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49] INFO:     127.0.0.1:55030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:56180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:57242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:57534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:58058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:58942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:59712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:33542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:34066 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP3] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP7] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP0] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP4] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP2] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP1] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP6] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP5] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP0] Prefill batch. #new-seq: 9, #new-token: 417, #cached-token: 6028, token usage: 0.12, #running-req: 1015, #queue-req: 201, 
[aiter] [fused_moe] using default for (417, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP0] [fused_moe] using default for (417, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (417, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP7] [fused_moe] using default for (417, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (417, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP3] [fused_moe] using default for (417, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (417, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP4] [fused_moe] using default for (417, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (417, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP1] [fused_moe] using default for (417, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (417, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP5] [fused_moe] using default for (417, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (417, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP2] [fused_moe] using default for (417, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (417, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP6] [fused_moe] using default for (417, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49] INFO:     127.0.0.1:54122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:55312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:55548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:56530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:58534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:58808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:60570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49 TP0] Prefill batch. #new-seq: 7, #new-token: 480, #cached-token: 4690, token usage: 0.12, #running-req: 1017, #queue-req: 194, 
[2025-10-23 11:12:49] INFO:     127.0.0.1:53938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:55658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:55680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:55728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:56342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:57046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:59810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:59916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:60080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:60218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:60678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:60780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:33828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:49] INFO:     127.0.0.1:34384 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP3] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP7] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP2] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP6] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP0] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP4] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP1] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:49 TP5] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP0] Prefill batch. #new-seq: 14, #new-token: 832, #cached-token: 9377, token usage: 0.12, #running-req: 1010, #queue-req: 180, 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP4] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP3] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP7] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP0] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP2] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP1] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP5] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP6] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50] INFO:     127.0.0.1:55344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:55858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:56086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:56172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:57342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:58548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:59362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50 TP0] Prefill batch. #new-seq: 7, #new-token: 374, #cached-token: 4690, token usage: 0.12, #running-req: 1017, #queue-req: 173, 
[aiter] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP7] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP4] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP0] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP3] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP5] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP1] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP6] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP2] [fused_moe] using default for (374, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50] INFO:     127.0.0.1:55390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:56488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:56892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:57266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:59694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:33368 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP7] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP4] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP3] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP5] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP0] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP1] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP6] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP2] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP0] Prefill batch. #new-seq: 6, #new-token: 288, #cached-token: 4018, token usage: 0.12, #running-req: 1018, #queue-req: 167, 
[2025-10-23 11:12:50] INFO:     127.0.0.1:59868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:60202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:32864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:33156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:33504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:34190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50 TP0] Prefill batch. #new-seq: 6, #new-token: 308, #cached-token: 4022, token usage: 0.12, #running-req: 1018, #queue-req: 161, 
[aiter] [fused_moe] using default for (308, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP0] [fused_moe] using default for (308, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (308, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (308, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP7] [fused_moe] using default for (308, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (308, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP4] [fused_moe] using default for (308, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (308, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (308, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP1] [fused_moe] using default for (308, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP5] [fused_moe] using default for (308, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP3] [fused_moe] using default for (308, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (308, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP2] [fused_moe] using default for (308, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (308, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP6] [fused_moe] using default for (308, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50] INFO:     127.0.0.1:55820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:56438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:57442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:58872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:58918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:60436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:33038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:33678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:34518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50 TP0] Prefill batch. #new-seq: 9, #new-token: 564, #cached-token: 6027, token usage: 0.12, #running-req: 1015, #queue-req: 152, 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP0] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP7] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP3] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP4] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP1] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP5] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP2] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP6] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50] INFO:     127.0.0.1:54882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:55202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:55690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:56148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:58194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:59526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:60402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:60912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:32948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:33310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:50] INFO:     127.0.0.1:33760 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP3] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP7] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP0] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP4] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP1] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP5] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP2] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:50 TP6] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP0] Prefill batch. #new-seq: 11, #new-token: 739, #cached-token: 7371, token usage: 0.12, #running-req: 1013, #queue-req: 141, 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP4] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP3] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP7] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP0] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP5] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP1] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP2] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP6] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51] INFO:     127.0.0.1:54074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:55424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:55630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:56134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:58890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:58928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:59368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:59442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:59582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:60880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:32928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51 TP0] Prefill batch. #new-seq: 11, #new-token: 551, #cached-token: 7372, token usage: 0.12, #running-req: 1013, #queue-req: 130, 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP7] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP0] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP3] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP1] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP4] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP5] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP2] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP6] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51] INFO:     127.0.0.1:54374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:55368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:55518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:56572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:56640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:59050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:60740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:33570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51 TP0] Prefill batch. #new-seq: 8, #new-token: 560, #cached-token: 5358, token usage: 0.12, #running-req: 1016, #queue-req: 122, 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP0] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP3] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP1] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP7] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP4] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP5] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP2] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP6] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51] INFO:     127.0.0.1:56104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:56260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:57418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:58210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:58650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:59822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:60010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:60670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51 TP0] Prefill batch. #new-seq: 8, #new-token: 546, #cached-token: 5361, token usage: 0.12, #running-req: 1016, #queue-req: 114, 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP2] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP3] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP0] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP1] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP4] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP7] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP6] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP5] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51] INFO:     127.0.0.1:54544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:54750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:54780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:55118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:56956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:56994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:57980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:59258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:33444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:33590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51] INFO:     127.0.0.1:33598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:51 TP0] Prefill batch. #new-seq: 11, #new-token: 769, #cached-token: 7374, token usage: 0.12, #running-req: 1013, #queue-req: 103, 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP0] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP7] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP3] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP4] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP1] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP5] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP2] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:51 TP6] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52] INFO:     127.0.0.1:54196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:54934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:55066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:55324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:55472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:55668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:56810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:56942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:57664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:58838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:58868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:60932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:33640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:33906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:34452 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP0] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP4] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP3] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP1] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP5] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP7] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP2] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP6] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP0] Prefill batch. #new-seq: 15, #new-token: 903, #cached-token: 10052, token usage: 0.12, #running-req: 1009, #queue-req: 88, 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP0] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP4] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP1] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP3] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP5] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP7] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP2] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP6] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52] INFO:     127.0.0.1:54356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:54618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:55888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:56114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:56522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:56818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:33612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:34002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:34096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52 TP0] Prefill batch. #new-seq: 9, #new-token: 465, #cached-token: 6029, token usage: 0.12, #running-req: 1015, #queue-req: 79, 
[aiter] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP7] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP0] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP4] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP5] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP3] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP1] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP6] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP2] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52] INFO:     127.0.0.1:54112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:54498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:55048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:56286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:56444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:56970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:57290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:58524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:59628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:60810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:32952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:33588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52 TP0] Prefill batch. #new-seq: 12, #new-token: 838, #cached-token: 8036, token usage: 0.12, #running-req: 1012, #queue-req: 67, 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP4] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP0] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP3] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP7] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP1] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP5] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP2] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP6] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52] INFO:     127.0.0.1:54484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:55588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:56590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:57086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:57460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:57648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:58480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:59338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:59412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:59534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:59548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:59786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:60606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:60630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:33366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:33398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:33420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:33722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:33914 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP0] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP3] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP1] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP7] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP4] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP5] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP2] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP6] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:52 TP0] Prefill batch. #new-seq: 19, #new-token: 1291, #cached-token: 12726, token usage: 0.12, #running-req: 1005, #queue-req: 48, 
[2025-10-23 11:12:52] INFO:     127.0.0.1:55836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:57328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:57838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:58116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:58260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:58624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:59452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:59660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:60096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:52] INFO:     127.0.0.1:34024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53 TP0] Prefill batch. #new-seq: 10, #new-token: 568, #cached-token: 6698, token usage: 0.13, #running-req: 1014, #queue-req: 38, 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP7] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP3] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP4] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP0] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP5] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP1] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP6] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP2] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53] INFO:     127.0.0.1:56378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:56612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:56752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:57642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:58974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:59200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:59236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:59770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:60110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:60854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:32852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:32894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:34412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:34686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53 TP0] Prefill batch. #new-seq: 14, #new-token: 801, #cached-token: 9384, token usage: 0.12, #running-req: 1010, #queue-req: 24, 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP7] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP0] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP3] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP4] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP1] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP5] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP2] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP6] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53] INFO:     127.0.0.1:54560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:54684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:54808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:54870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:57070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:57312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:57860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:58288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:58484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:59294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:60254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:60560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:60676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:32808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53 TP0] Prefill batch. #new-seq: 14, #new-token: 905, #cached-token: 9376, token usage: 0.12, #running-req: 1010, #queue-req: 10, 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP7] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP3] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP1] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP0] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP4] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP5] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP2] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP6] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP0] Decode batch. #running-req: 1010, #token: 120525, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5101.75, #queue-req: 10, 
[2025-10-23 11:12:53] INFO:     127.0.0.1:54452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:54776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:55378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:55732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:56366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:57088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:57716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:58668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:59928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:60446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:60526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:33472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:33850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:34530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53 TP0] Prefill batch. #new-seq: 10, #new-token: 536, #cached-token: 6704, token usage: 0.13, #running-req: 1010, #queue-req: 0, 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP0] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP3] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP4] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP7] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP5] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP1] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP2] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP6] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53] INFO:     127.0.0.1:54234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:55060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:57452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:58126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:59058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:59866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:34154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:53] INFO:     127.0.0.1:34562 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP3] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP4] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP7] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP0] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP1] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP5] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP2] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:53 TP6] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54] INFO:     127.0.0.1:54270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:55430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:57158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:58330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:59644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:59718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:60376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:60620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:33296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:33718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:34302 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP3] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP7] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP4] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP1] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP5] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP0] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP2] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP6] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54] INFO:     127.0.0.1:55020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:55500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:55578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:56554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:58494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:58552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:60118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:60330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:60390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:33676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:33814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:34376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:34682 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP3] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP7] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP4] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP1] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP5] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP0] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP2] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP6] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54] INFO:     127.0.0.1:55410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:56090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:56456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:57440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:58174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:58414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:59652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:59836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:60718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:32792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:32966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:33530 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP4] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP3] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP7] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP1] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP5] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP0] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP2] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP6] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54] INFO:     127.0.0.1:54404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:54738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:55166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:55800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:56198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:56908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:57820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:58550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:59908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:60450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:34064 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP3] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP7] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP1] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP4] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP5] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP0] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP2] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP6] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54] INFO:     127.0.0.1:54340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:54960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:58426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:58946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:58990 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP4] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP7] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP5] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP3] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP1] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP0] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP2] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP6] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54] INFO:     127.0.0.1:54690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:55908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:58448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:59002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:59170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:60704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:33172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:33300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:33706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:33766 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP7] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP4] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP3] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP1] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP5] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP0] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP2] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP6] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54] INFO:     127.0.0.1:56504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:58272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:59064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:33666 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP4] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP3] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP7] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP1] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP5] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP0] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP2] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP6] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54] INFO:     127.0.0.1:54626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:55094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:55330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:57464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:59888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:60196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:54] INFO:     127.0.0.1:34544 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP3] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP4] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP7] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP1] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP5] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP0] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP2] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:54 TP6] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55] INFO:     127.0.0.1:55854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:57996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:59398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:59972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:60516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:60960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:60964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:32974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:33114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:33252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:33780 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP3] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP4] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP7] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP5] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP1] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP0] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP2] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP6] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55] INFO:     127.0.0.1:54706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:55998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:57840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:58342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:58780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:60124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:34312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:34510 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP3] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP4] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP7] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP1] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP5] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP0] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP2] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP6] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55] INFO:     127.0.0.1:54660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:55180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:55768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:56512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:57472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:57546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:58328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:58906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:34016 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP3] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP1] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP0] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP2] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP6] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP4] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP7] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP5] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55] INFO:     127.0.0.1:54420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:54528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:56656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:56984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:57238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:57786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:58026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:59938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:60686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:32944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:33460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:33538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:34834 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP7] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP4] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP0] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP2] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP6] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP3] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP5] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP1] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55] INFO:     127.0.0.1:57374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:57406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:57754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:58306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:59178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:59270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:60978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:33234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:33994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:34290 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP2] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP4] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP6] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP0] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP3] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP7] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP1] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP5] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55] INFO:     127.0.0.1:54008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:54396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:54580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:54858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:55122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:56618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:57508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:57732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:59710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:60076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:60168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:60410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:34040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:55] INFO:     127.0.0.1:34332 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP2] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP4] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP3] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP1] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP0] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP7] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP6] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:55 TP5] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56] INFO:     127.0.0.1:54604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:55172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:56344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:57202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:57438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:57542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:58812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:58960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:59238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:59382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:60266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:60796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:33514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:33642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:33976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:34618 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP4] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP0] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP1] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP3] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP5] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP2] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP7] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP6] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56] INFO:     127.0.0.1:53980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:59082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:59838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:60608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:33600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:34356 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP4] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP0] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP2] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP6] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP3] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP7] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP1] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP5] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56] INFO:     127.0.0.1:54468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:54978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:55436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:56232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:56272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:56788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:56840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:57358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:59756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:60654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:60688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:33328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:33384 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP2] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP4] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP6] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP0] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP3] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP7] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP1] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP5] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56] INFO:     127.0.0.1:54898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:54950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:55144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:55194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:55486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:56636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:57346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:57876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:59308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:59312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:60440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:33072 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP2] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP6] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP4] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP0] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP3] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP7] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP1] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP5] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56] INFO:     127.0.0.1:54248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:55230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:58162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:58412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:59688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:60240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:60752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:60896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:33932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:34112 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (815, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP2] [fused_moe] using default for (815, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (815, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP4] [fused_moe] using default for (815, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (815, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP6] [fused_moe] using default for (815, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (815, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP0] [fused_moe] using default for (815, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (815, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP3] [fused_moe] using default for (815, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (815, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (815, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP7] [fused_moe] using default for (815, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (815, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP1] [fused_moe] using default for (815, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP5] [fused_moe] using default for (815, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56] INFO:     127.0.0.1:55258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:56880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:57908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:58488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:59016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:59194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:59858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:32880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:33344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:33562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:33854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:35248 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP2] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP6] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP4] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP0] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP1] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP3] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP7] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP5] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56] INFO:     127.0.0.1:54228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:55008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:56544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:58572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:59112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:59140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:59870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:35450 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP4] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP6] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP2] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP0] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP7] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP3] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP5] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP1] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56] INFO:     127.0.0.1:54202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:55236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:55808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:57120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:57204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:57872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:57916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:58508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:34806 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP6] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP4] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP7] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP2] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP3] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP0] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP5] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56 TP1] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:56] INFO:     127.0.0.1:54324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:57430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:57484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:58076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:58182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:58614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:58822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:59054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:59488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:59618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:59802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:60180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:33016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:33168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:34144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:34578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:56] INFO:     127.0.0.1:34862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:54282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:55016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:55158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:56490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:56854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:58704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:59594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:60792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:60868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:34372 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP2] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP4] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP0] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP6] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP1] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP5] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP7] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP3] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57] INFO:     127.0.0.1:54138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:57998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:58684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:59498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:60200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:34498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:35884 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP2] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP3] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP1] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP0] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP6] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP7] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP4] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP5] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP3] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP2] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP6] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP0] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP4] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP1] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP5] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP7] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP6] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP4] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP7] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP5] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP2] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP0] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP3] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP1] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57] INFO:     127.0.0.1:54308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:55370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:56860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:58468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:58652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:33092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:34490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:34616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:34700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:35372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:35894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:54820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:56720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:57114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:57332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:58292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:34256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:34482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:34918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:54212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:56150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:56422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:58352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:34160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:34648 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP4] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP5] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP7] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP6] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP0] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP2] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP3] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP1] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57] INFO:     127.0.0.1:54022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:54274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:54512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:55268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:55982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:56600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:57146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:57524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:57780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:58678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:59188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:59198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:59514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:60346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:33078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:33316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:34776 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP4] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP6] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP5] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP7] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP2] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP1] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP3] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP0] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57] INFO:     127.0.0.1:54996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:57794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:58424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:58436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:60006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:33454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:33692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:34150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:35616 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP2] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP6] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP4] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP7] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP3] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP0] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP5] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP1] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57] INFO:     127.0.0.1:54490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:55234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:56828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:57186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:57316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:57604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:57738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:57896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:58592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:60042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:60834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:60938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:32826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:33226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:57] INFO:     127.0.0.1:35830 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP0] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP4] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP7] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP3] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP5] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP1] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP2] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:57 TP6] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58] INFO:     127.0.0.1:55536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:56316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:56520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:57808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:58984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:60064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:33140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:33242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:33942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:35070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:35528 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP0] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP4] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP1] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP5] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP2] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP6] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP3] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP7] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58] INFO:     127.0.0.1:54044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:54058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:57640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:58380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:59220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:59300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:32896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:34224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:34440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:34992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:36558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:37012 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP0] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP4] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP2] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP6] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP3] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP7] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP1] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP5] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58] INFO:     127.0.0.1:54114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:55362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:55470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:56702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:56938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:57058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:58764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:58902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:60198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:33264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:34082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:34132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:34418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:35578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:35866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:36058 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP4] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP0] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP2] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP1] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP6] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP5] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP3] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP7] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58] INFO:     127.0.0.1:53974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:54218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:56206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:58112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:59100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:60492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:33432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:33890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:34760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:35334 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP4] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP0] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP2] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP6] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP1] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP3] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP5] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP7] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58] INFO:     127.0.0.1:54912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:56016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:56040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:56182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:58422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:58732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:59472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:60476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:33100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:34044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:34176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:34702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:35610 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP4] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP0] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP2] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP1] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP3] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP6] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP5] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP7] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58] INFO:     127.0.0.1:54168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:54302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:56678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:56714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:57102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:57830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:58142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:58396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:58924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:59790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:59900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:34854 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP4] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP0] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP2] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP6] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP3] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP1] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP5] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP7] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58] INFO:     127.0.0.1:54100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:54828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:55564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:55980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:59728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:60848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:33966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:34422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:34666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:36004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:37134 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP0] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP4] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP6] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP2] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP3] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP1] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP7] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP5] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP0] Decode batch. #running-req: 610, #token: 91774, token usage: 0.09, cuda graph: False, gen throughput (token/s): 6400.53, #queue-req: 0, 
[2025-10-23 11:12:58] INFO:     127.0.0.1:59204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:59732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:33988 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP4] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP0] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP6] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP2] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP3] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP5] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP7] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP1] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58] INFO:     127.0.0.1:58718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:60318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:60430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:60586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:34788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:58] INFO:     127.0.0.1:36432 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP6] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP4] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP2] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP0] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP7] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP3] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP5] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:58 TP1] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59] INFO:     127.0.0.1:54236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:54362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:55934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:56226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:57024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:59216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:59814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:60762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:34626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:35036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:35496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:36196 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP4] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP2] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP6] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP7] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP3] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP0] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP1] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP5] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59] INFO:     127.0.0.1:54980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:55878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:33128 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP2] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP6] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP4] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP0] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP3] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP7] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP1] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP5] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59] INFO:     127.0.0.1:54036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:55222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:55310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:56330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:57010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:57226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:57572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:59322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:60362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:60642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:60820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:33046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:33060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:34266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:35056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:37060 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP4] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP0] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP3] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP2] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP7] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP6] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP1] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP5] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59] INFO:     127.0.0.1:54260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:55646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:56662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:58042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:58332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:59286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:60922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:34556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:35006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:35132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:35308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:36152 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP4] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP2] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP6] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP0] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP3] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP7] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP1] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP5] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59] INFO:     127.0.0.1:54086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:57074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:59028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:59682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:60414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:34220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:34906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:35448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:36598 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP2] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP4] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP6] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP3] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP0] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP7] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP1] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP5] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59] INFO:     127.0.0.1:53978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:56216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:56306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:57000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:57582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:60906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:34244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:35600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:35636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:36428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:36520 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP2] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP6] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP4] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP3] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP0] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP7] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP1] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP5] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59] INFO:     127.0.0.1:55604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:55796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:57388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:58790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:33734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:35684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:35820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:36000 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP2] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP6] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP4] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP3] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP0] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP7] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP1] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59 TP5] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:12:59] INFO:     127.0.0.1:54878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:55966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:56388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:58078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:60026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:33200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:35122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:35480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:35646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:54698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:56626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:56692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:57562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:58128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:58232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:58578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:59378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:59824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:33000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:33050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:33274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:34196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:34234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:35172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:35540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:35948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:36254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:53996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:54416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:55300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:55662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:33756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:33876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:35890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:37034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:54594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:54672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:55924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:58010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:35030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:36298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:55456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:55746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:59176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:59886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:32912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:33018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:33176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:35772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:12:59] INFO:     127.0.0.1:36652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:58604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:60158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:33152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:35510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:35674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:36394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:56800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:59990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:33248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:33572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:34590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:34798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:36226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:54982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:56746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:35706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:35938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:55756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:56018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:57170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:57800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:58062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:58558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:59648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:33250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:34712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:34908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:35658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:53954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:53982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:59840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:34282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:35512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:35978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:36308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:37282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:55404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:57958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:58276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:59280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:59596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:60302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:33628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:35040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:36804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:36940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:37296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:55782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:56644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:57098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:59850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:60052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:35368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:36440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:37088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:57220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:57274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:58824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:60460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:36754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:37156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:57540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:57858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:58250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:59124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:32986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:34768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:36214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:36364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:36508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:54962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:55712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:56412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:56778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:57586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:35922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:36662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:36732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:37266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:59970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:35084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:35458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:36486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:36788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:56462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:56596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:58640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:59954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:60786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:33390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:33796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:36162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:36238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:54918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:56764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:60592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:60882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:32842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:33840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:34120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:35514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:35760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:36612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:55842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:59944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:60542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:60994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:33288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:36636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:36906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:36990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:55468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:57692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:59150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:60730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:33014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:34170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:34982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:35568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:35632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:35748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:36456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:00] INFO:     127.0.0.1:36556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:54524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:55704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:58090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:60156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:60550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:33546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:35018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:36074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:37196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:54108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:58144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:33674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:34444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:34870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:35694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:36482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:59266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:59546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:34072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:35466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:35728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:36500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:37386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:55942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:56024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:58186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:33394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:34822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:35356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:35584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:35664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:36402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:36998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:56300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:34726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:34892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:34934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:35406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:36526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:54150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:59470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:59980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:33944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:36378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:55034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:55358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:55524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:59346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:35284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:36510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:37364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:55950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:56242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:57428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:58240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:59152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:35892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:36866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:37308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:55520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:58106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:35388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:35864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:37068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:55136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:56352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:34346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:37262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:57668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:35058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:35316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:37374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01 TP0] Decode batch. #running-req: 276, #token: 51897, token usage: 0.05, cuda graph: True, gen throughput (token/s): 6033.41, #queue-req: 0, 
[2025-10-23 11:13:01] INFO:     127.0.0.1:54368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:60132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:60548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:37354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:57298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:57486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:60008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:34812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:35202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:35268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:35908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:36442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:36582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:36744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:37096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:35098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:55218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:55826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:55864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:34940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:36168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:36354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:37338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:55442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:56246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:32848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:35560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:37024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:37214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:56474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:35076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:36048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:01] INFO:     127.0.0.1:36358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:54040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:57036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:59672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:35946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:59458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:32820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:37168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:37220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:54556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:56558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:56856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:33032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:36388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:37322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:54384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:59070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:59234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:33950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:36194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:37050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:55290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:57190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:57588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:34660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:36182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:36986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:57494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:36080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:36720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:36956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:60552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:34320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:36698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:33800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:35218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:56400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:33638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:36988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:57882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:57964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:59250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:35752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:35850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:37064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:37120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:56156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:58286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:58880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:59752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:32778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:35110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:36148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:36560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:36936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:54642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:59568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:60288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:34922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:36774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:36852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:53990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:59310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:60276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:35168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:56074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:57132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:35328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:35422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:37106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:54220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:58282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:58590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:59040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:35734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:35794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:36202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:36306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:36592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:37180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:32776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:35872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:36670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:57846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:35296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:02] INFO:     127.0.0.1:54182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:56062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:60862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:33414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:35332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:36316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:37238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:56122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:58158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:35302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:36878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:37040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:37080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:35846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:36444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:36928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:54782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:60264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:36474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:36536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:56866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:34860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:35158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:35234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:36106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:36840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:35366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:35146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:36464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:37178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:35972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:36686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:35778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:37320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:54792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:60508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:34600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:59428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:35790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:36706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:36932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:34978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:59062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:34050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:35186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:35986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03 TP0] Decode batch. #running-req: 113, #token: 25683, token usage: 0.03, cuda graph: True, gen throughput (token/s): 3813.22, #queue-req: 0, 
[2025-10-23 11:13:03] INFO:     127.0.0.1:54656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:59608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:33486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:36026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:56162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:56734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:57556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:59156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:35810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:59956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:36096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:36274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:36762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:34842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:54880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:34880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:36550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:37234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:34468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:34808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:36620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:37200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:58458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:36288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:54286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:55064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:36326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:54762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:59736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:37090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:57926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:60566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:03] INFO:     127.0.0.1:36212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:36118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:58854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:35556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:37358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:35720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:37142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:54710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:57768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:33002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:35364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:36018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:36824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:35498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:36714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:32866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:35788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:36362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:36658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:37280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:54844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:36812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:37384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:33862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:36132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:35022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:60146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:36476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:56922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:58226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:35630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:36890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:36908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:35704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:34396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:36418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:60946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:34732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:04] INFO:     127.0.0.1:34968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05 TP0] Decode batch. #running-req: 38, #token: 9936, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1824.60, #queue-req: 0, 
[2025-10-23 11:13:05] INFO:     127.0.0.1:54944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:60234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:33924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:35254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:37212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:35394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:36258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:35964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:59144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:57616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:35656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:36708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:34748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:34956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:36752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:35432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:37250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:37128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:55480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:56546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:36140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:35346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:36914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:34436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:36340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:36570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:36044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:54956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:35270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:05] INFO:     127.0.0.1:57942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:06] INFO:     127.0.0.1:36970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:06 TP0] Decode batch. #running-req: 8, #token: 3019, token usage: 0.00, cuda graph: True, gen throughput (token/s): 805.90, #queue-req: 0, 
[2025-10-23 11:13:06] INFO:     127.0.0.1:35488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:06] INFO:     127.0.0.1:60770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:06] INFO:     127.0.0.1:32868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:06] INFO:     127.0.0.1:36032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:06] INFO:     127.0.0.1:34620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:06 TP0] Decode batch. #running-req: 2, #token: 1378, token usage: 0.00, cuda graph: True, gen throughput (token/s): 192.57, #queue-req: 0, 
[2025-10-23 11:13:06] INFO:     127.0.0.1:36180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:06] INFO:     127.0.0.1:34782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:13:19] INFO:     127.0.0.1:60178 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-23 11:13:19 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-10-23 11:13:20 TP7] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-10-23 11:13:20 TP0] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-10-23 11:13:20 TP1] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-10-23 11:13:20 TP3] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-10-23 11:13:21 TP2] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-10-23 11:13:21 TP4] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] start build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-10-23 11:13:21 TP5] start build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-10-23 11:13:21 TP6] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for gfx942.
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for host.
[92mSuccessfully preprocessed all matching files.[0m
[aiter] finish build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip], cost 53.64828743s
[2025-10-23 11:14:14 TP5] finish build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip], cost 53.64828743s
[2025-10-23 11:14:14] INFO:     127.0.0.1:60188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:14 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 11:14:14 TP0] Prefill batch. #new-seq: 41, #new-token: 41, #cached-token: 29760, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:14 TP0] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:14 TP2] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:14 TP4] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:14 TP6] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:14 TP3] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:14 TP1] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:14 TP5] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:14 TP7] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:14 TP0] Prefill batch. #new-seq: 46, #new-token: 46, #cached-token: 33445, token usage: 0.01, #running-req: 42, #queue-req: 0, 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:14 TP0] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:14 TP2] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:14 TP3] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:14 TP1] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:14 TP6] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:14 TP5] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:14 TP7] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:14 TP4] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP0] Prefill batch. #new-seq: 52, #new-token: 52, #cached-token: 38055, token usage: 0.01, #running-req: 88, #queue-req: 0, 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP7] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP2] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP3] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP0] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP6] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP4] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP1] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP5] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP0] Prefill batch. #new-seq: 53, #new-token: 53, #cached-token: 38637, token usage: 0.01, #running-req: 140, #queue-req: 0, 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP5] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP0] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP7] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP6] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP3] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP4] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP1] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP2] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 44328, token usage: 0.02, #running-req: 193, #queue-req: 0, 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP2] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP3] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP0] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP7] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP1] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP6] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP4] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP5] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP0] Prefill batch. #new-seq: 66, #new-token: 66, #cached-token: 47950, token usage: 0.02, #running-req: 254, #queue-req: 0, 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP3] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP2] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP1] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP0] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP7] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP4] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP6] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP5] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 45138, token usage: 0.02, #running-req: 320, #queue-req: 0, 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP2] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP3] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP1] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP0] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP7] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP4] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP6] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP5] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP0] Prefill batch. #new-seq: 71, #new-token: 71, #cached-token: 51957, token usage: 0.03, #running-req: 382, #queue-req: 0, 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP2] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP0] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP1] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP3] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP7] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP6] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP4] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP5] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP0] Prefill batch. #new-seq: 67, #new-token: 67, #cached-token: 48558, token usage: 0.03, #running-req: 453, #queue-req: 0, 
[2025-10-23 11:14:15 TP0] Prefill batch. #new-seq: 84, #new-token: 84, #cached-token: 61011, token usage: 0.04, #running-req: 520, #queue-req: 0, 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP2] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP0] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP3] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP7] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP1] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP4] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP6] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:15 TP5] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP0] Prefill batch. #new-seq: 67, #new-token: 67, #cached-token: 48649, token usage: 0.04, #running-req: 604, #queue-req: 0, 
[2025-10-23 11:14:16 TP0] Prefill batch. #new-seq: 90, #new-token: 90, #cached-token: 65573, token usage: 0.05, #running-req: 671, #queue-req: 0, 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP7] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP6] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP4] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP5] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP0] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP2] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP1] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP3] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP0] Prefill batch. #new-seq: 72, #new-token: 72, #cached-token: 52698, token usage: 0.05, #running-req: 761, #queue-req: 0, 
[2025-10-23 11:14:16 TP0] Prefill batch. #new-seq: 96, #new-token: 96, #cached-token: 69668, token usage: 0.06, #running-req: 833, #queue-req: 0, 
[2025-10-23 11:14:16 TP0] Prefill batch. #new-seq: 76, #new-token: 76, #cached-token: 55866, token usage: 0.06, #running-req: 929, #queue-req: 0, 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP2] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP1] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP3] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP7] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP4] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP6] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP5] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP0] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP0] Prefill batch. #new-seq: 19, #new-token: 19, #cached-token: 13991, token usage: 0.06, #running-req: 1005, #queue-req: 17, 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP0] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP2] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP3] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP1] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP4] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP7] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP6] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:16 TP5] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:20 TP0] Decode batch. #running-req: 1024, #token: 91556, token usage: 0.09, cuda graph: False, gen throughput (token/s): 388.70, #queue-req: 295, 
[2025-10-23 11:14:20] INFO:     127.0.0.1:41330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:20 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 730, token usage: 0.10, #running-req: 1023, #queue-req: 294, 
[2025-10-23 11:14:21] INFO:     127.0.0.1:39038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:21 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 729, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-10-23 11:14:21] INFO:     127.0.0.1:41378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:21 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 701, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-10-23 11:14:22] INFO:     127.0.0.1:39086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:22] INFO:     127.0.0.1:42040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:22] INFO:     127.0.0.1:38564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:22] INFO:     127.0.0.1:40080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:22] INFO:     127.0.0.1:42782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:22] INFO:     127.0.0.1:43160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:22] INFO:     127.0.0.1:46954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:22 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 1421, token usage: 0.11, #running-req: 1022, #queue-req: 290, 
[2025-10-23 11:14:22] INFO:     127.0.0.1:39508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:22] INFO:     127.0.0.1:40754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:22] INFO:     127.0.0.1:41196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:22] INFO:     127.0.0.1:44110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:22] INFO:     127.0.0.1:47094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:22 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7398, token usage: 0.11, #running-req: 1014, #queue-req: 280, 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP7] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP3] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP2] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP6] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP5] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP1] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP4] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP0] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22] INFO:     127.0.0.1:39886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:22] INFO:     127.0.0.1:41360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:22] INFO:     127.0.0.1:44134 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP6] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP2] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP3] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP7] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP1] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP5] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP0] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP4] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 2178, token usage: 0.11, #running-req: 1021, #queue-req: 277, 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP6] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP2] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP7] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP3] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP5] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP1] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP4] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22 TP0] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:22] INFO:     127.0.0.1:39102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:22] INFO:     127.0.0.1:39150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:22] INFO:     127.0.0.1:39192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:22] INFO:     127.0.0.1:40644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:22] INFO:     127.0.0.1:42960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:22] INFO:     127.0.0.1:42980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:22] INFO:     127.0.0.1:43138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:22] INFO:     127.0.0.1:43298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:22 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5778, token usage: 0.11, #running-req: 1016, #queue-req: 269, 
[2025-10-23 11:14:23] INFO:     127.0.0.1:38922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:39978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:40176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:41306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:43426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:47082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:47222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5076, token usage: 0.11, #running-req: 1017, #queue-req: 262, 
[2025-10-23 11:14:23] INFO:     127.0.0.1:41660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:42210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:43618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:45240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:46456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3573, token usage: 0.11, #running-req: 1019, #queue-req: 257, 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:23 TP2] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:23 TP7] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:23 TP6] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:23 TP3] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:23 TP5] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:23 TP1] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:23 TP4] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:23 TP0] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:23] INFO:     127.0.0.1:41754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:42748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:44376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:46662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2887, token usage: 0.11, #running-req: 1020, #queue-req: 253, 
[2025-10-23 11:14:23] INFO:     127.0.0.1:39358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:39912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:42018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:42916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:43150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:45980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:46248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:46588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:47136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6583, token usage: 0.11, #running-req: 1015, #queue-req: 244, 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:23 TP6] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:23 TP7] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:23 TP2] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:23 TP5] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:23 TP3] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:23 TP1] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:23 TP4] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:23 TP0] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:23] INFO:     127.0.0.1:38544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:39594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:40610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:40936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:44254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:44340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:45740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:46106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:46378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23] INFO:     127.0.0.1:47618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:23 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7367, token usage: 0.11, #running-req: 1014, #queue-req: 234, 
[2025-10-23 11:14:24] INFO:     127.0.0.1:39864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:41862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:45538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:46078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:46928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3709, token usage: 0.11, #running-req: 1019, #queue-req: 229, 
[2025-10-23 11:14:24] INFO:     127.0.0.1:39158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:39626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:42858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:43066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:43326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:44036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4394, token usage: 0.12, #running-req: 1018, #queue-req: 223, 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:24 TP0] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:24 TP2] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:24 TP6] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:24 TP7] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:24 TP3] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:24 TP4] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:24 TP1] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:24 TP5] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:24] INFO:     127.0.0.1:41038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:41748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:42596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:43548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:43924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:44660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:46310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:47344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:47606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6535, token usage: 0.12, #running-req: 1015, #queue-req: 214, 
[2025-10-23 11:14:24] INFO:     127.0.0.1:39540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:39846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:39928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:40278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:40854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:42358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:44686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:46390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5725, token usage: 0.12, #running-req: 1016, #queue-req: 206, 
[2025-10-23 11:14:24] INFO:     127.0.0.1:39016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:39172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:39524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:42558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:42988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:43638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:44284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:44910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:46466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:24] INFO:     127.0.0.1:47276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7238, token usage: 0.12, #running-req: 1014, #queue-req: 196, 
[2025-10-23 11:14:25] INFO:     127.0.0.1:38744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:39738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:40232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:40944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:41936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:41964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:43194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:44202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:44766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:45216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7344, token usage: 0.12, #running-req: 1014, #queue-req: 186, 
[2025-10-23 11:14:25] INFO:     127.0.0.1:39602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:40316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:40392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:40982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:41576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:44392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:44546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:44918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:45412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:47272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7233, token usage: 0.12, #running-req: 1014, #queue-req: 176, 
[2025-10-23 11:14:25] INFO:     127.0.0.1:38534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:40134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:40472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:40514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:40770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:40772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:41166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:41826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:42870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:44744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7171, token usage: 0.12, #running-req: 1014, #queue-req: 166, 
[2025-10-23 11:14:25] INFO:     127.0.0.1:39782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:40806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:41426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:41768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:41966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:43884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:44826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:45346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:45666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:46272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7235, token usage: 0.12, #running-req: 1014, #queue-req: 156, 
[2025-10-23 11:14:25] INFO:     127.0.0.1:39044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:43014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:44480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:45172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:45688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:46208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:46896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:25] INFO:     127.0.0.1:47308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5832, token usage: 0.12, #running-req: 1016, #queue-req: 148, 
[2025-10-23 11:14:26] INFO:     127.0.0.1:40848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:43218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:43986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:44104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:45030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:45082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:46612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5213, token usage: 0.12, #running-req: 1017, #queue-req: 141, 
[2025-10-23 11:14:26] INFO:     127.0.0.1:39576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:40338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:41082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:41450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:42874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:43404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:45520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:46394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:46686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6501, token usage: 0.12, #running-req: 1015, #queue-req: 132, 
[2025-10-23 11:14:26] INFO:     127.0.0.1:38686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:39088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:39778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:40570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:43116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:43392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:43994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:44048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:44164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:45390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:45774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:47070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8802, token usage: 0.12, #running-req: 1012, #queue-req: 120, 
[2025-10-23 11:14:26] INFO:     127.0.0.1:39112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:40352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:40722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:41066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:42864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:43576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:46036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5173, token usage: 0.12, #running-req: 1017, #queue-req: 113, 
[2025-10-23 11:14:26] INFO:     127.0.0.1:41164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:43850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:44418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:45902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:46510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:26] INFO:     127.0.0.1:46790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:46966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5166, token usage: 0.12, #running-req: 1017, #queue-req: 106, 
[2025-10-23 11:14:27] INFO:     127.0.0.1:39346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:39478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:41172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:41484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:41500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:42006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:42494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:45486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:46346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:46526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7375, token usage: 0.12, #running-req: 1014, #queue-req: 96, 
[2025-10-23 11:14:27] INFO:     127.0.0.1:38882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:39410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:40058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:40766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:40842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:41318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:41882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:42176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:42814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:42880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:46784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:47630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8638, token usage: 0.12, #running-req: 1012, #queue-req: 84, 
[2025-10-23 11:14:27] INFO:     127.0.0.1:40026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:40152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:40534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:40556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:41114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:41148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:43826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:43956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:44676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:27] INFO:     127.0.0.1:46550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:28 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7282, token usage: 0.12, #running-req: 1014, #queue-req: 74, 
[2025-10-23 11:14:31] INFO:     127.0.0.1:38760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:31] INFO:     127.0.0.1:39206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:31] INFO:     127.0.0.1:40286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:31] INFO:     127.0.0.1:40666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:31] INFO:     127.0.0.1:41492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:31] INFO:     127.0.0.1:42928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:31] INFO:     127.0.0.1:43284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:31] INFO:     127.0.0.1:44194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:31] INFO:     127.0.0.1:45322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:31] INFO:     127.0.0.1:45478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:31] INFO:     127.0.0.1:46470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:31] INFO:     127.0.0.1:46508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:31] INFO:     127.0.0.1:46672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:31 TP0] Prefill batch. #new-seq: 13, #new-token: 13, #cached-token: 9629, token usage: 0.12, #running-req: 1011, #queue-req: 61, 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:32 TP2] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:32 TP3] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:32 TP0] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:32 TP1] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:32 TP7] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:32 TP4] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:32 TP6] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:32 TP5] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:32 TP0] Decode batch. #running-req: 1011, #token: 119580, token usage: 0.12, cuda graph: False, gen throughput (token/s): 3524.19, #queue-req: 61, 
[2025-10-23 11:14:32] INFO:     127.0.0.1:39042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:40252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:40464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:44026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:44094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:44120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:44342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:45252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:45282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:45440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:46262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:46298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:46316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:47296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:47658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32 TP0] Prefill batch. #new-seq: 15, #new-token: 15, #cached-token: 11043, token usage: 0.13, #running-req: 1009, #queue-req: 46, 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:32 TP2] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:32 TP6] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:32 TP7] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:32 TP3] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:32 TP1] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:32 TP5] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:32 TP4] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:32 TP0] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:32] INFO:     127.0.0.1:39742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:40054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:40516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:40540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:41814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:41942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:42348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:42602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:42682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:43126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:44030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:44234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:44488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:45808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:46910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32 TP0] Prefill batch. #new-seq: 15, #new-token: 15, #cached-token: 10892, token usage: 0.13, #running-req: 1009, #queue-req: 31, 
[2025-10-23 11:14:32] INFO:     127.0.0.1:39718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:40616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:41256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:41612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:41774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:41796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:42188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:43758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:44756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:45752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7315, token usage: 0.13, #running-req: 1014, #queue-req: 21, 
[2025-10-23 11:14:32] INFO:     127.0.0.1:39220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:39824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:40324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:41594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:42470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:42608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:43034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:44742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:44956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:45196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:45312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32] INFO:     127.0.0.1:45634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:32 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8751, token usage: 0.13, #running-req: 1012, #queue-req: 9, 
[2025-10-23 11:14:33] INFO:     127.0.0.1:38796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:39754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:39986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:40400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:40564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:41624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:42218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:42386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:43472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:44570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:45160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:46358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:46738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:47422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:47462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6507, token usage: 0.13, #running-req: 1009, #queue-req: 0, 
[2025-10-23 11:14:33] INFO:     127.0.0.1:39492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:41952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:47032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:39164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:39968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:40498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:41670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:42696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:44204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:44280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:44542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:45058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:45548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:47180 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP2] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP6] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP3] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP7] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP1] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP5] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP0] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP4] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33] INFO:     127.0.0.1:39590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:40178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:42314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:42660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:42972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:44326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:44554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:45004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:45102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:46720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:47246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:47686 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP7] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP6] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP4] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP5] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP0] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP3] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP1] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP2] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33] INFO:     127.0.0.1:39760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:40238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:40822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:41150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:41922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:43230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:43514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:43660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:44222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:44450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:45366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:45618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:45820 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP2] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP6] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP7] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP3] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP1] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP5] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP0] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP4] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33] INFO:     127.0.0.1:39244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:39488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:40532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:43182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:46180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:46936 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP2] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP6] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP3] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP7] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP1] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP5] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP0] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP4] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33] INFO:     127.0.0.1:38972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:39734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:43370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:43436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:43500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:46578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:33] INFO:     127.0.0.1:46638 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP0] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP2] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP1] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP6] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP7] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP3] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP4] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:33 TP5] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34] INFO:     127.0.0.1:39142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:39306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:40994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:42684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:43686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:45362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:46062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:46202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:46676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:47196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:47354 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP6] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP2] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP3] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP7] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP1] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP0] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP5] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP4] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34] INFO:     127.0.0.1:39278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:40068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:40600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:43000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:43246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:46596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:46622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:47670 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP6] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP2] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP7] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP3] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP1] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP5] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP0] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP4] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34] INFO:     127.0.0.1:39694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:43512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:44506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:44882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:45112 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP6] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP2] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP7] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP3] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP5] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP1] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP4] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP0] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34] INFO:     127.0.0.1:39222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:40500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:41840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:42352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:42504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:44012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:44614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:45564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:45600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:45830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:46006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:46690 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP6] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP2] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP7] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP3] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP5] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP1] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP0] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP4] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34] INFO:     127.0.0.1:39376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:39558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:40000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:40718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:41072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:43174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:45222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:45938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:46150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:47644 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP2] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP6] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP3] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP7] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP5] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP1] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP0] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP4] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34] INFO:     127.0.0.1:38888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:39332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:39952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:40422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:40892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:42048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:44724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:45700 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP6] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP7] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP3] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP2] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP1] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP5] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP0] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP4] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34] INFO:     127.0.0.1:38936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:38952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:39122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:39840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:40612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:41494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:42282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:42700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:43060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:44010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:45326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:45508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:45786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:45920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:46126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:46356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:40844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:41350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:41856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:41916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:42674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:42764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:43340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:43610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:43700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:43864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:45588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:34] INFO:     127.0.0.1:47238 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP2] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP6] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP3] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP7] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP1] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP5] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP0] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:34 TP4] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35] INFO:     127.0.0.1:38644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:39230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:39854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:40110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:40142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:40654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:41738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:42222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:44276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:44852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:46890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:47168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:39280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:39680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:41420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:41700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:41874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:42030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:43282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:43452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:43460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:43812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:44978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:45048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:45278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:45340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:45422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:46628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:46870 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP2] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP1] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP7] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP6] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP3] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP5] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP0] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP4] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35] INFO:     127.0.0.1:38600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:39910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:39938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:45794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:46538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:46722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:46744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:46888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:47268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:47390 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP6] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP7] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP2] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP3] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP1] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP5] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP0] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP4] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35] INFO:     127.0.0.1:39424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:40738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:41284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:41844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:44314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:45296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:46238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:46290 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP6] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP7] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP4] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP5] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP0] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP2] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP1] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP3] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35] INFO:     127.0.0.1:39334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:39548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:39898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:40014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:40028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:40586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:41718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:42406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:42540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:43938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:45098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:46408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:46438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:46498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:46880 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP6] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP4] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP2] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP0] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP5] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP7] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP3] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP1] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35] INFO:     127.0.0.1:39736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:41008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:42640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:42850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:43098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:44866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:44942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:46840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:48174 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP6] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP2] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP4] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP7] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP0] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP3] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP5] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP1] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35] INFO:     127.0.0.1:40162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:43532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:44992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:46234 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP2] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP6] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP4] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP0] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP5] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP7] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP1] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP3] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35] INFO:     127.0.0.1:42570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:43420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:43646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:44496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:45388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:35] INFO:     127.0.0.1:48402 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP6] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP2] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP0] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP4] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP5] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP3] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP7] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:35 TP1] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36] INFO:     127.0.0.1:39250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:41658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:42246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:42396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:42432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:42996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:43742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:45850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:48876 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP2] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP6] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP3] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP7] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP1] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP0] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP5] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP4] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36] INFO:     127.0.0.1:39692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:40458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:41516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:41894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:42666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:43048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:43076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:44076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:44192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:44366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:44888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:46636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:47028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:47874 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP2] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP6] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP3] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP7] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP1] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP0] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP5] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP4] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36] INFO:     127.0.0.1:40040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:41992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:42808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:43050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:43178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:43582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:44148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:47254 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP2] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP6] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP3] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP7] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP0] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP4] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP1] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP5] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36] INFO:     127.0.0.1:38732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:38824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:39180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:39260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:40044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:41474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:42324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:44080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:44898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:45484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:47130 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP6] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP2] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP7] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP3] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP0] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP4] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP5] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP1] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP6] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP2] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP7] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP0] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP4] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP3] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP5] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP1] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36] INFO:     127.0.0.1:45956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:47144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:47290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:47412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:48346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:39494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:39876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:40760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:41634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:41838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:42760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:43688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:45448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:47956 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP2] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP6] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP7] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP3] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP0] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP1] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP4] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP5] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36] INFO:     127.0.0.1:39348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:39638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:42958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:45032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:46424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:46906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:46970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:47446 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP2] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP6] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP3] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP7] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP0] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP4] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP1] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP5] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36] INFO:     127.0.0.1:38578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:38638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:38878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:40052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:41050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:41054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:41240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:41438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:41666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:42262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:42580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:42772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:43114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:43716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:45084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:45202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:45952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:46120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:46222 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP2] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP6] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP7] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP3] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP0] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP4] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP1] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP5] [fused_moe] using default for (719, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36] INFO:     127.0.0.1:38914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:38954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:39566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:39766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:41372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:42826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:42888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:42902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:44648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:45922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:47012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:47328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:48440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:48466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:36] INFO:     127.0.0.1:48556 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP2] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP6] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP3] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP7] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP0] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP1] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP5] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:36 TP4] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37] INFO:     127.0.0.1:40642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:41976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:42120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:42232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:42274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:43080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:43086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:43222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:43378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:45552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:45648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:48042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:48810 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP2] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP6] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP3] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP7] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP1] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP5] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP0] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP4] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37] INFO:     127.0.0.1:39704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:42168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:43030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:43488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:44728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:44748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:45134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:45992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:47536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:47660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:48472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:49020 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (679, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP2] [fused_moe] using default for (679, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (679, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (679, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP3] [fused_moe] using default for (679, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP6] [fused_moe] using default for (679, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (679, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP7] [fused_moe] using default for (679, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (679, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (679, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP5] [fused_moe] using default for (679, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP1] [fused_moe] using default for (679, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (679, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP0] [fused_moe] using default for (679, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (679, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP4] [fused_moe] using default for (679, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37] INFO:     127.0.0.1:40186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:41596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:42446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:43316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:43624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:43794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:43908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:44078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:48850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:49166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:49540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:49990 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP2] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP6] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP7] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP3] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP1] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP5] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP0] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP4] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37] INFO:     127.0.0.1:38720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:41096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:41216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:42138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:42720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:46122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:48530 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP2] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP0] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP3] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP6] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP1] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP7] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP4] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP5] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP0] Decode batch. #running-req: 667, #token: 99354, token usage: 0.10, cuda graph: False, gen throughput (token/s): 6422.59, #queue-req: 0, 
[2025-10-23 11:14:37] INFO:     127.0.0.1:39390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:40686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:41266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:45720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:47550 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP6] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP2] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP3] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP7] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP1] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP5] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP0] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP4] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37] INFO:     127.0.0.1:39708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:39716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:40702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:40712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:40820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:41184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:43082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:45124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:45960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:46052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:46986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:47530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:47704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:48550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:48858 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP2] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP6] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP3] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP7] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP0] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP1] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP5] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP4] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37] INFO:     127.0.0.1:38688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:39318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:39538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:41106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:41224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:41616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:42334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:43730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:43954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:44354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:46820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:47764 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP2] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP6] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP3] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP7] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP0] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP1] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP5] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP4] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37] INFO:     127.0.0.1:38712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:39808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:39838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:40968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:44286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:46334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:47116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:47494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:48260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:48296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:37] INFO:     127.0.0.1:50110 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP2] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP6] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP4] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP0] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP3] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP1] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP7] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:37 TP5] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38] INFO:     127.0.0.1:39050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:42014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:42592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:44294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:46556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:46778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:47042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:47092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:48404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:48982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:49360 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP2] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP6] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP1] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP5] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP3] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP7] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP0] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP4] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38] INFO:     127.0.0.1:41858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:41996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:42074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:42546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:42840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:45442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:45468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:46694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:47518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:48454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:38838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:40622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:40868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:41690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:43692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:44408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:45288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:45402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:46854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:47508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:47592 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP2] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP6] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP3] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP7] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP0] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP1] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP5] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP4] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38] INFO:     127.0.0.1:38676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:40568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:40682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:41582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:41810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:43784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:43898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:44266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:45966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:47058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:49090 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP6] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP2] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP3] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP7] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP0] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP4] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP1] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP5] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38] INFO:     127.0.0.1:38590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:38654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:39822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:40696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:41004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:41546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:43768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:45018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:45912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:46164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:47096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:48018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:48088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:48234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:50010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:38808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:39002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:40262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:40292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:41532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:42622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:42794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:43966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:45068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:45530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:46094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:46682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:47112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:47854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:47940 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP6] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP2] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP7] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP3] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP4] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP0] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP5] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38 TP1] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:38] INFO:     127.0.0.1:42106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:44710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:47004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:48586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:48980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:49378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:40782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:40998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:41402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:44252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:45044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:45502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:46762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:48660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:48804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:49272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:49494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:39032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:42294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:42566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:44476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:44632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:45374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:46646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:48082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:48602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:40798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:41298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:42564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:42668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:43380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:43588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:44814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:48138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:48484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:48920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:49234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:40300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:40380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:40544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:42204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:42302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:42528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:42612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:43304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:44000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:44434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:45826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:46912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:38] INFO:     127.0.0.1:48856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:41108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:41208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:41724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:44068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:49958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:38774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:39070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:40450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:40950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:41728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:42536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:45116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:45178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:46196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:47982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:48948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:38628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:42058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:44522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:45766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:45878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:48444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:48644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:48758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:49116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:49626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:40222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:41346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:43202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:44678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:45434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:49210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:40908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:46020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:46134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:46484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:46608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:47718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:48674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:48898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:41174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:44698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:46148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:47800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:48614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:38548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:40410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:40694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:41676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:42486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:43356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:44216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:44840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:47068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:47342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:47842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:48008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:49974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:38612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:44460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:44586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:47154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:47430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:48318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:48330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:49474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:49576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:49746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:42704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:43878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:49392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:49868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:50046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:50258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:41644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:43026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:44186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:46136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:46286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:50242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:39396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:41772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:48648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:49194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:49486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:42022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:42372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:43260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:45824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:46988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:48050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:48886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:49050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:49164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:49356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:49438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:49710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:49756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:40092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:41138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:41296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:42728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:43658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:44060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:48420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:49470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:49630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:50230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:44608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:45652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:49228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:50094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:39654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:40368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:42154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:45424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:46278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:48748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:39] INFO:     127.0.0.1:49600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:38902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:40366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:41278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:43518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:43852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:45012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:45194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:45236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:49608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:49904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:45598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:47826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:48590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:49528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:49612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:49810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:39062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:40496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:43672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:44106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:45862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:46752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:48538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:48734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:49280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:49312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:50150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:46448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:47978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:48470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:48836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:38704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:48436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:48514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:48710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:42208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:42416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:42624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:47756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:49344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:49720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:50334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:40876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:41022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:48382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:48568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:49028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:49458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:49954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:50216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:38920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:47576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:47836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:49330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:49488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:38780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:39686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:40628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:46352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:46834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:47086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:47994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:48208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:49682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:40484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:41902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:42736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:44626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:47920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:48160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:48632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:48868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:50264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:50316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40 TP0] Decode batch. #running-req: 311, #token: 56926, token usage: 0.06, cuda graph: True, gen throughput (token/s): 6023.28, #queue-req: 0, 
[2025-10-23 11:14:40] INFO:     127.0.0.1:40436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:40846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:43264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:47234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:47876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:49132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:40108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:40206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:41562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:46072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:47728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:47816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:49794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:39440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:40708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:44530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:47690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:48348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:40120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:42200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:50056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:50332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:38862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:44782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:48124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:48900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:49586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:50026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:40022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:41780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:44582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:44662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:46950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:47316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:48074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:48192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:40] INFO:     127.0.0.1:50120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:41714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:42762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:48034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:50322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:39462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:40800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:48498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:50292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:45676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:47496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:47796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:50188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:40166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:42016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:42090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:43602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:44246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:48906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:40632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:45640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:47780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:50134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:50204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:50308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:42590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:44138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:43614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:46850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:47478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:42086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:48066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:40190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:38988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:42356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:44058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:39470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:48080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:48716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:50016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:42054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:42520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:42646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:44970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:50080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:40576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:44302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:48378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:43974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:45612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:47752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:48128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:48246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:38618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:38964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:45264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:47694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:48808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:44988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:45176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:50066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:39670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:40756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:40912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:43842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:43950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:45888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:47864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:50142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:47886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:48788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:48838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:41] INFO:     127.0.0.1:49180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:43268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:48216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:48722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:49580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:47212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:39218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:49274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:49410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:49282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:39364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:40988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:46704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:45604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:46324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:48218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:48824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:49644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:50044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:50214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:40830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:44964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:48164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:49070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:49426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:49842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:40922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:41390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:42910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:40510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:43564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:48324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:39290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:40930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:43668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:48776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:48934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:48112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:48778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:48994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:50124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:41252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:41416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:50038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:44596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42 TP0] Decode batch. #running-req: 126, #token: 29974, token usage: 0.03, cuda graph: True, gen throughput (token/s): 4151.02, #queue-req: 0, 
[2025-10-23 11:14:42] INFO:     127.0.0.1:45144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:48276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:47908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:48096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:48964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:49664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:49830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:39270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:43682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:44024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:46370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:47366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:48150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:48302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:48488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:40964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:41122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:43600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:48792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:49048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:49250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:39132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:47744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:49804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:50276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:42] INFO:     127.0.0.1:40212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:38852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:41662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:44300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:49292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:42460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:47562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:47828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:49514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:38660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:43348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:49266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:38668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:50004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:45204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:45504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:46568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:42476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:44178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:48694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:49764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:50314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:39610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:42774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:50160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:42268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:39792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:46920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:49450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:41460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:48988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:49054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:49768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:39498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:43808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:48462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:49676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:45704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:48760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:49314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:45574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:49638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:50220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:45840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:50328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:40116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:44798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:47970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:45454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:46772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:47590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:49220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:49818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:40004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:48584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:43] INFO:     127.0.0.1:39774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:50174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:49726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:39454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:49560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44 TP0] Decode batch. #running-req: 40, #token: 11684, token usage: 0.01, cuda graph: True, gen throughput (token/s): 2039.53, #queue-req: 0, 
[2025-10-23 11:14:44] INFO:     127.0.0.1:48362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:46806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:49500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:49800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:48186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:44930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:50212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:49668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:41158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:42128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:45724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:44468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:47680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:49086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:48390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:39858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:50218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:42942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:50070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:49024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:47894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:48946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:47622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:44] INFO:     127.0.0.1:48626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:45] INFO:     127.0.0.1:49824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:45] INFO:     127.0.0.1:48286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:45] INFO:     127.0.0.1:48442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:45 TP0] Decode batch. #running-req: 13, #token: 4889, token usage: 0.01, cuda graph: True, gen throughput (token/s): 956.88, #queue-req: 0, 
[2025-10-23 11:14:45] INFO:     127.0.0.1:49366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:45] INFO:     127.0.0.1:49888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:45] INFO:     127.0.0.1:49422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:45] INFO:     127.0.0.1:48194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:45] INFO:     127.0.0.1:49286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:45] INFO:     127.0.0.1:49006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:45] INFO:     127.0.0.1:45418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:45] INFO:     127.0.0.1:47402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:45] INFO:     127.0.0.1:47380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:45] INFO:     127.0.0.1:49142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:46 TP0] Decode batch. #running-req: 3, #token: 1682, token usage: 0.00, cuda graph: True, gen throughput (token/s): 325.91, #queue-req: 0, 
[2025-10-23 11:14:46] INFO:     127.0.0.1:48690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:46] INFO:     127.0.0.1:44446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:46] INFO:     127.0.0.1:47926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:58] INFO:     127.0.0.1:51920 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-23 11:14:58 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 11:14:59] INFO:     127.0.0.1:51926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:14:59 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 11:14:59 TP0] Prefill batch. #new-seq: 34, #new-token: 34, #cached-token: 24643, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP2] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP6] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP4] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP0] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP1] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP3] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP7] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP5] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP0] Prefill batch. #new-seq: 46, #new-token: 46, #cached-token: 33551, token usage: 0.01, #running-req: 35, #queue-req: 0, 
[2025-10-23 11:14:59 TP0] Prefill batch. #new-seq: 50, #new-token: 50, #cached-token: 36475, token usage: 0.01, #running-req: 81, #queue-req: 0, 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP2] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP0] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP1] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP3] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP7] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP6] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP4] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP5] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP0] Prefill batch. #new-seq: 58, #new-token: 58, #cached-token: 42369, token usage: 0.01, #running-req: 131, #queue-req: 0, 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP2] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP3] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP0] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP1] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP6] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP7] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP4] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP5] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP0] Prefill batch. #new-seq: 56, #new-token: 56, #cached-token: 40626, token usage: 0.02, #running-req: 189, #queue-req: 0, 
[2025-10-23 11:14:59 TP0] Prefill batch. #new-seq: 65, #new-token: 65, #cached-token: 47269, token usage: 0.02, #running-req: 245, #queue-req: 0, 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP2] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP0] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP3] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP1] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP4] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP6] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP7] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP5] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:14:59 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 44470, token usage: 0.02, #running-req: 310, #queue-req: 0, 
[2025-10-23 11:14:59 TP0] Prefill batch. #new-seq: 72, #new-token: 72, #cached-token: 52591, token usage: 0.03, #running-req: 371, #queue-req: 0, 
[2025-10-23 11:15:00 TP0] Prefill batch. #new-seq: 99, #new-token: 99, #cached-token: 71781, token usage: 0.03, #running-req: 443, #queue-req: 0, 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP3] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP2] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP1] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP0] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP4] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP7] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP6] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP5] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP0] Prefill batch. #new-seq: 78, #new-token: 78, #cached-token: 56723, token usage: 0.04, #running-req: 542, #queue-req: 0, 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP2] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP3] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP4] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP7] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP1] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP6] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP5] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP0] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP0] Prefill batch. #new-seq: 93, #new-token: 93, #cached-token: 67610, token usage: 0.04, #running-req: 620, #queue-req: 0, 
[aiter] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP2] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP3] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP0] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP1] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP4] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP6] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP7] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP5] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP2] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP4] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP6] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP0] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP3] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP1] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP7] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP5] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP0] Prefill batch. #new-seq: 38, #new-token: 38, #cached-token: 27651, token usage: 0.05, #running-req: 713, #queue-req: 0, 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP2] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP0] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP3] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP1] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP6] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP7] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP4] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP5] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP0] Prefill batch. #new-seq: 68, #new-token: 68, #cached-token: 49720, token usage: 0.05, #running-req: 751, #queue-req: 0, 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP0] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP2] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP4] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP6] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP1] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP3] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP7] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP5] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:00 TP0] Prefill batch. #new-seq: 52, #new-token: 52, #cached-token: 37809, token usage: 0.05, #running-req: 819, #queue-req: 0, 
[2025-10-23 11:15:01 TP0] Prefill batch. #new-seq: 79, #new-token: 79, #cached-token: 57539, token usage: 0.06, #running-req: 871, #queue-req: 0, 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP2] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP3] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP1] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP0] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP4] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP6] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP7] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP5] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP0] Prefill batch. #new-seq: 57, #new-token: 57, #cached-token: 41842, token usage: 0.06, #running-req: 950, #queue-req: 0, 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP2] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP3] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP1] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP6] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP4] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP0] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP7] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP5] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP0] Prefill batch. #new-seq: 17, #new-token: 17, #cached-token: 12571, token usage: 0.06, #running-req: 1007, #queue-req: 70, 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP7] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP0] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP5] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP3] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP1] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP6] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP4] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:01 TP2] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:04] INFO:     127.0.0.1:54824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:04 TP0] Decode batch. #running-req: 1024, #token: 90084, token usage: 0.09, cuda graph: False, gen throughput (token/s): 1456.50, #queue-req: 295, 
[2025-10-23 11:15:04 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 718, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-10-23 11:15:05] INFO:     127.0.0.1:53768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:05 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 723, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-10-23 11:15:05] INFO:     127.0.0.1:54884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:06 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-10-23 11:15:06] INFO:     127.0.0.1:51968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:06] INFO:     127.0.0.1:52578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:06] INFO:     127.0.0.1:55592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:06] INFO:     127.0.0.1:56446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:06] INFO:     127.0.0.1:52398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:06] INFO:     127.0.0.1:56198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:06 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2977, token usage: 0.11, #running-req: 1020, #queue-req: 288, 
[2025-10-23 11:15:06] INFO:     127.0.0.1:52956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:06] INFO:     127.0.0.1:53996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:06] INFO:     127.0.0.1:54704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:06] INFO:     127.0.0.1:57572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:06 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4333, token usage: 0.11, #running-req: 1018, #queue-req: 282, 
[2025-10-23 11:15:06] INFO:     127.0.0.1:52196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:06] INFO:     127.0.0.1:54848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:06] INFO:     127.0.0.1:57608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:06] INFO:     127.0.0.1:60462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:07 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2886, token usage: 0.11, #running-req: 1020, #queue-req: 278, 
[2025-10-23 11:15:07] INFO:     127.0.0.1:52264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:07] INFO:     127.0.0.1:52604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:07] INFO:     127.0.0.1:53468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:07] INFO:     127.0.0.1:54144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:07] INFO:     127.0.0.1:56368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:07] INFO:     127.0.0.1:56682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:07] INFO:     127.0.0.1:56786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:07] INFO:     127.0.0.1:57070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:07] INFO:     127.0.0.1:60306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:07 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6512, token usage: 0.11, #running-req: 1015, #queue-req: 269, 
[2025-10-23 11:15:07] INFO:     127.0.0.1:53064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:07] INFO:     127.0.0.1:53848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:07] INFO:     127.0.0.1:56896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:07 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 2217, token usage: 0.11, #running-req: 1021, #queue-req: 266, 
[2025-10-23 11:15:07] INFO:     127.0.0.1:55146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:07] INFO:     127.0.0.1:55720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:07] INFO:     127.0.0.1:60436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:07] INFO:     127.0.0.1:60582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:07 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2931, token usage: 0.11, #running-req: 1020, #queue-req: 262, 
[2025-10-23 11:15:07] INFO:     127.0.0.1:56520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:07] INFO:     127.0.0.1:57866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:07] INFO:     127.0.0.1:59738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:07 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 2146, token usage: 0.11, #running-req: 1021, #queue-req: 259, 
[2025-10-23 11:15:08] INFO:     127.0.0.1:52488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:53042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:55398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:56344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:56670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:59984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4339, token usage: 0.11, #running-req: 1018, #queue-req: 253, 
[2025-10-23 11:15:08] INFO:     127.0.0.1:51950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:52940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:53972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:54360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:54808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:59324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:59556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:59692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:59884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:60530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7306, token usage: 0.11, #running-req: 1014, #queue-req: 243, 
[2025-10-23 11:15:08] INFO:     127.0.0.1:52754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:53032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:58644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:59124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:59404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:60696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4483, token usage: 0.11, #running-req: 1018, #queue-req: 237, 
[2025-10-23 11:15:08] INFO:     127.0.0.1:52302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:53502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:55872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:56258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:56296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:56806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:57486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:58948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:59374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6598, token usage: 0.12, #running-req: 1015, #queue-req: 228, 
[2025-10-23 11:15:08] INFO:     127.0.0.1:54528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:55278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:56084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:56986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:57366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:57690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:58098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:59620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:60290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08] INFO:     127.0.0.1:60962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:08 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7289, token usage: 0.12, #running-req: 1014, #queue-req: 218, 
[2025-10-23 11:15:09] INFO:     127.0.0.1:52174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:53006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:53062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:53218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:54414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:56488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:58126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5038, token usage: 0.12, #running-req: 1017, #queue-req: 211, 
[2025-10-23 11:15:09] INFO:     127.0.0.1:52964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:54662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:55262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:55552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:56026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:56678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:57084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:58252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:59696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7157, token usage: 0.12, #running-req: 1014, #queue-req: 201, 
[2025-10-23 11:15:09] INFO:     127.0.0.1:53506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:56396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:58192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:58314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:59770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3715, token usage: 0.12, #running-req: 1019, #queue-req: 196, 
[2025-10-23 11:15:09] INFO:     127.0.0.1:52402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:52790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:53020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:54458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:55030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:55292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:56224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:57864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:57998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:58178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:58612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 7978, token usage: 0.12, #running-req: 1013, #queue-req: 185, 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:09 TP2] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:09 TP3] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:09 TP0] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:09 TP7] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:09 TP4] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:09 TP6] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:09 TP5] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:09 TP1] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:09] INFO:     127.0.0.1:51938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:52414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:53816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:53934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:54134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:54204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:55348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:57414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:58322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:58726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:58786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:09] INFO:     127.0.0.1:60608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8722, token usage: 0.12, #running-req: 1012, #queue-req: 173, 
[2025-10-23 11:15:10] INFO:     127.0.0.1:52834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:54080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:54434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:54916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:57770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:59580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5013, token usage: 0.12, #running-req: 1017, #queue-req: 166, 
[2025-10-23 11:15:10] INFO:     127.0.0.1:53572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:56416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:56814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:57750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:59066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3611, token usage: 0.12, #running-req: 1019, #queue-req: 161, 
[2025-10-23 11:15:10] INFO:     127.0.0.1:53734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:53876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:55444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:56634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:56886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:59086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:60420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5061, token usage: 0.12, #running-req: 1017, #queue-req: 154, 
[2025-10-23 11:15:10] INFO:     127.0.0.1:52630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:53330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:54310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:54922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:56312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:56352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:56504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:57940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:58420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:58470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:60048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 8094, token usage: 0.12, #running-req: 1013, #queue-req: 143, 
[2025-10-23 11:15:10] INFO:     127.0.0.1:52090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:52842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:53250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:53460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:54108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:56870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:57426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:57504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:57636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:58112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:58836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:58934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:59174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:59510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:59708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:10] INFO:     127.0.0.1:60002 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:10 TP6] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:10 TP2] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:10 TP4] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:10 TP0] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:10 TP3] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:10 TP7] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:10 TP1] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:10 TP5] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:11 TP0] Prefill batch. #new-seq: 16, #new-token: 16, #cached-token: 11638, token usage: 0.12, #running-req: 1008, #queue-req: 127, 
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP2] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP4] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP6] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP3] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP7] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP0] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP5] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP1] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 11:15:11] INFO:     127.0.0.1:52444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:53196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:53956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:54224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:58900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:59138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:60698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5179, token usage: 0.12, #running-req: 1017, #queue-req: 120, 
[2025-10-23 11:15:11] INFO:     127.0.0.1:54124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:54946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:56502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:57328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:57870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:59350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:59902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5167, token usage: 0.12, #running-req: 1017, #queue-req: 113, 
[2025-10-23 11:15:11] INFO:     127.0.0.1:54974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:55964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:56382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:57004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:59258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:59832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:59960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:60120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:60328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6661, token usage: 0.12, #running-req: 1015, #queue-req: 104, 
[2025-10-23 11:15:11] INFO:     127.0.0.1:52292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:52602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:52888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:53130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:53522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:54340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:54816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:55406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:55692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:56430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:59640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:59804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11] INFO:     127.0.0.1:59818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:11 TP0] Prefill batch. #new-seq: 13, #new-token: 13, #cached-token: 9470, token usage: 0.12, #running-req: 1011, #queue-req: 91, 
[2025-10-23 11:15:12] INFO:     127.0.0.1:52760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:52768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:54154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:54602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:54628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:54652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:54752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:57298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:57408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:60106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7231, token usage: 0.12, #running-req: 1014, #queue-req: 81, 
[2025-10-23 11:15:12 TP0] Decode batch. #running-req: 1014, #token: 119798, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5481.35, #queue-req: 81, 
[2025-10-23 11:15:12] INFO:     127.0.0.1:52138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:53238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:53312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:53838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:54018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:54548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:54956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:56774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:57566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:57702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7304, token usage: 0.12, #running-req: 1014, #queue-req: 71, 
[2025-10-23 11:15:12] INFO:     127.0.0.1:52358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:52428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:53664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:53728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:53874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:55356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:55476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:57558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:57594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:57842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:57938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:58712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:59802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:60232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12 TP0] Prefill batch. #new-seq: 15, #new-token: 15, #cached-token: 11077, token usage: 0.13, #running-req: 1009, #queue-req: 56, 
[2025-10-23 11:15:12] INFO:     127.0.0.1:52752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:52978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:53912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:55842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:56336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:56710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:57028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:57488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:57726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:58664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:58684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:59564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:59610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:59622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:60886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12 TP0] Prefill batch. #new-seq: 15, #new-token: 15, #cached-token: 11011, token usage: 0.13, #running-req: 1009, #queue-req: 41, 
[2025-10-23 11:15:12] INFO:     127.0.0.1:54392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:55136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:55316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:55688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:56918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:57242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:58176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:58202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:59160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12] INFO:     127.0.0.1:60248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:12 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7275, token usage: 0.13, #running-req: 1014, #queue-req: 31, 
[2025-10-23 11:15:13] INFO:     127.0.0.1:52278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:53404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:53438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:55046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:55328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:55466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:55884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:56192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:58512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:58592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:58774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:58880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:58958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:59118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:60882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13 TP0] Prefill batch. #new-seq: 15, #new-token: 15, #cached-token: 10949, token usage: 0.13, #running-req: 1009, #queue-req: 16, 
[2025-10-23 11:15:13] INFO:     127.0.0.1:52234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:53198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:53606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:54054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:55072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:55428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:56592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:56596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:58022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:58328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:58710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:59032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:60802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13 TP0] Prefill batch. #new-seq: 14, #new-token: 14, #cached-token: 10204, token usage: 0.13, #running-req: 1010, #queue-req: 2, 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:13 TP2] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:13 TP6] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:13 TP4] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:13 TP3] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:13 TP7] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:13 TP0] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:13 TP1] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:13 TP5] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:13] INFO:     127.0.0.1:53030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:53750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:55458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:56090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:58488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:58556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:59680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:60866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:60918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 1422, token usage: 0.13, #running-req: 1015, #queue-req: 0, 
[2025-10-23 11:15:13] INFO:     127.0.0.1:52154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:52668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:53352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:55186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:57708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:57974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:59794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:60372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:60972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:53012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:53650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:54190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:56400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:56464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:58010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:58426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:58960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:13] INFO:     127.0.0.1:60564 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:13 TP4] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:13 TP2] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:13 TP6] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:13 TP0] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:13 TP3] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:13 TP7] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:13 TP1] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:13 TP5] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14] INFO:     127.0.0.1:53532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:54260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:54384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:54630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:54674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:54920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:56538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:57912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:58388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:58434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:59922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:60000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:60032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:60598 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP6] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP4] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP2] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP0] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP3] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP7] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP5] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP1] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14] INFO:     127.0.0.1:52180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:52380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:53410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:53702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:54398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:55808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:56156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:56260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:57714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:58200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:58766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:59170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:59726 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP2] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP4] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP6] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP0] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP1] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP5] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP3] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP7] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14] INFO:     127.0.0.1:52872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:55074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:56844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:56948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:59018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:60276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:52342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:52548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:54478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:56236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:56972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:57166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:59948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:52508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:52690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:53108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:54316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:56694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:56916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:57050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:58738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:59366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:59494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:60570 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP2] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP6] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP4] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP0] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP3] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP1] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP7] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP5] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14] INFO:     127.0.0.1:52294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:52312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:54168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:56964 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP2] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP4] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP6] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP0] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP1] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP5] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP3] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP7] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14] INFO:     127.0.0.1:53922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:55982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:57464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:58078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:58278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:59274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:59478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:60552 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP4] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP6] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP2] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP0] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP5] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP7] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP1] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP3] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14] INFO:     127.0.0.1:52274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:53046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:54166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:54564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:55858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:56248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:57360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:57964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:58560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:59314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:59428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:60014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:14] INFO:     127.0.0.1:60222 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP6] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP4] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP2] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP0] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP7] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP5] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP3] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:14 TP1] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15] INFO:     127.0.0.1:53060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:53092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:53486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:53822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:54066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:55474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:56908 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP6] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP2] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP4] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP0] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP7] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP3] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP5] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP1] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15] INFO:     127.0.0.1:52364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:53226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:53514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:53990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:54500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:54958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:56184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:58922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:59094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:59148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:59904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:60230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:60876 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP6] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP2] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP4] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP0] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP3] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP7] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP1] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP5] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15] INFO:     127.0.0.1:51984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:54860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:55380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:55514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:56494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:56584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:57196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:57342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:58734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:58974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:59648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:59752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:59936 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP2] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP6] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP4] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP0] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP3] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP7] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP1] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP5] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15] INFO:     127.0.0.1:52192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:52466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:52592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:53474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:54356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:56196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:57754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:58174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:58978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:59416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:60588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:60906 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP6] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP2] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP4] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP0] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP3] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP7] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP1] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP5] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15] INFO:     127.0.0.1:52500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:52904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:53422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:54248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:55218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:55400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:55556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:55774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:56434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:56760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:56932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:57282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:57448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:57744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:58448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:60250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:33000 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP2] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP6] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP4] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP0] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP3] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP7] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP1] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP5] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15] INFO:     127.0.0.1:52002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:55254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:55378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:57062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:58356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:58678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:58812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:60212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:60756 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP2] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP6] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP4] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP0] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP3] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP7] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP1] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP5] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15] INFO:     127.0.0.1:53384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:55376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:58496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:58648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:59574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:59810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:59882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:60072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:60604 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP2] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP4] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP6] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP0] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP3] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP7] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP1] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP5] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15] INFO:     127.0.0.1:52220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:52718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:53780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:54366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:55234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:55442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:55994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:57276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:57376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:57892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:58256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:58702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:15] INFO:     127.0.0.1:58752 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP2] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP6] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP4] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP0] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP3] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP7] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP1] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:15 TP5] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16] INFO:     127.0.0.1:51970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:53670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:56136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:59718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:52480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:55802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:55910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:56452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:57688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:57928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:58780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:59782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:60332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:32802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:33202 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP2] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP4] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP6] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP0] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP3] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP1] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP7] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP5] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16] INFO:     127.0.0.1:53464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:54758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:55728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:56890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:57122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:57954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:59528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:60798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:33396 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP4] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP6] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP2] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP0] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP1] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP3] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP5] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP7] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16] INFO:     127.0.0.1:52642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:53390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:53416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:53442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:55492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:55744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:55816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:55886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:55934 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP4] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP2] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP6] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP0] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP3] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP7] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP1] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP5] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16] INFO:     127.0.0.1:52532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:54352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:55408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:55900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:56052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:56164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:56280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:56460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:57086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:57214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:57524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:58262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:32874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:33884 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP2] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP6] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP4] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP0] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP3] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP7] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP1] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP5] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16] INFO:     127.0.0.1:52710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:53686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:56216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:56246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:56984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:57826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:59214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:59916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:60338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:60682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:60994 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP4] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP6] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP2] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP0] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP3] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP7] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP1] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP5] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16] INFO:     127.0.0.1:52166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:53862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:54908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:57546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:58344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:59874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:60380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:60602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:33864 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP2] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP4] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP6] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP3] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP0] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP7] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP1] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP5] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP4] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP6] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP2] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP7] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP0] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP3] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP5] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16 TP1] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:16] INFO:     127.0.0.1:52330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:52698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:56456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:56798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:57226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:57252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:58302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:59520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:59638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:60782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:16] INFO:     127.0.0.1:33330 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP6] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP4] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP2] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17] INFO:     127.0.0.1:53794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:55062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:55362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:56264 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP7] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP0] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17] INFO:     127.0.0.1:56458 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP3] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP5] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17] INFO:     127.0.0.1:56612 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP1] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17] INFO:     127.0.0.1:57138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:58822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:58970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:59280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:59658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:60514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:60542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:60716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:52106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:52474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:52734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:53976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:55120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:56536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:56548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:58858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:59044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:59190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:59900 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP4] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP6] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP2] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP0] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP7] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP3] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP5] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP1] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17] INFO:     127.0.0.1:52058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:53558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:54540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:54542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:55172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:56078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:56560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:57206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:57538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:58616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:58874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:60160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:33076 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP6] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP4] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP2] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP0] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP7] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP3] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP1] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP5] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17] INFO:     127.0.0.1:52610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:52622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:54498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:54872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:55782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:56492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:58100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:58474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:60622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:33468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:33558 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP2] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP6] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP4] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP0] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP1] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP5] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP3] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP7] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17] INFO:     127.0.0.1:55340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:55484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:55644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:55742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:56478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:57818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:58150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:60360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:60732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:33432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:33808 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP2] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP6] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP4] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP0] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP1] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP3] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP5] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP7] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17] INFO:     127.0.0.1:53164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:58170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:59262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:59266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:60204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:33060 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP6] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP2] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP4] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP0] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP3] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP7] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP1] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP5] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17] INFO:     127.0.0.1:55666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:56952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:57364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:59276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:59330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:59420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:59466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:60834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:33494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:34540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:35050 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP2] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP6] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP4] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP0] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP3] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP7] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP1] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP5] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP0] Decode batch. #running-req: 683, #token: 98742, token usage: 0.10, cuda graph: False, gen throughput (token/s): 6302.04, #queue-req: 0, 
[2025-10-23 11:15:17] INFO:     127.0.0.1:52132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:52824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:52846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:52862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:54586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:54708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:54720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:54944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:56278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:56428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:56668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:56722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:59114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:33508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:34020 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP2] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP4] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP6] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP0] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP3] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP7] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP1] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP5] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17] INFO:     127.0.0.1:53368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:54294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:54722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:56364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:57068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:57786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:58286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:58892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:60150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:17] INFO:     127.0.0.1:60636 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP2] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP6] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP4] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP0] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP3] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP7] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP1] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:17 TP5] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18] INFO:     127.0.0.1:52214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:54002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:55676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:55766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:58542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:60104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:60830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:32926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:33280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:33566 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP6] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP4] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP2] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP0] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP7] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP3] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP1] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP5] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18] INFO:     127.0.0.1:52984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:54698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:55106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:55810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:57100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:57378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:57840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:58530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:59294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:60042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:60262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:52114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:53766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:54444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:57772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:60396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:33994 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP2] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP4] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP6] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP0] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP3] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP7] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP1] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP5] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18] INFO:     127.0.0.1:53078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:54236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:54328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:55202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:55790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:56080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:56176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:57512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:60492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:60766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:33030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:33846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:35212 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP4] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP2] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP6] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP0] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP3] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP7] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP1] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP5] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18] INFO:     127.0.0.1:52218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:55396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:55500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:55712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:56012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:56210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:58140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:58772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:33756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:34374 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP2] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP6] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP4] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP0] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP3] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP7] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP1] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP5] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18] INFO:     127.0.0.1:52808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:53280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:53800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:54094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:54100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:54226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:54840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:57182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:57254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:57884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:58846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:59364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:32982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:33458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:34164 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP2] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP4] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP6] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP0] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP3] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP7] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP1] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP5] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18] INFO:     127.0.0.1:53542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:54426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:57984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:58784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:59844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:60730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:60900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:52072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:52920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:53120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:53338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:54284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:54730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:55000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:55022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:55200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:55252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:55534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:56116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:57344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:59302 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP2] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP4] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP6] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP0] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP3] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP7] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP1] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP5] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18] INFO:     127.0.0.1:52562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:54070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:54684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:56200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:57410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:57646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:58392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:58688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:59254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:59994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:60672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:60938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:33106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:33264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:34114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:18] INFO:     127.0.0.1:35094 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP2] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP4] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP6] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP0] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP3] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP7] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP1] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:18 TP5] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:19] INFO:     127.0.0.1:53642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:54372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:55614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:55924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:57440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:57898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:58162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:58464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:59388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:60742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:32910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:32934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:33388 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:19 TP6] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:19 TP4] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:19 TP2] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:19 TP0] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:19 TP3] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:19 TP7] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:19 TP1] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:19 TP5] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:19] INFO:     127.0.0.1:53214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:54492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:54900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:58914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:60356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:33054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:33542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:33602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:34392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:34516 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:19 TP4] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:19 TP6] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:19 TP2] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:19 TP0] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:19 TP7] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:19 TP3] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:19 TP1] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:19 TP5] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:19] INFO:     127.0.0.1:52318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:53850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:54578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:54792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:54988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:55076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:59542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:60458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:60500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:33646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:33794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:33974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:34254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:52044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:54588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:56030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:56854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:57014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:58094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:58234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:58380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:33606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:54408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:55594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:55992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:56570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:56796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:57890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:58936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:33482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:33932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:34590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:52796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:53140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:54410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:55600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:55708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:56104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:59208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:60018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:60450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:60476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:33078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:33856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:35060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:52028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:52248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:56010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:58032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:60080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:34228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:53594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:57580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:57972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:34632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:55424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:56040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:59132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:59230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:33022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:33120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:33650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:34370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:52520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:56608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:57930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:59342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:32808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:33416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:34202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:53550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:55634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:59422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:59800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:60744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:33152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:33658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:33916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:53102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:54088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:54782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:56828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:57718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:57732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:59456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:59760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:60190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:60818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:51960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:51992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:52076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:52202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:59442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:60762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:32794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:32900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:33634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:35074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:55948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:56618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:56658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:57336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:59410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:33970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:19] INFO:     127.0.0.1:34800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:55090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:59968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:33302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:35152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:35348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:55308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:55878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:58514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:60548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:60954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:35344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:54470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:56734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:57392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:35198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:55242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:57672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:59206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:33914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:55540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:60352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:35326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:53596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:54618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:55654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:56836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:35190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:57126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:57320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:58250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:58588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:58824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:59052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:59590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:33278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:57720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:58064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:58386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:58632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:33708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:33738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:33834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:35030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:53898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:58982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:33404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:33524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:33592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:35236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:52992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:54744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:54774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:56042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:59226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:59384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:32814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:52126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:52778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:53264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:54584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:56128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:59484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:33672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:52068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:33678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:35410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:52454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:52620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:54388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:60292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:60664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:33014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:33316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:33536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:35044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:59602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:60978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:32946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:33380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:33440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:52168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:53628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:54514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:57508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:58596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:32894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20 TP0] Decode batch. #running-req: 315, #token: 57639, token usage: 0.06, cuda graph: True, gen throughput (token/s): 6081.01, #queue-req: 0, 
[2025-10-23 11:15:20] INFO:     127.0.0.1:60144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:33190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:20] INFO:     127.0.0.1:34360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:54130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:56748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:59858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:33230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:33878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:34890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:35328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:35358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:53444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:54032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:54114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:55008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:32846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:32966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:35112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:35442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:53024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:60410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:32796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:32962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:33472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:55700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:57496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:60312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:60594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:33364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:34058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:34124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:35376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:53392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:54180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:58102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:58218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:58576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:52818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:53890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:54112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:55318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:58088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:58582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:33156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:34470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:35162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:60648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:60736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:33094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:33218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:34152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:34580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:34688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:55908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:58046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:32940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:34480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:35288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:35392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:52974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:54214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:33502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:35016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:53180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:53370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:54048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:55226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:55528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:59076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:33898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:52226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:55620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:57622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:33034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:56194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:32858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:33918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:35304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:35372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:35414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:54454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:54886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:56068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:59034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:34780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:35210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:53808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:57264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:60176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:32984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:33178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:34288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:34734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:56626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:57048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:35002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:53296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:34154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:60852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:34482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:34874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:34982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:35010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:53662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:54152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:54268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:21] INFO:     127.0.0.1:60028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:57310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:34078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:34522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:34640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:34952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:35106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:54298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:57796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:58362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:33098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:33724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:35188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:56146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:34068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:34134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:34552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:34786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:33136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:33822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:53148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:33274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:53580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:57380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:58378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:34858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:54254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:35174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:35250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:55828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:56650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:60574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:33780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:34180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:34264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:56732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:32826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:33236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:33270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:33840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:34606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:59010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:53618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:56328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:34272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:55156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:59246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:53942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:33340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:35082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:53924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:59632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:33250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:33830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:34216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:34434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:34676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:35136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:35316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:52012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:57150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:58354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:33682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:34426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:33754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:34448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:35126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:52652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:54918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:33778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:34098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:34842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:60060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:33130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:33314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:33656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:32784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:35366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:52074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:56976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:33948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:34664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:35220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22 TP0] Decode batch. #running-req: 122, #token: 27588, token usage: 0.03, cuda graph: True, gen throughput (token/s): 4105.62, #queue-req: 0, 
[2025-10-23 11:15:22] INFO:     127.0.0.1:53716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:55580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:33172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:52468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:52554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:52674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:22] INFO:     127.0.0.1:57482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:52394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:58554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:59210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:52934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:56470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:34708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:34920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:57658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:59666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:33066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:33506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:34334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:58608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:54934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:57032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:33698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:33790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:34074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:34236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:35394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:55978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:58062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:57810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:34526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:35306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:57000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:33968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:34936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:35260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:34248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:34284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:53598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:54638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:56812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:53038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:33122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:57112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:54614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:58916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:34094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:34830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:55942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:60270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:52738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:32792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:33298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:34462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:58850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:55752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:34720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:34818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:52982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:34010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:23] INFO:     127.0.0.1:33766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:35342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:35428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:60712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:33040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:34654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:58226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:33466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:33354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:59106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:53314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:60932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:33578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:34926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:35272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:34218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:34382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:34766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:34902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24 TP0] Decode batch. #running-req: 40, #token: 10832, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1941.13, #queue-req: 0, 
[2025-10-23 11:15:24] INFO:     127.0.0.1:60088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:33206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:34312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:60136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:33956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:58976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:55930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:32950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:34568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:34696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:60968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:33382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:35172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:24] INFO:     127.0.0.1:35310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:25] INFO:     127.0.0.1:58324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:25] INFO:     127.0.0.1:33990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:25] INFO:     127.0.0.1:53150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:25] INFO:     127.0.0.1:54738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:25] INFO:     127.0.0.1:34104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:25] INFO:     127.0.0.1:32834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:25] INFO:     127.0.0.1:34034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:25] INFO:     127.0.0.1:33618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:25] INFO:     127.0.0.1:52346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:25 TP0] Decode batch. #running-req: 15, #token: 4141, token usage: 0.00, cuda graph: True, gen throughput (token/s): 945.38, #queue-req: 0, 
[2025-10-23 11:15:25] INFO:     127.0.0.1:55646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:25] INFO:     127.0.0.1:33226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:25] INFO:     127.0.0.1:33294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:25] INFO:     127.0.0.1:33452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:25] INFO:     127.0.0.1:34988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:25] INFO:     127.0.0.1:58796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:25] INFO:     127.0.0.1:59110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:25] INFO:     127.0.0.1:34928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:25] INFO:     127.0.0.1:34276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:25] INFO:     127.0.0.1:60844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:25] INFO:     127.0.0.1:32770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:26] INFO:     127.0.0.1:34024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:26] INFO:     127.0.0.1:32880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:26 TP0] Decode batch. #running-req: 2, #token: 1266, token usage: 0.00, cuda graph: True, gen throughput (token/s): 296.34, #queue-req: 0, 
[2025-10-23 11:15:26] INFO:     127.0.0.1:33674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:26] INFO:     127.0.0.1:34148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:39] INFO:     127.0.0.1:53232 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-23 11:15:39 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 11:15:39] INFO:     127.0.0.1:53242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:39 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 11:15:39 TP0] Prefill batch. #new-seq: 23, #new-token: 23, #cached-token: 16706, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP1] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP0] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP3] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP4] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP2] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP7] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP6] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP5] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP0] Prefill batch. #new-seq: 43, #new-token: 43, #cached-token: 31580, token usage: 0.01, #running-req: 24, #queue-req: 0, 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP0] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP2] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP3] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP4] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP1] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP6] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP7] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP5] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP0] Prefill batch. #new-seq: 50, #new-token: 50, #cached-token: 36346, token usage: 0.01, #running-req: 67, #queue-req: 0, 
[2025-10-23 11:15:39 TP0] Prefill batch. #new-seq: 59, #new-token: 59, #cached-token: 42892, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP2] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP3] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP1] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP0] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP7] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP6] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP4] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP5] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:39 TP0] Prefill batch. #new-seq: 53, #new-token: 53, #cached-token: 38407, token usage: 0.01, #running-req: 176, #queue-req: 0, 
[2025-10-23 11:15:40 TP0] Prefill batch. #new-seq: 65, #new-token: 65, #cached-token: 47420, token usage: 0.02, #running-req: 229, #queue-req: 0, 
[2025-10-23 11:15:40 TP0] Prefill batch. #new-seq: 59, #new-token: 59, #cached-token: 42862, token usage: 0.02, #running-req: 294, #queue-req: 0, 
[2025-10-23 11:15:40 TP0] Prefill batch. #new-seq: 72, #new-token: 72, #cached-token: 52588, token usage: 0.03, #running-req: 353, #queue-req: 0, 
[2025-10-23 11:15:40 TP0] Prefill batch. #new-seq: 68, #new-token: 68, #cached-token: 49472, token usage: 0.03, #running-req: 425, #queue-req: 0, 
[2025-10-23 11:15:40 TP0] Prefill batch. #new-seq: 24, #new-token: 24, #cached-token: 17353, token usage: 0.03, #running-req: 493, #queue-req: 0, 
[2025-10-23 11:15:40 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5046, token usage: 0.03, #running-req: 517, #queue-req: 0, 
[2025-10-23 11:15:40 TP0] Prefill batch. #new-seq: 47, #new-token: 47, #cached-token: 34059, token usage: 0.04, #running-req: 524, #queue-req: 0, 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:40 TP2] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:40 TP4] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:40 TP0] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:40 TP6] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:40 TP1] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:40 TP3] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:40 TP7] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:40 TP5] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:40 TP0] Prefill batch. #new-seq: 48, #new-token: 48, #cached-token: 35044, token usage: 0.04, #running-req: 571, #queue-req: 0, 
[2025-10-23 11:15:41 TP0] Prefill batch. #new-seq: 84, #new-token: 84, #cached-token: 61150, token usage: 0.04, #running-req: 619, #queue-req: 0, 
[2025-10-23 11:15:41 TP0] Prefill batch. #new-seq: 56, #new-token: 56, #cached-token: 40695, token usage: 0.05, #running-req: 703, #queue-req: 0, 
[2025-10-23 11:15:41 TP0] Prefill batch. #new-seq: 92, #new-token: 92, #cached-token: 67178, token usage: 0.05, #running-req: 759, #queue-req: 0, 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:41 TP0] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:41 TP2] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:41 TP3] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:41 TP1] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:41 TP7] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:41 TP4] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:41 TP6] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:41 TP5] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:41 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 44107, token usage: 0.06, #running-req: 851, #queue-req: 0, 
[2025-10-23 11:15:41 TP0] Prefill batch. #new-seq: 98, #new-token: 98, #cached-token: 72014, token usage: 0.06, #running-req: 912, #queue-req: 0, 
[aiter] [fused_moe] using default for (98, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (98, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:41 TP7] [fused_moe] using default for (98, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:41 TP4] [fused_moe] using default for (98, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (98, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:41 TP6] [fused_moe] using default for (98, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (98, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:41 TP5] [fused_moe] using default for (98, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (98, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:41 TP2] [fused_moe] using default for (98, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (98, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (98, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:41 TP1] [fused_moe] using default for (98, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:41 TP3] [fused_moe] using default for (98, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (98, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:41 TP0] [fused_moe] using default for (98, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:41 TP0] Prefill batch. #new-seq: 14, #new-token: 14, #cached-token: 10277, token usage: 0.06, #running-req: 1010, #queue-req: 52, 
[2025-10-23 11:15:45] INFO:     127.0.0.1:56126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:45 TP0] Decode batch. #running-req: 1024, #token: 90353, token usage: 0.09, cuda graph: False, gen throughput (token/s): 1449.33, #queue-req: 295, 
[2025-10-23 11:15:45 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 711, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-10-23 11:15:46] INFO:     127.0.0.1:56186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:46 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 710, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-10-23 11:15:47] INFO:     127.0.0.1:53280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47] INFO:     127.0.0.1:54806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47] INFO:     127.0.0.1:56896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47] INFO:     127.0.0.1:54220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47] INFO:     127.0.0.1:57644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 2201, token usage: 0.11, #running-req: 1021, #queue-req: 290, 
[2025-10-23 11:15:47] INFO:     127.0.0.1:55216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47] INFO:     127.0.0.1:55484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47] INFO:     127.0.0.1:59052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47] INFO:     127.0.0.1:33722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4475, token usage: 0.11, #running-req: 1018, #queue-req: 284, 
[2025-10-23 11:15:47] INFO:     127.0.0.1:53576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47] INFO:     127.0.0.1:55444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47] INFO:     127.0.0.1:56144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47] INFO:     127.0.0.1:59070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47] INFO:     127.0.0.1:33642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3698, token usage: 0.11, #running-req: 1019, #queue-req: 279, 
[2025-10-23 11:15:47] INFO:     127.0.0.1:53700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47] INFO:     127.0.0.1:55408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47] INFO:     127.0.0.1:56014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47] INFO:     127.0.0.1:56416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47] INFO:     127.0.0.1:57914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47] INFO:     127.0.0.1:58050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47] INFO:     127.0.0.1:58120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47] INFO:     127.0.0.1:58200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5805, token usage: 0.11, #running-req: 1016, #queue-req: 271, 
[2025-10-23 11:15:47] INFO:     127.0.0.1:55054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47] INFO:     127.0.0.1:58300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47] INFO:     127.0.0.1:33700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47] INFO:     127.0.0.1:33848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:47 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2840, token usage: 0.11, #running-req: 1020, #queue-req: 267, 
[2025-10-23 11:15:48] INFO:     127.0.0.1:54198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:57066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:57574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:58522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:33102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3659, token usage: 0.11, #running-req: 1019, #queue-req: 262, 
[2025-10-23 11:15:48] INFO:     127.0.0.1:56542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:59394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:60700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:33312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2861, token usage: 0.11, #running-req: 1020, #queue-req: 258, 
[2025-10-23 11:15:48] INFO:     127.0.0.1:55014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:55060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:56076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:56684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:56868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:57782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:60936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:32942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:33252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:33774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7285, token usage: 0.11, #running-req: 1014, #queue-req: 248, 
[2025-10-23 11:15:48] INFO:     127.0.0.1:53260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:54074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:54474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:55104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:56102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:60172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:32790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:33054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:34216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6682, token usage: 0.11, #running-req: 1015, #queue-req: 239, 
[2025-10-23 11:15:48] INFO:     127.0.0.1:53942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:54046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:55564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:48] INFO:     127.0.0.1:32768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3582, token usage: 0.11, #running-req: 1019, #queue-req: 234, 
[2025-10-23 11:15:49] INFO:     127.0.0.1:54010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:56596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:57198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:57706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:57752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:58214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:58958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5146, token usage: 0.12, #running-req: 1017, #queue-req: 227, 
[2025-10-23 11:15:49] INFO:     127.0.0.1:56058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:58812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:59192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:59632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:32988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3701, token usage: 0.12, #running-req: 1019, #queue-req: 222, 
[2025-10-23 11:15:49] INFO:     127.0.0.1:53514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:53980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:54494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:55028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:55160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:59684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:59784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:60516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:33062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:33216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7224, token usage: 0.12, #running-req: 1014, #queue-req: 212, 
[2025-10-23 11:15:49] INFO:     127.0.0.1:53472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:54118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:55818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:55882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:56530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:57368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:58536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:59096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:59270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:59830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:33134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 7850, token usage: 0.12, #running-req: 1013, #queue-req: 201, 
[2025-10-23 11:15:49] INFO:     127.0.0.1:53460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:53552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:53858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:54878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:57980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:59404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:59544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:59724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:59732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:49] INFO:     127.0.0.1:60132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7361, token usage: 0.12, #running-req: 1014, #queue-req: 191, 
[2025-10-23 11:15:50] INFO:     127.0.0.1:53602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:54562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:54646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:56328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:57986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:59850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:60264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:60340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:33408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:33894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7244, token usage: 0.12, #running-req: 1014, #queue-req: 181, 
[2025-10-23 11:15:50] INFO:     127.0.0.1:53250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:53484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:54720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:55178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:55262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:56606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:58874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:60378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:32932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6538, token usage: 0.12, #running-req: 1015, #queue-req: 172, 
[2025-10-23 11:15:50] INFO:     127.0.0.1:54938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:55132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:56220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:56578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:58226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:59254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:59368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:60650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5730, token usage: 0.12, #running-req: 1016, #queue-req: 164, 
[2025-10-23 11:15:50] INFO:     127.0.0.1:59502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:59976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:60948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50] INFO:     127.0.0.1:33702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:50 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2899, token usage: 0.12, #running-req: 1020, #queue-req: 160, 
[2025-10-23 11:15:51] INFO:     127.0.0.1:54052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:56752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:57606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:58282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:60018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:60496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:33426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:34132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5849, token usage: 0.12, #running-req: 1016, #queue-req: 152, 
[2025-10-23 11:15:51] INFO:     127.0.0.1:54548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:55186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:56242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:57550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:57832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:58064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:33374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:33562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:33648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6642, token usage: 0.12, #running-req: 1015, #queue-req: 143, 
[2025-10-23 11:15:51] INFO:     127.0.0.1:54058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:54346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:55076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:55414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:57584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:58048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:58280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:58880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:58978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:60448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:60718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:33622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8665, token usage: 0.12, #running-req: 1012, #queue-req: 131, 
[2025-10-23 11:15:51] INFO:     127.0.0.1:53426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:54482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:54774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:54974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:55552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:57800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:60312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:33280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5906, token usage: 0.12, #running-req: 1016, #queue-req: 123, 
[2025-10-23 11:15:51] INFO:     127.0.0.1:54170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:55928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:56712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:58728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:59426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:60656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:60832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:33176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:51] INFO:     127.0.0.1:33498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6614, token usage: 0.12, #running-req: 1015, #queue-req: 114, 
[2025-10-23 11:15:52] INFO:     127.0.0.1:53766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:54924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:55642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:56256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:56280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:57316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:32900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:32998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:33172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:33190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7349, token usage: 0.12, #running-req: 1014, #queue-req: 104, 
[2025-10-23 11:15:52] INFO:     127.0.0.1:53626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:53780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:54040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:54290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:54682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:56056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:56106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:57040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:57900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:57918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:58444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:33228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:33482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:34102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52 TP0] Prefill batch. #new-seq: 14, #new-token: 14, #cached-token: 10247, token usage: 0.12, #running-req: 1010, #queue-req: 90, 
[2025-10-23 11:15:52] INFO:     127.0.0.1:53772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:54240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:54446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:54544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:55744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:55766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:56566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:58854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:59050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:59666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7229, token usage: 0.12, #running-req: 1014, #queue-req: 80, 
[2025-10-23 11:15:52 TP0] Decode batch. #running-req: 1014, #token: 119542, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5537.36, #queue-req: 80, 
[2025-10-23 11:15:52] INFO:     127.0.0.1:53452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:54214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:55798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:55812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:56268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:56766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:56780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:58186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:59162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:59722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:60250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:60730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:33158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:33310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52 TP0] Prefill batch. #new-seq: 15, #new-token: 15, #cached-token: 11013, token usage: 0.13, #running-req: 1009, #queue-req: 65, 
[2025-10-23 11:15:52] INFO:     127.0.0.1:54500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:54906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:54922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:55460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:56000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:57878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:58926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:59036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:59062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:59378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:59504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:60182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:60216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:32916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:32974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:32980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:52] INFO:     127.0.0.1:33944 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:52 TP2] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:52 TP6] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:52 TP3] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:52 TP4] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:52 TP0] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:52 TP7] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:52 TP1] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:52 TP5] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:53 TP0] Prefill batch. #new-seq: 17, #new-token: 17, #cached-token: 12572, token usage: 0.13, #running-req: 1007, #queue-req: 48, 
[2025-10-23 11:15:53] INFO:     127.0.0.1:53902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:54104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:54684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:56608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:57070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:57180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:58034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:58132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:58964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:59222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:33584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 7964, token usage: 0.13, #running-req: 1013, #queue-req: 37, 
[2025-10-23 11:15:53] INFO:     127.0.0.1:53788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:55106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:56034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:56046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:56402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:57028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:57904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:58658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:60120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:60440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:60688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:33076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8745, token usage: 0.13, #running-req: 1012, #queue-req: 25, 
[2025-10-23 11:15:53] INFO:     127.0.0.1:54022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:55404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:56340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:57222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:57280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:57470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:57626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:58784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:59884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:60234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:60610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 8041, token usage: 0.13, #running-req: 1013, #queue-req: 14, 
[2025-10-23 11:15:53] INFO:     127.0.0.1:54504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:55148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:55608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:58348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:59028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:59552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:60064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:60116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:33960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:34022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53] INFO:     127.0.0.1:34202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:53 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 8025, token usage: 0.13, #running-req: 1013, #queue-req: 3, 
[2025-10-23 11:15:54] INFO:     127.0.0.1:53492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:54904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:55482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:56764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:60528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:33666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 2149, token usage: 0.13, #running-req: 1018, #queue-req: 0, 
[2025-10-23 11:15:54] INFO:     127.0.0.1:55512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:55828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:56452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:56726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:59178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:59276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:59546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:33042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:34140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:53590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:54452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:54532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:54756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:55898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:57172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:57804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:57938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:58078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:59954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:33822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:33862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:34224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:55392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:55582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:55632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:56222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:58084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:59216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:59454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:60292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:60604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:60746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:33290 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP2] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP6] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP4] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP3] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP0] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP7] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP1] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP5] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54] INFO:     127.0.0.1:53810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:53844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:54354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:57520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:57962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:32862 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP2] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP6] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP4] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP0] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP3] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP7] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP1] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP5] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54] INFO:     127.0.0.1:56366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:58270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:58320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:58356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:33000 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP2] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP6] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP4] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP3] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP0] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP7] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP1] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54 TP5] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:54] INFO:     127.0.0.1:53592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:55286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:57610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:57692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:58384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:58604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:59968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:60272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:60984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:33306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:33350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:33842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:54] INFO:     127.0.0.1:33912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:53728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:54378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:54656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:54694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:54762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:55698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:56352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:58124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:58474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:60068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:33256 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP2] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP6] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP4] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP0] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP3] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP7] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP1] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP5] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55] INFO:     127.0.0.1:53526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:54318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:55526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:58370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:58556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:59340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:59520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:59752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:59802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:60268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:60880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:33574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:33616 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP2] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP6] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP4] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP0] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP3] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP7] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP1] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP5] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55] INFO:     127.0.0.1:54408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:57178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:57332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:57950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:58944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:59586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:60102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:60566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:60920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:33352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:53684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:53926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:54434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:55668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:58546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:60904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:54242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:54264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:54900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:55974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:56794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:57884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:58314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:59716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:60724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:33582 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP6] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP4] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP7] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP5] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP2] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP0] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP3] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP1] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55] INFO:     127.0.0.1:54182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:54584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:56072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:56278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:56912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:57124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:58912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:59296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:59876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:60858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:33304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:33612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:34336 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP2] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP6] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP4] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP1] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP3] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP7] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP0] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP5] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55] INFO:     127.0.0.1:54822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:55956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:56156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:56668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:56740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:58402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:58500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:58622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:59556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:60554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:60582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:32804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:32852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:34138 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP2] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP6] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP0] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP4] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP7] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP3] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP1] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP5] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55] INFO:     127.0.0.1:53918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:54224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:54574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:54650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:55986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:57082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:57920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:33802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:55] INFO:     127.0.0.1:33858 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP2] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP6] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP3] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP0] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP4] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP7] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP1] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:55 TP5] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56] INFO:     127.0.0.1:55622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:56624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:56698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:56882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:58176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:58334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:58698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:60002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:60212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:60288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:60350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:33090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:33244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:33534 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP2] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP6] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP0] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP4] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP3] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP7] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP1] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP5] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56] INFO:     127.0.0.1:53312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:53872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:59438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:33206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:33394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:33442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:33882 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP2] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP6] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP4] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP7] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP3] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP0] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP1] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP5] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56] INFO:     127.0.0.1:53966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:54866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:54990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:56460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:59792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:60230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:60728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:32914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:32954 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP2] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP6] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP4] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP3] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP1] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP0] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP7] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP5] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56] INFO:     127.0.0.1:53634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:53792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:54256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:54670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:54958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:55592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:56652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:57248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:57346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:57542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:60048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:33116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:34232 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP2] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP6] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP0] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP4] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP3] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP7] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP1] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP5] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56] INFO:     127.0.0.1:53288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:53710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:55098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:55536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:57502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:57656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:58044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:60302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:60490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:33518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:33652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:34294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:56208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:57254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:59490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:32902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:33146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:33670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:33962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:34716 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP2] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP6] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP0] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP4] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP3] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP7] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP1] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP5] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56] INFO:     127.0.0.1:55960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:58288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:58542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:59518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:59800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:32834 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP2] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP6] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP4] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP0] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP3] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP7] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP1] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP5] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56] INFO:     127.0.0.1:53304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:53990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:56484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:57092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:57232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:57736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:58630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:60800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:56] INFO:     127.0.0.1:34944 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP2] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP6] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP4] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP0] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP3] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP7] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP1] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:56 TP5] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57] INFO:     127.0.0.1:53508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:53948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:56716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:57398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:57818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:58428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:58460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:59388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:59814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:33660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:34080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:34370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:34422 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP2] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP6] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP4] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP3] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP0] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP7] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP1] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP5] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57] INFO:     127.0.0.1:53836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:54418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:54550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:56172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:57872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:57944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:58042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:58820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:59862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:60160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:33868 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP2] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP6] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP4] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP3] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP0] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP7] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP1] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP5] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57] INFO:     127.0.0.1:53448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:57350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:57840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:57908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:58740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:59032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:59148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:59816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:60464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:35400 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP2] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP6] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP4] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP3] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP0] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP7] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP1] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP5] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57] INFO:     127.0.0.1:55472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:56816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:57710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:57726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:58208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:60896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:60938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:33340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:33818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:33942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:34118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:35420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:56394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:56638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:56802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:57998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:58570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:59602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:60758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:33770 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP6] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP2] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP4] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP7] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP0] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP3] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP1] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP5] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57] INFO:     127.0.0.1:54002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:55452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:56088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:57088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:57860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:33032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:33094 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP6] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP2] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP4] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP7] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP0] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP3] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP1] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP5] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57] INFO:     127.0.0.1:53370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:54154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:54616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:54712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:55284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:55430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:55654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:56442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:57104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:58096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:59962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:59978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:60032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:32884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:34878 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP2] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP4] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP6] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP0] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP7] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP3] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP1] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57 TP5] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:57] INFO:     127.0.0.1:54228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:54936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:57792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:58142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:33296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:33664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:33972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:34168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:57] INFO:     127.0.0.1:35106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:53474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:55388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:55730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:56602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:56982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:57046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:57108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:57414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:57978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:59352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:60398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:60630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:32808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:34962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:35354 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP2] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP6] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP4] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP0] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP3] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP7] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP1] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP5] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58] INFO:     127.0.0.1:56130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:59700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:59744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:60882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:34028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:34062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:34196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:34574 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP2] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP6] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP4] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP1] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP3] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP0] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP7] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP5] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58] INFO:     127.0.0.1:53396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:54686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:55200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:55680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:56360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:58362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:58650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:58802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:60084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:33904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:34278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:34484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:35024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:36532 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP2] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP6] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP4] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP0] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP3] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP7] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP1] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP5] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP0] Decode batch. #running-req: 690, #token: 99571, token usage: 0.10, cuda graph: False, gen throughput (token/s): 6252.95, #queue-req: 0, 
[2025-10-23 11:15:58] INFO:     127.0.0.1:53444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:54496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:54952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:55232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:55848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:56250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:56330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:57024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:57558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:58508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:32814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:33762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:35072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:35410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:35548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:36112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:54630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:54730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:54916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:57868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:58676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:33462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:33786 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP2] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP6] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP0] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP4] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP3] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP7] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP1] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP5] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58] INFO:     127.0.0.1:54338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:55002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:55752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:58026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:58646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:58702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:59412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:60072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:60676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:33596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:34184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:34846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:35098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:53412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:53542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:54736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:54768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:55888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:55942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:56380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:57174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:57724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:58660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:58846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:59382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:32996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:33510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:33978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:34362 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP2] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP6] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP4] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP0] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP3] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP7] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP1] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP5] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58] INFO:     127.0.0.1:53442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:53828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:55340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:57664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:59280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:59916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:60864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:33546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:34006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:35516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:36666 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP2] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP6] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP4] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP0] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP3] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP7] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP1] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58 TP5] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:58] INFO:     127.0.0.1:54094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:54362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:55748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:56686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:57270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:57456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:58] INFO:     127.0.0.1:59308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:55896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:57352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:60430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:33208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:34548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:35948 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP2] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP6] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP4] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP3] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP0] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP7] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP1] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP5] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59] INFO:     127.0.0.1:53506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:55312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:56840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:58254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:59938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:60298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:60328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:60388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:33712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:34978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:35690 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP2] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP4] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP6] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP0] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP3] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP7] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP1] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP5] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59] INFO:     127.0.0.1:53560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:54302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:54870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:54976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:59246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:59656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:59682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:60908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:60962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:35648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:35960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:53376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:53808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:55688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:56026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:56298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:56518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:56852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:56938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:57716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:58666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:60246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:60486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:60684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:60844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:32850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:32964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:33682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:36562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:53652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:56318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:57496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:57680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:58860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:59112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:59994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:34496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:34558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:34628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:34802 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP2] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP6] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP4] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP0] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP3] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP7] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP1] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP5] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59] INFO:     127.0.0.1:53360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:53970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:55248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:33662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:34406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:36148 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP6] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP4] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP2] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP0] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP3] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP7] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP1] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP5] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59] INFO:     127.0.0.1:54152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:55912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:56190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:59240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:34932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:35096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:35152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:35210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:36058 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP4] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP6] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP2] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP7] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP3] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP0] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP5] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP1] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59] INFO:     127.0.0.1:54472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:55278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:56112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:56288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:59536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:33326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:33748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:35346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:35512 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP2] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP6] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP4] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP7] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP0] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP3] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP1] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP5] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59] INFO:     127.0.0.1:55852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:57132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:57384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:57548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:58272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:59012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:59774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:34626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:35168 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP6] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP2] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP4] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP0] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP3] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP7] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP1] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59 TP5] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:15:59] INFO:     127.0.0.1:55660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:56928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:57148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:57936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:58204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:58896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:60788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:32784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:32846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:33366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:33740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:34152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:35038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:35474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:35778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:36188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:54276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:55376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:55490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:59442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:59696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:35390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:15:59] INFO:     127.0.0.1:36546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:53346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:53400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:54564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:55300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:55610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:57360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:57482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:33478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:35838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:54704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:55124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:55164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:56520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:56826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:58616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:59334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:59534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:60702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:60978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:34530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:35200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:35284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:60070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:60362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:60940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:55158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:59616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:59866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:32816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:33156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:34054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:35442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:56508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:57160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:58028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:60510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:60768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:34946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:35226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:35738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:54752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:56100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:56458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:57396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:58244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:59208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:35182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:36536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:53274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:53324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:59136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:59464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:60586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:32828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:34394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:34540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:35006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:36480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:57308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:57634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:58794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:35504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:36332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:53950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:56390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:58012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:34862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:36850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:56550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:58752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:59478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:36308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:36672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:36844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:54518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:56892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:56956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:57212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:57406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:58872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:33556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:33658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:34248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:35712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:36050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:36328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:53464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:54120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:55174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:55326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:59086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:34586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:35452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:35590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:35668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:35938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:36596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:36814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:59558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:60478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:36650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:53840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:54402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:54850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:55114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:55968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:57008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:60128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:60360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:32956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:33730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:34016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:35298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:35656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:35752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:00] INFO:     127.0.0.1:36284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:56084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:57058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:59010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:59924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:60144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:60636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:33416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:35016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:54384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:55356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:59580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:60580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:57062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:58586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:60802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:60818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:35088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:35136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:35254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:35976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:53794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:54324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:33120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:33270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:34262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:35220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:35238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:35372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:35904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:53446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:57508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:34376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:34520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:35842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:33636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:34960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:54460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:56948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:34292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:34318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:34470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:34852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:35046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:35922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:35958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:55820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:60124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:32876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:34382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:34916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:34982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:54874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:55578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:33516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:33896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:34040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:34798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01 TP0] Decode batch. #running-req: 314, #token: 57321, token usage: 0.06, cuda graph: True, gen throughput (token/s): 5990.18, #queue-req: 0, 
[2025-10-23 11:16:01] INFO:     127.0.0.1:58992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:34768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:35186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:35908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:55056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:55504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:58174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:59562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:35412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:53668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:54610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:55082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:56314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:57430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:33692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:35658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:53888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:55618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:60118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:54838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:57052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:58928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:34436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:53764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:54086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:55868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:59756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:34560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:34674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:35430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:35566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:54530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:55840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:56582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:34732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:35970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:58072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:34606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:01] INFO:     127.0.0.1:36878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:53756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:57250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:59598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:33860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:35884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:54886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:55790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:60652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:35060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:54918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:56492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:56844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:34326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:34368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:34426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:35880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:55778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:54744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:56966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:60106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:35462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:56188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:58684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:60626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:35688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:55042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:58164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:58488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:33640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:33988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:53676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:54790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:34600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:34702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:57196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:56468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:56920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:58718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:59890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:34622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:35270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:35364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:55914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:59228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:55716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:59090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:34668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:35568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:35632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:53334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:58768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:34282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:59902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:60816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:34412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:35332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:35550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:54998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:58832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:59646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:33906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:34812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:54788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:57596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:60588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:35242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:02] INFO:     127.0.0.1:36688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:33832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:35374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:55638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:35822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:35974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:36346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:54136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:58112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:59318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:60780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:34784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:35706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:35810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:33214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:33382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:54388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:54932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:56426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:60196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:32992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:34822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:34898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:35854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:36776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:34788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:36590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:56002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:57534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:58414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:36218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:35370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:35994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:56206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:34658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:34890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:35306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:58588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:34868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:35602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:54032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:34644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:36682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:36862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:58386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:35490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:36228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03 TP0] Decode batch. #running-req: 123, #token: 27709, token usage: 0.03, cuda graph: True, gen throughput (token/s): 4146.90, #queue-req: 0, 
[2025-10-23 11:16:03] INFO:     127.0.0.1:34442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:34676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:34690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:36074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:36588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:60100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:34888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:35312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:35518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:35886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:36434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:55712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:35894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:36252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:34682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:34854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:33016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:55372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:55534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:56236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:58472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:33598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:35318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:36444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:59566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:35582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:57318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:34346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:35794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:36894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:59124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:33984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:35728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:36088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:03] INFO:     127.0.0.1:36544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:35510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:36720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:34302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:53702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:58228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:53386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:53742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:53720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:60130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:60536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:35236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:55662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:34996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:57296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:54596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:35304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:35998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:60672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:35044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:57100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:35228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:36262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:36362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:53648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:60548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:35626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:35528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:53826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:36206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:36334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:36840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:59766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:36928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:36790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:34510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:57768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:36428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:58568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:35122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:36398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:04] INFO:     127.0.0.1:36734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:35956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:35610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05 TP0] Decode batch. #running-req: 43, #token: 11889, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1968.11, #queue-req: 0, 
[2025-10-23 11:16:05] INFO:     127.0.0.1:33450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:36128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:35764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:57276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:33488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:33522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:33968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:59840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:60414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:60686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:35872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:34902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:36256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:36304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:33936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:56994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:33992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:53618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:35486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:36802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:55118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:34440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:35540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:33926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:05] INFO:     127.0.0.1:35180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:06] INFO:     127.0.0.1:36628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:06] INFO:     127.0.0.1:34836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:06 TP0] Decode batch. #running-req: 16, #token: 5120, token usage: 0.01, cuda graph: True, gen throughput (token/s): 972.37, #queue-req: 0, 
[2025-10-23 11:16:06] INFO:     127.0.0.1:34964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:06] INFO:     127.0.0.1:34378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:06] INFO:     127.0.0.1:60332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:06] INFO:     127.0.0.1:36414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:06] INFO:     127.0.0.1:34754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:06] INFO:     127.0.0.1:36482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:06] INFO:     127.0.0.1:59768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:06] INFO:     127.0.0.1:35980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:06] INFO:     127.0.0.1:35534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:06] INFO:     127.0.0.1:34088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:06] INFO:     127.0.0.1:35862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:06] INFO:     127.0.0.1:34742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:06] INFO:     127.0.0.1:34068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:06] INFO:     127.0.0.1:34454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:06 TP0] Decode batch. #running-req: 1, #token: 955, token usage: 0.00, cuda graph: True, gen throughput (token/s): 322.97, #queue-req: 0, 
[2025-10-23 11:16:07] INFO:     127.0.0.1:35676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:19] INFO:     127.0.0.1:49962 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-23 11:16:19 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 11:16:19] INFO:     127.0.0.1:49970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:19 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 11:16:20 TP0] Prefill batch. #new-seq: 41, #new-token: 41, #cached-token: 29768, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-23 11:16:20 TP0] Prefill batch. #new-seq: 47, #new-token: 47, #cached-token: 34162, token usage: 0.01, #running-req: 42, #queue-req: 0, 
[2025-10-23 11:16:20 TP0] Prefill batch. #new-seq: 51, #new-token: 51, #cached-token: 37371, token usage: 0.01, #running-req: 89, #queue-req: 0, 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:20 TP2] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:20 TP0] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:20 TP1] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:20 TP7] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:20 TP4] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:20 TP3] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:20 TP6] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:20 TP5] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:20 TP0] Prefill batch. #new-seq: 59, #new-token: 59, #cached-token: 42974, token usage: 0.01, #running-req: 140, #queue-req: 0, 
[2025-10-23 11:16:20 TP0] Prefill batch. #new-seq: 47, #new-token: 47, #cached-token: 34212, token usage: 0.02, #running-req: 199, #queue-req: 0, 
[2025-10-23 11:16:20 TP0] Prefill batch. #new-seq: 19, #new-token: 19, #cached-token: 13833, token usage: 0.02, #running-req: 246, #queue-req: 0, 
[2025-10-23 11:16:20 TP0] Prefill batch. #new-seq: 69, #new-token: 69, #cached-token: 50078, token usage: 0.02, #running-req: 265, #queue-req: 0, 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:20 TP0] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:20 TP4] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:20 TP6] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:20 TP2] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:20 TP7] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:20 TP1] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:20 TP5] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:20 TP3] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:20 TP0] Prefill batch. #new-seq: 50, #new-token: 50, #cached-token: 36393, token usage: 0.02, #running-req: 334, #queue-req: 0, 
[2025-10-23 11:16:21 TP0] Prefill batch. #new-seq: 79, #new-token: 79, #cached-token: 57736, token usage: 0.03, #running-req: 384, #queue-req: 0, 
[2025-10-23 11:16:21 TP0] Prefill batch. #new-seq: 55, #new-token: 55, #cached-token: 39731, token usage: 0.03, #running-req: 463, #queue-req: 0, 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP7] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP2] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP0] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP4] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP1] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP3] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP6] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP5] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP0] Prefill batch. #new-seq: 87, #new-token: 87, #cached-token: 63316, token usage: 0.04, #running-req: 518, #queue-req: 0, 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP2] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP0] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP1] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP7] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP3] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP4] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP6] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP5] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP0] Prefill batch. #new-seq: 58, #new-token: 58, #cached-token: 42166, token usage: 0.04, #running-req: 605, #queue-req: 0, 
[2025-10-23 11:16:21 TP0] Prefill batch. #new-seq: 92, #new-token: 92, #cached-token: 66993, token usage: 0.05, #running-req: 663, #queue-req: 0, 
[2025-10-23 11:16:21 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 45315, token usage: 0.05, #running-req: 755, #queue-req: 0, 
[2025-10-23 11:16:21 TP0] Prefill batch. #new-seq: 97, #new-token: 97, #cached-token: 70284, token usage: 0.06, #running-req: 817, #queue-req: 0, 
[aiter] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP2] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP7] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP0] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP1] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP4] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP3] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP6] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:21 TP5] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:22 TP0] Prefill batch. #new-seq: 70, #new-token: 70, #cached-token: 51376, token usage: 0.06, #running-req: 914, #queue-req: 0, 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:22 TP2] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:22 TP1] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:22 TP3] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:22 TP7] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:22 TP4] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:22 TP6] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:22 TP5] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:22 TP0] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:22 TP0] Prefill batch. #new-seq: 40, #new-token: 40, #cached-token: 29384, token usage: 0.06, #running-req: 984, #queue-req: 67, 
[2025-10-23 11:16:25 TP0] Decode batch. #running-req: 1024, #token: 88009, token usage: 0.09, cuda graph: False, gen throughput (token/s): 1369.20, #queue-req: 295, 
[2025-10-23 11:16:25] INFO:     127.0.0.1:53066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:25 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 790, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-10-23 11:16:26] INFO:     127.0.0.1:50322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:26 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 776, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-10-23 11:16:26] INFO:     127.0.0.1:58798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:26 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 772, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-10-23 11:16:26] INFO:     127.0.0.1:53092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:27 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 750, token usage: 0.10, #running-req: 1023, #queue-req: 291, 
[2025-10-23 11:16:27] INFO:     127.0.0.1:50692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:27 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 737, token usage: 0.10, #running-req: 1023, #queue-req: 290, 
[2025-10-23 11:16:27] INFO:     127.0.0.1:50020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:27] INFO:     127.0.0.1:51270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:27 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 1462, token usage: 0.10, #running-req: 1022, #queue-req: 288, 
[2025-10-23 11:16:27] INFO:     127.0.0.1:51220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:27] INFO:     127.0.0.1:51732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:27 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 1478, token usage: 0.11, #running-req: 1022, #queue-req: 286, 
[2025-10-23 11:16:27] INFO:     127.0.0.1:50982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:27] INFO:     127.0.0.1:53790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:27] INFO:     127.0.0.1:54828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:27 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 2208, token usage: 0.11, #running-req: 1021, #queue-req: 283, 
[2025-10-23 11:16:28] INFO:     127.0.0.1:50346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28] INFO:     127.0.0.1:50468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28] INFO:     127.0.0.1:50740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28] INFO:     127.0.0.1:54818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2926, token usage: 0.11, #running-req: 1020, #queue-req: 279, 
[2025-10-23 11:16:28] INFO:     127.0.0.1:51682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28] INFO:     127.0.0.1:52388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28] INFO:     127.0.0.1:52920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28] INFO:     127.0.0.1:55896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2914, token usage: 0.11, #running-req: 1020, #queue-req: 275, 
[2025-10-23 11:16:28] INFO:     127.0.0.1:53058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28] INFO:     127.0.0.1:55002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28] INFO:     127.0.0.1:55908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28] INFO:     127.0.0.1:58762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2891, token usage: 0.11, #running-req: 1020, #queue-req: 271, 
[2025-10-23 11:16:28] INFO:     127.0.0.1:50316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28] INFO:     127.0.0.1:52684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28] INFO:     127.0.0.1:54542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28] INFO:     127.0.0.1:54706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28] INFO:     127.0.0.1:54994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28] INFO:     127.0.0.1:55082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28] INFO:     127.0.0.1:55374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28] INFO:     127.0.0.1:58030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28] INFO:     127.0.0.1:59346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6499, token usage: 0.11, #running-req: 1015, #queue-req: 262, 
[2025-10-23 11:16:28] INFO:     127.0.0.1:50522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28] INFO:     127.0.0.1:50574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28] INFO:     127.0.0.1:52180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28] INFO:     127.0.0.1:55224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28] INFO:     127.0.0.1:58882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:28 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4310, token usage: 0.11, #running-req: 1018, #queue-req: 256, 
[2025-10-23 11:16:29] INFO:     127.0.0.1:50000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:52338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:53366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:53980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:58186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:58896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4318, token usage: 0.11, #running-req: 1018, #queue-req: 250, 
[2025-10-23 11:16:29] INFO:     127.0.0.1:50400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:51238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:53512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:56202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:58392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3710, token usage: 0.11, #running-req: 1019, #queue-req: 245, 
[2025-10-23 11:16:29] INFO:     127.0.0.1:51216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:52410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:53618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:54526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:54980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:57808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:58150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:58320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:58938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6638, token usage: 0.11, #running-req: 1015, #queue-req: 236, 
[2025-10-23 11:16:29] INFO:     127.0.0.1:53032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:57064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:57606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:57910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:59166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3659, token usage: 0.11, #running-req: 1019, #queue-req: 231, 
[2025-10-23 11:16:29] INFO:     127.0.0.1:50844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:50908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:51254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:51798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:52210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:57388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:57880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:29] INFO:     127.0.0.1:59042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5878, token usage: 0.11, #running-req: 1016, #queue-req: 223, 
[2025-10-23 11:16:30] INFO:     127.0.0.1:50736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:51000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:52002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:53502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:54676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:54762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:55126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:55824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:56596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:58744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:59074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 7968, token usage: 0.12, #running-req: 1013, #queue-req: 212, 
[2025-10-23 11:16:30] INFO:     127.0.0.1:50728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:50922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:51224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:52396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:54358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:55310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:55678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:56508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:58092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6442, token usage: 0.12, #running-req: 1015, #queue-req: 203, 
[2025-10-23 11:16:30] INFO:     127.0.0.1:51852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:51866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:51912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:52276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:52716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:53272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:54114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:55376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:56550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:56648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 8016, token usage: 0.12, #running-req: 1013, #queue-req: 192, 
[2025-10-23 11:16:30] INFO:     127.0.0.1:49986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:50648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:51486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:52034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:52118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:52884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:53776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:54284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:56080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:57238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:58198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 8020, token usage: 0.12, #running-req: 1013, #queue-req: 181, 
[2025-10-23 11:16:30] INFO:     127.0.0.1:51530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:52168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:52630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:54574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:54584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:55994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:56218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:56582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:57044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:30] INFO:     127.0.0.1:59104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7279, token usage: 0.12, #running-req: 1014, #queue-req: 171, 
[2025-10-23 11:16:31] INFO:     127.0.0.1:50454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31] INFO:     127.0.0.1:52138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31] INFO:     127.0.0.1:56392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31] INFO:     127.0.0.1:56710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31] INFO:     127.0.0.1:57148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31] INFO:     127.0.0.1:58470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4260, token usage: 0.12, #running-req: 1018, #queue-req: 165, 
[2025-10-23 11:16:31] INFO:     127.0.0.1:51546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31] INFO:     127.0.0.1:52748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31] INFO:     127.0.0.1:53558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31] INFO:     127.0.0.1:54690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31] INFO:     127.0.0.1:56704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31] INFO:     127.0.0.1:58056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4345, token usage: 0.12, #running-req: 1018, #queue-req: 159, 
[2025-10-23 11:16:31] INFO:     127.0.0.1:51298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31] INFO:     127.0.0.1:51878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31] INFO:     127.0.0.1:53136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31] INFO:     127.0.0.1:53516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31] INFO:     127.0.0.1:56064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31] INFO:     127.0.0.1:56180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31] INFO:     127.0.0.1:57504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5109, token usage: 0.12, #running-req: 1017, #queue-req: 152, 
[2025-10-23 11:16:31] INFO:     127.0.0.1:50130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31] INFO:     127.0.0.1:51810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31] INFO:     127.0.0.1:54728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31] INFO:     127.0.0.1:55748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31] INFO:     127.0.0.1:56306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31] INFO:     127.0.0.1:56842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31] INFO:     127.0.0.1:58870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:31 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5081, token usage: 0.12, #running-req: 1017, #queue-req: 145, 
[2025-10-23 11:16:32] INFO:     127.0.0.1:50756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:52542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:54934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:55756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:56890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3716, token usage: 0.12, #running-req: 1019, #queue-req: 140, 
[2025-10-23 11:16:32] INFO:     127.0.0.1:51390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:53180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:54560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:54660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:55196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:57392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:57660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:57988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:58168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:58432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:58674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 7931, token usage: 0.12, #running-req: 1013, #queue-req: 129, 
[2025-10-23 11:16:32] INFO:     127.0.0.1:51094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:51416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:51420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:52534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:53670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:55172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:55840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:57332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:57530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:57626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7488, token usage: 0.12, #running-req: 1014, #queue-req: 119, 
[2025-10-23 11:16:32] INFO:     127.0.0.1:50630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:50734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:50808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:51636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:51846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:54776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:57216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:57732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:57840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:58218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:58336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:58488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8779, token usage: 0.12, #running-req: 1012, #queue-req: 107, 
[2025-10-23 11:16:32] INFO:     127.0.0.1:51454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:52868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:53634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:55604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:56230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:56528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:58240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:58370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:32] INFO:     127.0.0.1:58588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6652, token usage: 0.12, #running-req: 1015, #queue-req: 98, 
[2025-10-23 11:16:33 TP0] Decode batch. #running-req: 1015, #token: 118979, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5354.00, #queue-req: 98, 
[2025-10-23 11:16:33] INFO:     127.0.0.1:50180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:50358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:51100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:52964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:53198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:53216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:53706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:54246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:54622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:58254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7245, token usage: 0.12, #running-req: 1014, #queue-req: 88, 
[2025-10-23 11:16:33] INFO:     127.0.0.1:51768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:52256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:52690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:53034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:53936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:54608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:55326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:55924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:58558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:59220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7232, token usage: 0.12, #running-req: 1014, #queue-req: 78, 
[2025-10-23 11:16:33] INFO:     127.0.0.1:51076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:51662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:52022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:52704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:52846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:52990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:55610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:55708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:58276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6659, token usage: 0.13, #running-req: 1015, #queue-req: 69, 
[2025-10-23 11:16:33] INFO:     127.0.0.1:51516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:52604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:52820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:53214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:53708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:54602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:55056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:55868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:55884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:55958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:57142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:58116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:58228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33 TP0] Prefill batch. #new-seq: 13, #new-token: 13, #cached-token: 9565, token usage: 0.13, #running-req: 1011, #queue-req: 56, 
[2025-10-23 11:16:33] INFO:     127.0.0.1:50410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:50770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:50826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:51336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:53564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:55806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:55906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:56174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:56182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:57078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:57100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:57260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:57414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:58042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:58100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:59300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:59304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:33] INFO:     127.0.0.1:59412 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:33 TP4] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:33 TP2] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:33 TP6] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:33 TP0] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:33 TP7] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:33 TP3] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:33 TP1] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:33 TP5] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:34 TP0] Prefill batch. #new-seq: 18, #new-token: 18, #cached-token: 13145, token usage: 0.13, #running-req: 1006, #queue-req: 38, 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:34 TP2] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:34 TP0] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:34 TP6] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:34 TP4] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:34 TP5] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:34 TP1] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:34 TP3] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:34 TP7] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:34] INFO:     127.0.0.1:51342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:51936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:52132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:52472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:54096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:54354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:54570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:55020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:55828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:56018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:56574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:57652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:58698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:59292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34 TP0] Prefill batch. #new-seq: 14, #new-token: 14, #cached-token: 10152, token usage: 0.13, #running-req: 1010, #queue-req: 24, 
[2025-10-23 11:16:34] INFO:     127.0.0.1:51442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:51526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:52518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:53296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:53528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:54888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:55230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:55520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:57312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:57596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:58360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:59120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8874, token usage: 0.13, #running-req: 1012, #queue-req: 12, 
[2025-10-23 11:16:34] INFO:     127.0.0.1:50334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:51564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:53284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:53542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:54130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:54202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:54846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:55658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:57130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:57456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 7926, token usage: 0.13, #running-req: 1013, #queue-req: 1, 
[2025-10-23 11:16:34] INFO:     127.0.0.1:51668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:52110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:53308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:53954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:56404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:56902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:56952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:57024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:58134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:59258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34] INFO:     127.0.0.1:59414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:34 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 716, token usage: 0.13, #running-req: 1013, #queue-req: 0, 
[2025-10-23 11:16:35] INFO:     127.0.0.1:51024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:51550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:52306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:53694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:54372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:56346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:58840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:50218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:51116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:51388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:51746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:51980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:52176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:52886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:53386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:55974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:56096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:56854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:58082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:58982 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:35 TP3] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:35 TP7] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:35 TP0] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:35 TP4] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:35 TP2] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:35 TP6] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:35 TP1] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:35 TP5] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:35] INFO:     127.0.0.1:51064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:54052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:54722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:58176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:58486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:59008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:59274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:50374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:50598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:51278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:52856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:53120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:54424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:54760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:55112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:55282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:56006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:56262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:57196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:57468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:57668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:50268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:50418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:50744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:50792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:53148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:54644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:56376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:57974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:58728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:50560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:51092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:51472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:53608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:55162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:55228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:55254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:58130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:58286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:52024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:52578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:54264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:54556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:54732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:57172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:57544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:57866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:57984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:58376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:58410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:58984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:35] INFO:     127.0.0.1:59400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:50248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:52246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:52356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:55018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:57156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:57400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:58330 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP4] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP2] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP6] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP7] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP0] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP3] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP1] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP5] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36] INFO:     127.0.0.1:50508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:50688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:51354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:51950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:55264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:55356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:55396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:56688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:56928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:58662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:59410 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP2] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP6] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP4] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP0] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP7] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP3] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP1] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP5] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36] INFO:     127.0.0.1:50056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:50234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:50488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:50538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:52050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:53594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:54248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:54628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:55790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:56456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:56992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:57670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:57794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:58416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:58692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:50044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:51694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:52668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:52778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:54090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:55400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:56600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:57244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:57784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:57964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:59232 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP2] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP4] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP6] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP1] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP0] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP7] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP3] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP5] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36] INFO:     127.0.0.1:50412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:50644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:50958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:51510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:52116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:52608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:53724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:53798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:56338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:58362 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP0] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP4] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP2] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP6] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP7] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP3] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP1] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP5] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36] INFO:     127.0.0.1:50520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:52084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:53210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:53500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:54464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:54832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:57368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:57428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:57754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:58190 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP0] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP4] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP2] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP6] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP1] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP7] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP3] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP5] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36] INFO:     127.0.0.1:51156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:53082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:53622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:53686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:53744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:54864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:55466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:56406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:57432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:57916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:58678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:58936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:58966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:59234 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP0] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP4] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP2] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP6] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP3] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP7] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP1] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP5] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36] INFO:     127.0.0.1:50146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:51570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:53440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:53968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:56108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:56664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:36] INFO:     127.0.0.1:59000 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP4] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP0] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP2] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP6] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP7] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP3] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP1] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:36 TP5] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:37] INFO:     127.0.0.1:50506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:50864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:51418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:53130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:53784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:54026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:55072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:55216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:55246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:55578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:55774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:56868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:57090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:58174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:58648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:50036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:50548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:50824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:51200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:51314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:55366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:56278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:56758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:58266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:58308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:59028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:59278 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:37 TP4] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:37 TP0] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:37 TP7] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:37 TP3] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:37 TP2] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:37 TP6] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:37 TP1] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:37 TP5] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:37] INFO:     127.0.0.1:50702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:52570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:52994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:53604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:56160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:57636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:58016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:58060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:54140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:54256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:55680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:56820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:56900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:58206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:58784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:59808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:54398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:54478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:54786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:54878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:55480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:55550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:56052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:56680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:57210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:58812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:50420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:50976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:53664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:54038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:55284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:56320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:57116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:57176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:58004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:60066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:50914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:51050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:51372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:55202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:55390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:58852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:59566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:50202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:50590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:53456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:54002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:54162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:54514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:54588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:56322 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:37 TP4] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:37 TP2] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:37 TP6] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:37 TP0] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:37 TP3] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:37 TP7] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:37 TP1] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:37 TP5] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:37] INFO:     127.0.0.1:51284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:51986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:52122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:52260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:52926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:53648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:53742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:54322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:54430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:54486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:54792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:55452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:56196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:57706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:58510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:58816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:37] INFO:     127.0.0.1:60514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:50878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:50942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:53354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:54456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:54502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:56714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:57290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:57320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:59022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:51114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:51150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:52194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:54074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:55636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:55870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:55946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:56702 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP0] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP4] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP5] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP2] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP6] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP1] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP7] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP3] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38] INFO:     127.0.0.1:50100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:50122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:51362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:52042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:52510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:55102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:56856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:58942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:59196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:50286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:50390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:53334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:53580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:53730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:54638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:54924 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP5] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP0] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP4] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP2] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP6] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP1] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP7] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP3] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38] INFO:     127.0.0.1:51826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:53996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:54572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:59150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:59994 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP5] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP0] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP4] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP2] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP6] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP1] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP3] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP7] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38] INFO:     127.0.0.1:52622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:52736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:53394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:53818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:54016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:54342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:54856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:55032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:55350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:56832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:56912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:57256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:57756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:58010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:59608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:60096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:60224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:60448 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP5] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP0] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP4] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP1] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP2] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP7] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP3] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP6] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38] INFO:     127.0.0.1:51708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:53100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:53922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:54022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:54522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:56492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:56598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:58346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:58592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:58696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:58838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:59364 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP5] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP4] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP7] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP6] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP0] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP1] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP2] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP3] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP0] Decode batch. #running-req: 705, #token: 100462, token usage: 0.10, cuda graph: False, gen throughput (token/s): 6118.56, #queue-req: 0, 
[2025-10-23 11:16:38] INFO:     127.0.0.1:50188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:50428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:51630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:52240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:52640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:53540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:53856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:54700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:55164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:55864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:56552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:57404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:57492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:57822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:57898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:38] INFO:     127.0.0.1:60150 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP4] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP5] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP0] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP2] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP6] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP1] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP7] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:38 TP3] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39] INFO:     127.0.0.1:50164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:52330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:53908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:55266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:56566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:56936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:57054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:57768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:57780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:59664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:60186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:60500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:60684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:33016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:33436 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP0] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP4] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP5] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP2] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP6] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP1] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP7] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP3] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39] INFO:     127.0.0.1:50482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:50720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:51502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:52224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:54184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:54812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:55368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:55506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:55668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:59178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:60828 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP0] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP4] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP5] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP1] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP7] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP3] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP2] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP6] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39] INFO:     127.0.0.1:50162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:50260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:51012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:52938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:53184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:53894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:54974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:56692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:57938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:58620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:59310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:50368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:50614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:50854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:51756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:52950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:52976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:53276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:53484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:55854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:56132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:56416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:57584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:58122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:60210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:51034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:51586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:52568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:54616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:54752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:55492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:56192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:56250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:56570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:56930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:58526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:58844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:59324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:60648 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP5] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP4] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP0] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP1] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP7] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP3] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP2] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP6] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39] INFO:     127.0.0.1:52908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:53330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:54058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:55532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:55540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:55704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:59948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:33564 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP5] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP4] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP0] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP1] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP7] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP3] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP2] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP6] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39] INFO:     127.0.0.1:52100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:52646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:56118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:58638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:58926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:32840 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP5] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP4] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP0] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP1] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP7] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP3] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP2] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP6] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39] INFO:     127.0.0.1:50300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:51248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:54034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:54496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:56366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:57180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:59478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:60134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:50104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:50884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:51926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:52786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:52798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:53616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:53740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:54448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:56782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:57274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:57306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:59058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:59134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:39] INFO:     127.0.0.1:59350 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP5] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP4] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP1] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP7] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP3] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP0] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP2] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:39 TP6] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40] INFO:     127.0.0.1:50972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:51898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:52078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:53050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:54268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:56826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:57226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:59228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:59336 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP4] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP5] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP1] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP7] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP3] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP0] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP2] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP6] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40] INFO:     127.0.0.1:50082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:50662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:52572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:57790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:59702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:59768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:59898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:60034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:60536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:33458 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP4] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP5] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP1] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP7] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP3] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP0] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP2] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP6] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40] INFO:     127.0.0.1:50492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:52436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:53760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:55768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:57118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:57738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:57968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:58282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:59596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:59668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:59928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:51838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:52658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:53220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:54380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:54806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:55936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:56874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:57886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:60206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:60236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:60294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:32844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:33044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:53318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:54444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:56050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:56542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:57440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:58824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:58908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:60464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:60654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:32976 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP5] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP4] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP1] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP7] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP3] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP0] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP2] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40 TP6] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 11:16:40] INFO:     127.0.0.1:50708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:53226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:56292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:57346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:58414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:58922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:59758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:60252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:60758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:60916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:50674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:52560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:56978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:58400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:60154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:60512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:60598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:50094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:50900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:51962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:52762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:52810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:53940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:55182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:55342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:55850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:56484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:56624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:57854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:58712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:59248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:60896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:52352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:55098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:56234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:57690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:58444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:58906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:33430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:54390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:56564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:57380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:57612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:59270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:59978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:60296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:60392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:52160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:52580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:54186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:57976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:58542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:60112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:33090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:53242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:53802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:54854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:55978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:56356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:40] INFO:     127.0.0.1:32832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:51944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:54310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:54738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:56286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:59786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:60050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:60320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:60560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:60858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:50004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:50054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:53026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:54904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:56920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:57830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:57944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:57958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:58204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:58334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:59394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:60138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:60280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:54260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:59598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:55150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:57874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:59378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:59532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:60632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:60926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:53402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:54908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:58954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:33236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:51470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:53342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:54138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:54966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:55644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:55944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:59680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:33362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:33506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:33680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:51328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:33220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:33572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:33696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:53518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:59440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:59698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:59724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:60566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:60832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:60870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:32962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:33194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:50994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:54318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:55038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:55732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:32944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:33100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:33666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:52318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:52838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:54960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:55848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:59096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:60380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:53470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:53778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:55462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:55624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:55902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:56034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:56440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:56794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:59420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:33196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:52018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:53888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:57254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:57496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:58070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:60148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:33062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:33072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:51886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:53000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:56428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:56804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:57060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:57722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:58502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:58872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:60230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:60354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:33006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:33076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:33404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:50178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:52464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:57434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:59540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:59654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:60178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:60484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:32904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:33580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:55296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:55426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:57708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:59674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:60310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:32934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:33654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:52488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:57022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:60078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:60192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:60346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:41] INFO:     127.0.0.1:33338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:51784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:57034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:59494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:59960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:32826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:32852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:33438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:50238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:51138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:52062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:53944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:58748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:59584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:60024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:33172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:33416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42 TP0] Decode batch. #running-req: 329, #token: 59292, token usage: 0.06, cuda graph: True, gen throughput (token/s): 6071.65, #queue-req: 0, 
[2025-10-23 11:16:42] INFO:     127.0.0.1:51712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:54396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:58066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:32776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:54152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:59712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:59876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:60292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:60524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:60544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:60812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:32816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:50926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:51172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:53010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:58140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:58608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:32964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:33764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:51122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:52426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:59612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:59824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:32974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:33304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:33700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:52676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:53646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:55046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:55432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:59986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:60784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:50406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:51482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:52894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:53256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:53548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:53840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:58866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:59688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:33794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:58768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:59002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:60706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:52372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:53924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:59742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:59794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:33490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:50832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:52012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:52152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:54294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:56512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:56616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:59452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:59852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:33522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:56472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:59370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:59710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:33050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:33732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:50392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:51602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:59482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:60174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:33720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:32882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:33406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:33618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:52378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:57520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:60980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:53748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:56152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:58986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:60582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:33384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:33576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:33624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:42] INFO:     127.0.0.1:33756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:53858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:55834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:57482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:58634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:60846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:32804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:33712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:52590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:53108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:54336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:60994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:33134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:33204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:50068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:53424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:55566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:50562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:54748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:55358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:57006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:59302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:59736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:33174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:33358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:33284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:33370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:52548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:59432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:51616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:60366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:60474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:33396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:50284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:52600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:56632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:59756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:33534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:59606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:59776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:60494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:60716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:60746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:33028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:57436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:33208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:33474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:50108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:52290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:54412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:55590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:55910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:32950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:55646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:57072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:59944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:60734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:50384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:52452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:56772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:60404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:33524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:51432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:54106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:55690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:59618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:59912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:60334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:60438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:33586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:53370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:59886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:60798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:60816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:32896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:54944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:55040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:55438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:60934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:51720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:33272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:33276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:58114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:59180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:60482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:50386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:52504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:55312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:60006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:60938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:51534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:52030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:58460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:59892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:33118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:43] INFO:     127.0.0.1:33638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:52720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:59802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:33446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:33502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:50232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:56744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:60712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:59760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:59778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:60020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:33120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:60738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:50786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:53126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:59980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:60612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:32936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44 TP0] Decode batch. #running-req: 130, #token: 29665, token usage: 0.03, cuda graph: True, gen throughput (token/s): 4254.07, #queue-req: 0, 
[2025-10-23 11:16:44] INFO:     127.0.0.1:58224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:59640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:33588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:51400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:58574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:60422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:55814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:56964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:59636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:33054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:59800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:33140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:33348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:58162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:55354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:60444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:60910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:50836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:60728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:60638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:56422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:58292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:60884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:33554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:33710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:33746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:53806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:53832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:54232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:60666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:55928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:32996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:33630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:51188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:33354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:33596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:50828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:51268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:59574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:56142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:55140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:57036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:58714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:33496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:52830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:59672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:44] INFO:     127.0.0.1:60328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:54226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:33250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:54006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:54830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:32928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:59632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:59966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:60674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:57424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:60160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:33260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:57700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:33156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:50888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:60128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:60416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:59516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:33678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:33780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:60740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:56608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:57560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:57576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:58516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:32986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:54524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:59088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:60220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:33318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:33610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45 TP0] Decode batch. #running-req: 49, #token: 13731, token usage: 0.01, cuda graph: True, gen throughput (token/s): 2148.14, #queue-req: 0, 
[2025-10-23 11:16:45] INFO:     127.0.0.1:33178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:33300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:60826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:32788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:50440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:33042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:59836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:45] INFO:     127.0.0.1:60770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:57352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:53410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:56712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:59468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:60878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:52878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:54168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:55412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:60948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:33154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:51274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:33108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:60618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:33644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:57280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:52968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:33548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:60268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:60964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:59952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:57240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46 TP0] Decode batch. #running-req: 19, #token: 6512, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1085.62, #queue-req: 0, 
[2025-10-23 11:16:46] INFO:     127.0.0.1:60696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:60092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:46] INFO:     127.0.0.1:53872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:47] INFO:     127.0.0.1:51646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:47] INFO:     127.0.0.1:33382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:47] INFO:     127.0.0.1:32868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:47] INFO:     127.0.0.1:60026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:47] INFO:     127.0.0.1:33322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:47] INFO:     127.0.0.1:59868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:47] INFO:     127.0.0.1:32916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:47] INFO:     127.0.0.1:59500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:47] INFO:     127.0.0.1:60676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:47] INFO:     127.0.0.1:59212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:47] INFO:     127.0.0.1:59408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:47] INFO:     127.0.0.1:59552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:47 TP0] Decode batch. #running-req: 4, #token: 2056, token usage: 0.00, cuda graph: True, gen throughput (token/s): 428.06, #queue-req: 0, 
[2025-10-23 11:16:47] INFO:     127.0.0.1:60326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:47] INFO:     127.0.0.1:53164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:48] INFO:     127.0.0.1:60804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:48 TP0] Decode batch. #running-req: 1, #token: 1099, token usage: 0.00, cuda graph: True, gen throughput (token/s): 102.07, #queue-req: 0, 
[2025-10-23 11:16:49 TP0] Decode batch. #running-req: 1, #token: 1139, token usage: 0.00, cuda graph: True, gen throughput (token/s): 56.33, #queue-req: 0, 
[2025-10-23 11:16:49 TP0] Decode batch. #running-req: 1, #token: 1179, token usage: 0.00, cuda graph: True, gen throughput (token/s): 56.33, #queue-req: 0, 
[2025-10-23 11:16:50 TP0] Decode batch. #running-req: 1, #token: 1219, token usage: 0.00, cuda graph: True, gen throughput (token/s): 56.31, #queue-req: 0, 
[2025-10-23 11:16:51 TP0] Decode batch. #running-req: 1, #token: 1259, token usage: 0.00, cuda graph: True, gen throughput (token/s): 56.32, #queue-req: 0, 
[2025-10-23 11:16:51] INFO:     127.0.0.1:51974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 11:16:56] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-23 11:16:59] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
