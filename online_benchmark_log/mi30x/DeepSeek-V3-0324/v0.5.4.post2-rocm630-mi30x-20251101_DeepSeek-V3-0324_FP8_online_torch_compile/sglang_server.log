INFO 11-01 10:50:42 __init__.py:179] Automatically detected platform rocm.
WARNING 11-01 10:50:42 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-01 10:50:42] WARNING server_args.py:1153: Attention backend not explicitly specified. Use aiter backend by default.
[2025-11-01 10:50:42] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-01 10:50:42] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.765, max_running_requests=1024, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=509898282, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=-1, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0)
[2025-11-01 10:50:43] Using default HuggingFace chat template with detected content format: string
INFO 11-01 10:50:52 __init__.py:179] Automatically detected platform rocm.
INFO 11-01 10:50:52 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-01 10:50:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-01 10:50:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-01 10:50:53 TP7] Process 290 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
[2025-11-01 10:50:53 TP4] Process 287 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
INFO 11-01 10:50:53 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
INFO 11-01 10:50:53 __init__.py:179] Automatically detected platform rocm.
INFO 11-01 10:50:53 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-01 10:50:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
INFO 11-01 10:50:53 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-11-01 10:50:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-01 10:50:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-01 10:50:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-01 10:50:53 TP5] Process 288 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
[2025-11-01 10:50:53 TP2] Process 285 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
[2025-11-01 10:50:53 TP7] Init torch distributed begin.
[2025-11-01 10:50:53 TP4] Init torch distributed begin.
[2025-11-01 10:50:53 TP1] Process 284 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
INFO 11-01 10:50:53 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
INFO 11-01 10:50:53 __init__.py:179] Automatically detected platform rocm.
INFO 11-01 10:50:53 __init__.py:179] Automatically detected platform rocm.
[2025-11-01 10:50:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-01 10:50:53 TP5] Init torch distributed begin.
[2025-11-01 10:50:53 TP2] Init torch distributed begin.
[2025-11-01 10:50:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-01 10:50:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-01 10:50:53 TP0] Process 283 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2025-11-01 10:50:53 TP1] Init torch distributed begin.
[2025-11-01 10:50:53 TP3] Process 286 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
[2025-11-01 10:50:54 TP6] Process 289 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
[2025-11-01 10:50:54 TP0] Init torch distributed begin.
[2025-11-01 10:50:54 TP3] Init torch distributed begin.
[2025-11-01 10:50:54 TP6] Init torch distributed begin.
[2025-11-01 10:50:54 TP0] sglang is using nccl==2.21.5
[2025-11-01 10:50:56 TP0] Init torch distributed ends. mem usage=3.65 GB
[2025-11-01 10:50:56 TP6] Init torch distributed ends. mem usage=3.95 GB
[2025-11-01 10:50:56 TP4] Init torch distributed ends. mem usage=4.01 GB
[2025-11-01 10:50:56 TP7] Init torch distributed ends. mem usage=3.94 GB
[2025-11-01 10:50:56 TP5] Init torch distributed ends. mem usage=3.93 GB
[2025-11-01 10:50:56 TP1] Init torch distributed ends. mem usage=4.07 GB
[2025-11-01 10:50:56 TP2] Init torch distributed ends. mem usage=4.07 GB
[2025-11-01 10:50:56 TP3] Init torch distributed ends. mem usage=4.06 GB
[2025-11-01 10:50:58 TP5] Load weight begin. avail mem=187.33 GB
[2025-11-01 10:50:58 TP0] Load weight begin. avail mem=187.61 GB
[2025-11-01 10:50:58 TP0] Detected fp8 checkpoint.
[2025-11-01 10:50:58 TP0] Only Deepseek V3/R1 on NV-platform with capability >= 80 can use shared experts fusion optimization. Shared experts fusion optimization is disabled.
[2025-11-01 10:50:58 TP3] Load weight begin. avail mem=187.20 GB
[2025-11-01 10:50:58 TP1] Load weight begin. avail mem=187.19 GB
[2025-11-01 10:50:58 TP7] Load weight begin. avail mem=187.32 GB
[2025-11-01 10:50:58 TP4] Load weight begin. avail mem=187.25 GB
[2025-11-01 10:50:58 TP2] Load weight begin. avail mem=187.19 GB
[2025-11-01 10:50:58 TP6] Load weight begin. avail mem=187.31 GB
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<01:25,  1.89it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:28,  5.69it/s]
Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:00<00:17,  8.93it/s]
Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:00<00:12, 12.79it/s]
Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:01<00:21,  6.99it/s]
Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:01<00:17,  8.79it/s]
Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:01<00:12, 11.55it/s]
Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:01<00:10, 13.61it/s]
Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:01<00:09, 14.72it/s]
Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:02<00:08, 16.52it/s]
Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:02<00:07, 18.06it/s]
Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:02<00:06, 19.71it/s]
Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:02<00:05, 22.10it/s]
Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:03<00:13,  9.50it/s]
Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:03<00:11, 11.19it/s]
Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:03<00:09, 12.78it/s]
Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:03<00:09, 12.93it/s]
Loading safetensors checkpoint shards:  28% Completed | 46/163 [00:03<00:09, 12.69it/s]
Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:03<00:07, 15.20it/s]
Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:04<00:07, 15.85it/s]
Loading safetensors checkpoint shards:  33% Completed | 53/163 [00:04<00:07, 14.52it/s]
Loading safetensors checkpoint shards:  34% Completed | 56/163 [00:04<00:06, 17.65it/s]
Loading safetensors checkpoint shards:  36% Completed | 59/163 [00:04<00:05, 18.79it/s]
Loading safetensors checkpoint shards:  38% Completed | 62/163 [00:04<00:06, 16.65it/s]
Loading safetensors checkpoint shards:  40% Completed | 65/163 [00:04<00:05, 18.72it/s]
Loading safetensors checkpoint shards:  42% Completed | 68/163 [00:05<00:10,  9.42it/s]
Loading safetensors checkpoint shards:  44% Completed | 71/163 [00:05<00:08, 11.32it/s]
Loading safetensors checkpoint shards:  45% Completed | 73/163 [00:05<00:07, 12.25it/s]
Loading safetensors checkpoint shards:  46% Completed | 75/163 [00:05<00:06, 13.45it/s]
Loading safetensors checkpoint shards:  48% Completed | 78/163 [00:05<00:05, 16.04it/s]
Loading safetensors checkpoint shards:  50% Completed | 81/163 [00:06<00:05, 15.41it/s]
Loading safetensors checkpoint shards:  52% Completed | 84/163 [00:06<00:04, 16.35it/s]
Loading safetensors checkpoint shards:  53% Completed | 86/163 [00:06<00:06, 12.83it/s]
Loading safetensors checkpoint shards:  55% Completed | 89/163 [00:06<00:04, 14.82it/s]
Loading safetensors checkpoint shards:  56% Completed | 92/163 [00:06<00:04, 17.16it/s]
Loading safetensors checkpoint shards:  58% Completed | 94/163 [00:07<00:04, 15.15it/s]
Loading safetensors checkpoint shards:  59% Completed | 96/163 [00:07<00:04, 15.85it/s]
Loading safetensors checkpoint shards:  60% Completed | 98/163 [00:07<00:04, 15.90it/s]
Loading safetensors checkpoint shards:  61% Completed | 100/163 [00:07<00:03, 16.28it/s]
Loading safetensors checkpoint shards:  63% Completed | 102/163 [00:07<00:04, 12.83it/s]
Loading safetensors checkpoint shards:  64% Completed | 105/163 [00:07<00:03, 15.82it/s]
Loading safetensors checkpoint shards:  66% Completed | 107/163 [00:08<00:08,  6.63it/s]
Loading safetensors checkpoint shards:  67% Completed | 109/163 [00:08<00:06,  7.94it/s]
Loading safetensors checkpoint shards:  68% Completed | 111/163 [00:08<00:05,  9.54it/s]
Loading safetensors checkpoint shards:  70% Completed | 114/163 [00:08<00:04, 11.88it/s]
Loading safetensors checkpoint shards:  71% Completed | 116/163 [00:09<00:03, 11.83it/s]
Loading safetensors checkpoint shards:  73% Completed | 119/163 [00:09<00:02, 14.89it/s]
Loading safetensors checkpoint shards:  75% Completed | 122/163 [00:09<00:02, 17.08it/s]
Loading safetensors checkpoint shards:  77% Completed | 125/163 [00:09<00:01, 19.41it/s]
Loading safetensors checkpoint shards:  79% Completed | 128/163 [00:09<00:01, 21.29it/s]
Loading safetensors checkpoint shards:  80% Completed | 131/163 [00:09<00:01, 21.82it/s]
Loading safetensors checkpoint shards:  83% Completed | 135/163 [00:09<00:01, 23.64it/s]
Loading safetensors checkpoint shards:  85% Completed | 138/163 [00:09<00:01, 23.16it/s]
Loading safetensors checkpoint shards:  87% Completed | 141/163 [00:10<00:00, 22.31it/s]
Loading safetensors checkpoint shards:  88% Completed | 144/163 [00:10<00:01, 17.01it/s]
Loading safetensors checkpoint shards:  90% Completed | 147/163 [00:10<00:00, 17.90it/s]
Loading safetensors checkpoint shards:  92% Completed | 150/163 [00:10<00:00, 18.50it/s]
Loading safetensors checkpoint shards:  94% Completed | 153/163 [00:10<00:00, 19.20it/s]
Loading safetensors checkpoint shards:  96% Completed | 156/163 [00:11<00:00, 18.90it/s]
Loading safetensors checkpoint shards:  97% Completed | 158/163 [00:12<00:00,  6.59it/s]
Loading safetensors checkpoint shards:  98% Completed | 160/163 [00:12<00:00,  7.84it/s]
Loading safetensors checkpoint shards:  99% Completed | 162/163 [00:12<00:00,  9.24it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:12<00:00, 13.31it/s]

[2025-11-01 10:51:47 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.63 GB, mem usage=79.56 GB.
[2025-11-01 10:51:47 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.64 GB, mem usage=79.56 GB.
[2025-11-01 10:51:47 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.63 GB, mem usage=79.56 GB.
[2025-11-01 10:51:48 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.05 GB, mem usage=79.56 GB.
[2025-11-01 10:51:57 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.78 GB, mem usage=79.56 GB.
[2025-11-01 10:51:57 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.69 GB, mem usage=79.56 GB.
[2025-11-01 10:51:57 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.76 GB, mem usage=79.56 GB.
[2025-11-01 10:51:58 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.77 GB, mem usage=79.56 GB.
[2025-11-01 10:51:58 TP0] Using KV cache dtype: torch.bfloat16
[2025-11-01 10:51:58 TP2] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-01 10:51:58 TP2] Memory pool end. avail mem=43.36 GB
[2025-11-01 10:51:58 TP0] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-01 10:51:58 TP0] Memory pool end. avail mem=43.78 GB
[2025-11-01 10:51:58 TP6] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-01 10:51:58 TP5] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-01 10:51:58 TP6] Memory pool end. avail mem=43.49 GB
[2025-11-01 10:51:58 TP5] Memory pool end. avail mem=43.51 GB
[2025-11-01 10:51:58 TP3] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-01 10:51:58 TP3] Memory pool end. avail mem=43.37 GB
[2025-11-01 10:51:58 TP4] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-01 10:51:58 TP1] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-01 10:51:58 TP1] Memory pool end. avail mem=43.37 GB
[2025-11-01 10:51:58 TP4] Memory pool end. avail mem=43.42 GB
[2025-11-01 10:51:58 TP7] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-01 10:51:58 TP7] Memory pool end. avail mem=43.50 GB
[2025-11-01 10:52:00 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=43.16 GB
[2025-11-01 10:52:00 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=43.16 GB
[2025-11-01 10:52:00 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=43.30 GB
[2025-11-01 10:52:00 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=43.17 GB
[2025-11-01 10:52:00 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=43.22 GB
[2025-11-01 10:52:00 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=43.29 GB
[2025-11-01 10:52:00 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=43.58 GB
[2025-11-01 10:52:00 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=42.94 GB):   0%|          | 0/52 [00:00<?, ?it/s][2025-11-01 10:52:01 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=43.29 GB
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-01 10:52:03 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-01 10:52:03 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-01 10:52:03 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-01 10:52:03 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-01 10:52:03 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-01 10:52:03 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-01 10:52:03 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-01 10:52:03 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:03 TP2] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:03 TP5] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:03 TP7] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:03 TP6] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:03 TP1] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:03 TP0] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:03 TP3] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:03 TP4] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-01 10:52:05 TP2] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-01 10:52:05 TP5] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-01 10:52:05 TP1] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-01 10:52:05 TP7] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-01 10:52:05 TP3] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-01 10:52:05 TP4] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP2] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP2] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP2] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP2] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP2] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:05 TP2] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP5] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP5] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP5] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP5] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP5] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:05 TP5] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-01 10:52:05 TP0] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP1] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP1] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP1] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP1] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP1] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:05 TP1] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP7] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP7] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP7] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP7] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP7] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP4] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP4] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP4] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP4] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP4] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:05 TP7] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP3] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP3] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[2025-11-01 10:52:05 TP3] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP3] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:05 TP3] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:05 TP4] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:05 TP3] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:06 TP0] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:06 TP0] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:06 TP0] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:06 TP0] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:06 TP0] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:06 TP0] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-01 10:52:06 TP6] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:06 TP6] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:06 TP6] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:06 TP6] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:06 TP6] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:06 TP6] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:06 TP6] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
Capturing batches (bs=512 avail_mem=42.94 GB):   2%|         | 1/52 [00:06<05:25,  6.38s/it]Capturing batches (bs=496 avail_mem=42.27 GB):   2%|         | 1/52 [00:06<05:25,  6.38s/it][aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:08 TP5] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:08 TP2] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:08 TP1] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:08 TP6] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:08 TP7] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:08 TP0] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:08 TP3] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:08 TP4] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=496 avail_mem=42.27 GB):   4%|         | 2/52 [00:07<02:58,  3.57s/it]Capturing batches (bs=480 avail_mem=42.25 GB):   4%|         | 2/52 [00:07<02:58,  3.57s/it][aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:08 TP1] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:08 TP0] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:08 TP2] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:08 TP3] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:08 TP4] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:08 TP5] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:08 TP6] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:08 TP7] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=480 avail_mem=42.25 GB):   6%|         | 3/52 [00:08<01:44,  2.13s/it]Capturing batches (bs=464 avail_mem=42.25 GB):   6%|         | 3/52 [00:08<01:44,  2.13s/it][aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:09 TP7] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:09 TP2] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:09 TP6] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:09 TP5] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:09 TP1] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:09 TP3] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:09 TP0] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:09 TP4] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=464 avail_mem=42.25 GB):   8%|         | 4/52 [00:08<01:09,  1.45s/it]Capturing batches (bs=448 avail_mem=42.24 GB):   8%|         | 4/52 [00:08<01:09,  1.45s/it][aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:09 TP3] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:09 TP2] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:09 TP0] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:09 TP6] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:09 TP7] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:09 TP1] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:09 TP4] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:09 TP5] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=448 avail_mem=42.24 GB):  10%|         | 5/52 [00:09<00:50,  1.08s/it]Capturing batches (bs=432 avail_mem=42.24 GB):  10%|         | 5/52 [00:09<00:50,  1.08s/it][aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP3] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP7] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP2] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP1] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP5] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP0] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP6] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP4] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=432 avail_mem=42.24 GB):  12%|        | 6/52 [00:09<00:39,  1.17it/s]Capturing batches (bs=416 avail_mem=42.23 GB):  12%|        | 6/52 [00:09<00:39,  1.17it/s][aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP6] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP7] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP5] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP4] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP2] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP0] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP1] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP3] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=416 avail_mem=42.23 GB):  13%|        | 7/52 [00:10<00:32,  1.40it/s]Capturing batches (bs=400 avail_mem=42.23 GB):  13%|        | 7/52 [00:10<00:32,  1.40it/s][aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP1] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP0] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP2] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP7] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP3] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP5] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP6] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:10 TP4] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=400 avail_mem=42.23 GB):  15%|        | 8/52 [00:10<00:27,  1.61it/s]Capturing batches (bs=384 avail_mem=42.22 GB):  15%|        | 8/52 [00:10<00:27,  1.61it/s][aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:11 TP5] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:11 TP6] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:11 TP4] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:11 TP2] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:11 TP1] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:11 TP3] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:11 TP0] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:11 TP7] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=384 avail_mem=42.22 GB):  17%|        | 9/52 [00:10<00:22,  1.95it/s]Capturing batches (bs=368 avail_mem=42.22 GB):  17%|        | 9/52 [00:10<00:22,  1.95it/s][aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:11 TP7] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:11 TP6] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:11 TP5] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:11 TP0] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:11 TP3] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:11 TP4] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:11 TP2] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:11 TP1] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=368 avail_mem=42.22 GB):  19%|        | 10/52 [00:11<00:20,  2.07it/s]Capturing batches (bs=352 avail_mem=42.22 GB):  19%|        | 10/52 [00:11<00:20,  2.07it/s][aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP1] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP2] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP0] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP4] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP7] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP3] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP5] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP6] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=352 avail_mem=42.22 GB):  21%|        | 11/52 [00:11<00:19,  2.14it/s]Capturing batches (bs=336 avail_mem=42.21 GB):  21%|        | 11/52 [00:11<00:19,  2.14it/s][aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP1] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP2] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP0] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP5] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP3] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP4] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP7] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP6] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=336 avail_mem=42.21 GB):  23%|       | 12/52 [00:12<00:18,  2.21it/s]Capturing batches (bs=320 avail_mem=42.21 GB):  23%|       | 12/52 [00:12<00:18,  2.21it/s][aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP5] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP4] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP6] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP0] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP2] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP1] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP3] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:12 TP7] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=320 avail_mem=42.21 GB):  25%|       | 13/52 [00:12<00:15,  2.52it/s]Capturing batches (bs=304 avail_mem=42.20 GB):  25%|       | 13/52 [00:12<00:15,  2.52it/s][aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP5] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP7] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP6] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP1] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP3] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP4] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP0] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP2] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=304 avail_mem=42.20 GB):  27%|       | 14/52 [00:12<00:15,  2.48it/s]Capturing batches (bs=288 avail_mem=42.20 GB):  27%|       | 14/52 [00:12<00:15,  2.48it/s][aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP6] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP5] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP2] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP4] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP1] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP7] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP0] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP3] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=288 avail_mem=42.20 GB):  29%|       | 15/52 [00:12<00:13,  2.76it/s]Capturing batches (bs=272 avail_mem=42.20 GB):  29%|       | 15/52 [00:12<00:13,  2.76it/s][aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP3] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP2] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP5] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP4] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP0] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP1] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP7] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:13 TP6] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=272 avail_mem=42.20 GB):  31%|       | 16/52 [00:13<00:13,  2.63it/s]Capturing batches (bs=256 avail_mem=42.19 GB):  31%|       | 16/52 [00:13<00:13,  2.63it/s][aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP5] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP6] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP7] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP0] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP3] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP1] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP2] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP4] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP2] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP2] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP2] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP0] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP0] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP2] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP0] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:14 TP2] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:14 TP2] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP0] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP1] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP1] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:14 TP0] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP1] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:14 TP0] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP1] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP3] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:14 TP1] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP3] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP3] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:14 TP1] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP3] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:14 TP3] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP5] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:14 TP3] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP5] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP5] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP5] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:14 TP5] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:14 TP5] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP7] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP7] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP7] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP7] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:14 TP7] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP4] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP4] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:14 TP7] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP4] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP4] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:14 TP4] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:14 TP4] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP6] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP6] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP6] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:14 TP6] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:14 TP6] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:14 TP6] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=256 avail_mem=42.19 GB):  33%|      | 17/52 [00:13<00:13,  2.55it/s]Capturing batches (bs=248 avail_mem=42.18 GB):  33%|      | 17/52 [00:13<00:13,  2.55it/s][aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:14 TP1] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:14 TP0] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:14 TP4] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:14 TP7] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:14 TP3] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:14 TP5] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:14 TP2] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:14 TP6] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=248 avail_mem=42.18 GB):  35%|      | 18/52 [00:14<00:13,  2.50it/s]Capturing batches (bs=240 avail_mem=42.18 GB):  35%|      | 18/52 [00:14<00:13,  2.50it/s][aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP7] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP5] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP3] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP6] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP1] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP0] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP4] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP2] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=240 avail_mem=42.18 GB):  37%|      | 19/52 [00:14<00:13,  2.45it/s]Capturing batches (bs=232 avail_mem=42.17 GB):  37%|      | 19/52 [00:14<00:13,  2.45it/s][aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP0] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP3] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP1] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP6] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP4] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP5] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP7] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP2] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=232 avail_mem=42.17 GB):  38%|      | 20/52 [00:15<00:13,  2.43it/s]Capturing batches (bs=224 avail_mem=42.17 GB):  38%|      | 20/52 [00:15<00:13,  2.43it/s][aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP7] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP5] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP2] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP1] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP6] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP3] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:15 TP0] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:16 TP4] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=224 avail_mem=42.17 GB):  40%|      | 21/52 [00:15<00:12,  2.41it/s]Capturing batches (bs=216 avail_mem=42.16 GB):  40%|      | 21/52 [00:15<00:12,  2.41it/s][aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:16 TP1] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:16 TP4] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:16 TP2] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:16 TP0] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:16 TP3] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:16 TP5] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:16 TP6] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:16 TP7] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=216 avail_mem=42.16 GB):  42%|     | 22/52 [00:15<00:12,  2.40it/s]Capturing batches (bs=208 avail_mem=42.16 GB):  42%|     | 22/52 [00:15<00:12,  2.40it/s][aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:16 TP6] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:16 TP5] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:16 TP1] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:16 TP2] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:16 TP4] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:16 TP3] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:16 TP0] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:16 TP7] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=208 avail_mem=42.16 GB):  44%|     | 23/52 [00:16<00:10,  2.68it/s]Capturing batches (bs=200 avail_mem=42.15 GB):  44%|     | 23/52 [00:16<00:10,  2.68it/s][aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP5] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP4] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP6] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP1] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP2] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP0] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP3] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP7] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=200 avail_mem=42.15 GB):  46%|     | 24/52 [00:16<00:10,  2.58it/s]Capturing batches (bs=192 avail_mem=42.15 GB):  46%|     | 24/52 [00:16<00:10,  2.58it/s][aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP1] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP2] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP5] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP0] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP6] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP4] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP3] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP7] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=192 avail_mem=42.15 GB):  48%|     | 25/52 [00:16<00:09,  2.77it/s]Capturing batches (bs=184 avail_mem=42.15 GB):  48%|     | 25/52 [00:16<00:09,  2.77it/s][aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP2] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP1] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP0] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP3] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP5] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP6] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP4] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:17 TP7] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=184 avail_mem=42.15 GB):  50%|     | 26/52 [00:17<00:11,  2.22it/s]Capturing batches (bs=176 avail_mem=42.14 GB):  50%|     | 26/52 [00:17<00:11,  2.22it/s][aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:18 TP2] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:18 TP0] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:18 TP1] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:18 TP3] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:18 TP5] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:18 TP6] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:18 TP4] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:18 TP7] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=176 avail_mem=42.14 GB):  52%|    | 27/52 [00:17<00:09,  2.52it/s]Capturing batches (bs=168 avail_mem=42.14 GB):  52%|    | 27/52 [00:17<00:09,  2.52it/s][aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:18 TP6] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:18 TP5] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:18 TP7] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:18 TP1] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:18 TP0] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:18 TP3] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:18 TP2] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:18 TP4] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=168 avail_mem=42.14 GB):  54%|    | 28/52 [00:18<00:09,  2.47it/s]Capturing batches (bs=160 avail_mem=42.14 GB):  54%|    | 28/52 [00:18<00:09,  2.47it/s][aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP1] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP5] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP2] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP0] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP6] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP3] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP4] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP7] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=160 avail_mem=42.14 GB):  56%|    | 29/52 [00:18<00:08,  2.74it/s]Capturing batches (bs=152 avail_mem=42.13 GB):  56%|    | 29/52 [00:18<00:08,  2.74it/s][aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP0] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP6] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP5] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP7] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP1] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP2] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP4] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP3] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=152 avail_mem=42.13 GB):  58%|    | 30/52 [00:18<00:08,  2.63it/s]Capturing batches (bs=144 avail_mem=42.13 GB):  58%|    | 30/52 [00:18<00:08,  2.63it/s][aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP5] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP2] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP1] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP6] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP3] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP7] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP0] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:19 TP4] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=144 avail_mem=42.13 GB):  60%|    | 31/52 [00:19<00:07,  2.88it/s]Capturing batches (bs=136 avail_mem=42.12 GB):  60%|    | 31/52 [00:19<00:07,  2.88it/s][aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP5] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP6] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP2] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP1] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP3] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP7] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP4] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP0] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=136 avail_mem=42.12 GB):  62%|   | 32/52 [00:19<00:06,  3.06it/s]Capturing batches (bs=128 avail_mem=42.12 GB):  62%|   | 32/52 [00:19<00:06,  3.06it/s][aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-01 10:52:20 TP5] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-01 10:52:20 TP6] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-01 10:52:20 TP7] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-01 10:52:20 TP2] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-01 10:52:20 TP0] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-01 10:52:20 TP3] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-01 10:52:20 TP1] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-01 10:52:20 TP4] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP5] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP6] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP7] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP0] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP2] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP3] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP1] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP4] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP5] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP6] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP7] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP0] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP2] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP3] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP1] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP5] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP6] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP4] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP7] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP0] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP3] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP2] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP1] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP4] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP6] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP5] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP0] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP7] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP1] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:20 TP2] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:20 TP4] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:20 TP6] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:20 TP5] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:20 TP3] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:20 TP0] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:20 TP7] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:20 TP1] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:20 TP2] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:20 TP4] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:20 TP3] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP6] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP5] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP0] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP7] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP1] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP2] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP3] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP4] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=128 avail_mem=42.12 GB):  63%|   | 33/52 [00:19<00:05,  3.20it/s]Capturing batches (bs=120 avail_mem=42.12 GB):  63%|   | 33/52 [00:19<00:05,  3.20it/s][aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP4] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP7] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP2] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP6] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP5] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP3] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP1] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP0] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=120 avail_mem=42.12 GB):  65%|   | 34/52 [00:20<00:06,  2.90it/s]Capturing batches (bs=112 avail_mem=42.12 GB):  65%|   | 34/52 [00:20<00:06,  2.90it/s][aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP5] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP6] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP1] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP2] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP3] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP4] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP0] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:20 TP7] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=112 avail_mem=42.12 GB):  67%|   | 35/52 [00:20<00:05,  3.10it/s]Capturing batches (bs=104 avail_mem=42.11 GB):  67%|   | 35/52 [00:20<00:05,  3.10it/s][aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:21 TP4] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:21 TP5] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:21 TP3] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:21 TP1] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:21 TP7] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:21 TP6] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:21 TP0] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:21 TP2] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=104 avail_mem=42.11 GB):  69%|   | 36/52 [00:20<00:05,  2.84it/s]Capturing batches (bs=96 avail_mem=42.11 GB):  69%|   | 36/52 [00:20<00:05,  2.84it/s] [aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:21 TP5] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:21 TP6] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:21 TP4] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:21 TP2] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:21 TP1] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:21 TP0] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:21 TP7] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:21 TP3] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=96 avail_mem=42.11 GB):  71%|   | 37/52 [00:21<00:04,  3.06it/s]Capturing batches (bs=88 avail_mem=42.10 GB):  71%|   | 37/52 [00:21<00:04,  3.06it/s][aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP1] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP5] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP2] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP0] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP3] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP6] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP4] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP7] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=88 avail_mem=42.10 GB):  73%|  | 38/52 [00:21<00:04,  2.82it/s]Capturing batches (bs=80 avail_mem=42.10 GB):  73%|  | 38/52 [00:21<00:04,  2.82it/s][aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP5] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP6] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP2] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP1] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP3] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP4] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP0] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP7] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=80 avail_mem=42.10 GB):  75%|  | 39/52 [00:21<00:04,  3.03it/s]Capturing batches (bs=72 avail_mem=42.10 GB):  75%|  | 39/52 [00:21<00:04,  3.03it/s][aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP3] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP1] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP0] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP4] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP5] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP6] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP7] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:22 TP2] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=72 avail_mem=42.10 GB):  77%|  | 40/52 [00:22<00:04,  2.80it/s]Capturing batches (bs=64 avail_mem=42.09 GB):  77%|  | 40/52 [00:22<00:04,  2.80it/s][aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP5] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP6] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP0] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP1] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP3] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP7] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP4] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP2] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-01 10:52:23 TP6] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-01 10:52:23 TP5] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-01 10:52:23 TP1] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-01 10:52:23 TP0] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-01 10:52:23 TP7] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-01 10:52:23 TP4] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-01 10:52:23 TP3] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-01 10:52:23 TP2] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP5] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP1] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP6] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP0] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP2] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP4] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:23 TP3] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:23 TP5] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:23 TP7] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:23 TP1] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:23 TP6] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:23 TP0] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:23 TP2] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:23 TP4] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:23 TP3] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-01 10:52:23 TP7] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP5] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP6] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP0] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP1] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP2] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP3] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP4] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP7] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=64 avail_mem=42.09 GB):  79%|  | 41/52 [00:22<00:03,  3.02it/s]Capturing batches (bs=56 avail_mem=42.08 GB):  79%|  | 41/52 [00:22<00:03,  3.02it/s][aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP6] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP5] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP7] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP2] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP3] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP4] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP0] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP1] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=56 avail_mem=42.08 GB):  81%|  | 42/52 [00:22<00:03,  2.81it/s]Capturing batches (bs=48 avail_mem=42.08 GB):  81%|  | 42/52 [00:22<00:03,  2.81it/s][aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP5] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP1] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP6] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP4] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP2] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP0] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP3] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:23 TP7] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=48 avail_mem=42.08 GB):  83%| | 43/52 [00:23<00:02,  3.03it/s]Capturing batches (bs=40 avail_mem=42.07 GB):  83%| | 43/52 [00:23<00:02,  3.03it/s][aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:24 TP6] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:24 TP1] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:24 TP2] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:24 TP5] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:24 TP3] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:24 TP0] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:24 TP4] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:52:24 TP7] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=40 avail_mem=42.07 GB):  85%| | 44/52 [00:23<00:02,  2.81it/s]Capturing batches (bs=32 avail_mem=42.07 GB):  85%| | 44/52 [00:23<00:02,  2.81it/s][rank0]:W1101 10:52:30.136000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:52:30.136000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1101 10:52:30.210000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:52:30.220000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:52:30.221000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1101 10:52:30.257000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1101 10:52:30.293000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:52:30.340000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:52:30.352000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:52:30.354000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:52:30.359000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1101 10:52:30.424000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1101 10:52:30.441000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:52:30.461000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:52:30.471000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:52:30.478000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1101 10:52:30.543000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1101 10:52:30.562000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:52:30.580000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:52:30.628000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1101 10:52:30.673000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:52:30.693000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:52:30.711000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1101 10:52:30.842000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1101 10:52:32.071000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:52:32.076000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:52:32.131000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:52:32.188000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:52:32.284000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:52:32.368000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:52:32.388000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:52:32.540000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:52:33.894000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:52:33.942000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:52:33.969000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:52:33.978000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:52:34.017000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:52:34.052000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:52:34.066000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:52:34.078000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:52:34.091000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:52:34.098000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:52:34.115000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:34 TP0] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1101 10:52:34.150000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:52:34.152000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:52:34.167000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:34 TP3] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1101 10:52:34.172000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:34 TP2] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1101 10:52:34.229000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:52:34.247000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:52:34.265000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:52:34.268000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:34 TP1] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1101 10:52:34.303000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:34 TP7] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:34 TP5] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1101 10:52:34.400000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:34 TP6] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1101 10:52:34.470000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:52:34.545000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:52:34.600000 283 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank4]:W1101 10:52:34.644000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:52:34.651000 286 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank2]:W1101 10:52:34.684000 285 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:52:34 TP4] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1101 10:52:34.785000 284 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank5]:W1101 10:52:34.798000 288 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank7]:W1101 10:52:34.804000 290 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank6]:W1101 10:52:34.996000 289 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank4]:W1101 10:52:35.329000 287 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_13 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_1 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_7 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_2 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_11 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_3 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_6 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.1934 seconds and 0.5240 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_11 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_13 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_3 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_5 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_6 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_23 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_1 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_2 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_7 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2775 seconds and 0.5173 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_13 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_11 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_3 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_7 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_23 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_6 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_17 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2745 seconds and 0.5165 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_15 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_11 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_13 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_5 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_7 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_2 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_3 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0065 ms 96.9% 
  triton_bmm_6 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_1 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2591 seconds and 0.6104 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_7 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_13 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_5 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_11 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_3 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_6 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_1 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_2 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_25 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2608 seconds and 0.5834 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_6 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_7 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_3 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_5 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_1 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_11 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_13 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_2 0.0066 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2617 seconds and 0.5412 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_7 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_1 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_3 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_5 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_13 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_6 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_11 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_2 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2719 seconds and 0.5170 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  bmm 0.0062 ms 100.0% 
  triton_bmm_5 0.0064 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_7 0.0064 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0064 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_11 0.0065 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_3 0.0065 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_13 0.0065 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_6 0.0065 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_2 0.0065 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_19 0.0066 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1995 seconds and 0.4981 seconds precompiling for 27 choices
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1101 10:52:45.820000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:52:46.019000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:52:46.071000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:52:46.123000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:52:46.212000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:52:46.236000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:52:46.278000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:52:46.320000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:52:46.556000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:52:46.596000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:52:46.652000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:52:46.723000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:52:46.748000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:52:46.783000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:52:46.821000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:52:47.272000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1101 10:52:50.076000 285 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1101 10:52:55.935000 286 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1101 10:52:57.247000 283 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1101 10:53:01.611000 284 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0068 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_41 0.0076 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0079 ms 86.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0083 ms 81.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0088 ms 77.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0089 ms 76.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0091 ms 75.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0091 ms 74.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0093 ms 73.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0095 ms 71.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 13.4420 seconds and 0.5279 seconds precompiling for 27 choices
[rank2]:W1101 10:53:06.281000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:06.365000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:06.474000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:06 TP2] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE bmm(16x32x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_29 0.0070 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_41 0.0075 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0080 ms 82.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0085 ms 78.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0089 ms 74.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_37 0.0090 ms 73.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0091 ms 73.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0091 ms 72.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_51 0.0095 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 10.2788 seconds and 0.5313 seconds precompiling for 27 choices
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0072 ms 93.9% 
  triton_bmm_41 0.0074 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0077 ms 87.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0083 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0086 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_37 0.0089 ms 75.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0090 ms 75.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0091 ms 74.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_51 0.0095 ms 70.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 9.8202 seconds and 0.5357 seconds precompiling for 27 choices
[rank3]:W1101 10:53:07.853000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:07.935000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:08.043000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:08 TP3] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1101 10:53:08.204000 289 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1101 10:53:08.286000 290 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1101 10:53:08.895000 287 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0069 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0073 ms 94.5% 
  triton_bmm_41 0.0077 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0079 ms 87.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_33 0.0085 ms 81.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_39 0.0085 ms 81.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_46 0.0087 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0089 ms 77.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0092 ms 75.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_32 0.0097 ms 71.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.9348 seconds and 0.5266 seconds precompiling for 27 choices
[rank5]:W1101 10:53:09.312000 288 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
[rank0]:W1101 10:53:09.673000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:09.758000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:09.870000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:09 TP0] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1101 10:53:10.694000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:10.786000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:10.893000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:10 TP1] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:13 TP2] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:15 TP3] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1101 10:53:15.459000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:15.545000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:15.662000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:15 TP2] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1101 10:53:17.108000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:17.182000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:17.279000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:17 TP3] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:17 TP0] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:17 TP1] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1101 10:53:18.488000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:18.562000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:18.564000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:18.637000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:18.661000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:18 TP0] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1101 10:53:18.734000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:18 TP1] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1101 10:53:18.886000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:19.170000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:19.402000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0067 ms 100.0% 
  triton_bmm_41 0.0075 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0077 ms 86.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0084 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0085 ms 78.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_37 0.0088 ms 75.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0089 ms 75.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0089 ms 74.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_31 0.0094 ms 70.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 11.1680 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x512, 16x512x128)
  bmm 0.0067 ms 100.0% 
  triton_bmm_29 0.0067 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_41 0.0075 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0079 ms 83.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0085 ms 78.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0086 ms 77.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0087 ms 76.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0088 ms 75.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0089 ms 74.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_31 0.0095 ms 70.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 11.4019 seconds and 0.0001 seconds precompiling for 27 choices
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:19 TP2] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1101 10:53:20.063000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0069 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0069 ms 98.8% 
  triton_bmm_41 0.0074 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0079 ms 86.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0084 ms 81.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0086 ms 79.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0087 ms 78.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0087 ms 78.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0088 ms 77.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0094 ms 73.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 11.2409 seconds and 0.0001 seconds precompiling for 27 choices
[rank3]:W1101 10:53:20.360000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:20.568000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:20.598000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:20.645000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0069 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0071 ms 97.2% 
  triton_bmm_41 0.0076 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0081 ms 85.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0086 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0087 ms 79.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0091 ms 75.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0094 ms 73.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0095 ms 72.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0097 ms 70.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 11.3236 seconds and 0.0000 seconds precompiling for 27 choices
[rank7]:W1101 10:53:20.745000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:20 TP7] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:20 TP3] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1101 10:53:21.009000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:21.083000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:21.181000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:21.190000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:21 TP2] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1101 10:53:21.243000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:21.266000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:21.319000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1101 10:53:21.364000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:21 TP6] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1101 10:53:21.418000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:21 TP4] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1101 10:53:21.751000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:21.827000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:21.835000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:21.919000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:21.926000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:21 TP5] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1101 10:53:22.090000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:22.155000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:22.356000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:22.371000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:22.391000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:22.431000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:22.528000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:22 TP3] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:22 TP0] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:22 TP1] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1101 10:53:23.963000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:24.041000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:24.051000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:24.115000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:24.149000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:24 TP0] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1101 10:53:24.212000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:24 TP1] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:25 TP7] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:25 TP5] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:25 TP6] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:26 TP4] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1101 10:53:26.504000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:26.580000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:26.680000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:26 TP7] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1101 10:53:26.973000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:27.047000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:27.110000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:27.143000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:27.146000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:27.185000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:27 TP5] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1101 10:53:27.217000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:27.282000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:27.314000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:27 TP6] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:27 TP4] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0100 ms 100.0% 
  triton_mm_55 0.0324 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0396 ms 25.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0454 ms 22.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0488 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0584 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_77 0.0682 ms 14.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0687 ms 14.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_58 0.0689 ms 14.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0706 ms 14.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5844 seconds and 0.3914 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_55 0.0334 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0406 ms 23.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0452 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0527 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0628 ms 15.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_74 0.0688 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0706 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_75 0.0714 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0720 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6414 seconds and 0.3691 seconds precompiling for 27 choices
[rank2]:W1101 10:53:29.113000 285 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edfb1cc7c60>
[rank2]:W1101 10:53:29.144000 285 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edfb1cc75d0>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:53:29 TP2] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1101 10:53:29.854000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:29.870000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:30.070000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:30.095000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:30.106000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:30.306000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:30.330000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:30.338000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:30.445000 286 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7d9d213540>
[rank6]:W1101 10:53:30.474000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:30.476000 286 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7d9d213570>
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0098 ms 100.0% 
  triton_mm_55 0.0305 ms 32.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0401 ms 24.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0453 ms 21.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0493 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0582 ms 16.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0680 ms 14.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0684 ms 14.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0688 ms 14.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0708 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6094 seconds and 0.3878 seconds precompiling for 27 choices
[rank4]:W1101 10:53:30.587000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0102 ms 100.0% 
  triton_mm_55 0.0325 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0394 ms 25.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0453 ms 22.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0488 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0666 ms 15.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_74 0.0687 ms 14.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0687 ms 14.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0704 ms 14.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_75 0.0716 ms 14.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5965 seconds and 0.3778 seconds precompiling for 27 choices
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:53:30 TP3] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1101 10:53:30.712000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:30 TP7] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:30 TP5] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:30 TP4] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1101 10:53:30.991000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:31.098000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:31.338000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:31 TP6] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1101 10:53:31.585000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:31.823000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:31.853000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:31.929000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:31.991000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:31.992000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:31.999000 284 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eef01a8aee0>
[rank7]:W1101 10:53:32.030000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:32.030000 284 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eef01a8ad00>
[rank2]:W1101 10:53:32.059000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:32.068000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:32.077000 283 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3705d4b570>
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:32 TP7] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1101 10:53:32.108000 283 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3705d4b090>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:53:32 TP1] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1101 10:53:32.166000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:32 TP5] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:53:32 TP0] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1101 10:53:32.230000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1101 10:53:32.291000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1101 10:53:32.471000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:32.530000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:32.706000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:32.727000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:32.770000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:32.801000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:32.898000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:32.942000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:32 TP6] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1101 10:53:33.006000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1101 10:53:33.059000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:33.134000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:33.183000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:33.232000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:53:33 TP4] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1101 10:53:33.314000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1101 10:53:33.427000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:33.538000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:33.554000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:33.611000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:33.664000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:33.775000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:33.791000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:33.846000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:33.899000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:34.007000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:34.027000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:34.080000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:34.195000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:34.243000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:34.272000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:34.319000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:34.435000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:34.479000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:34.507000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:34.554000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:34.671000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:34.711000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:34.743000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:34.786000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:34.907000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:34.943000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:34.981000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:35.018000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:35.151000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:35.187000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:35.219000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:35.250000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:35.387000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:35.419000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:35.451000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:35.482000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:35.623000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:35.650000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:35.687000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:35.714000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:35.859000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:35.883000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:35.935000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:36.018000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:36.099000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:36.119000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:36.180000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:36.254000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:36.334000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:36.414000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:36.419000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:36.493000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:36.574000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:36.654000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:36.658000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:36.731000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:36.811000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:36.895000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:36.963000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:36.970000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:37.051000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:37.130000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:37.203000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:37.207000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:37.299000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:37.366000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:37.450000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:37.455000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:37.534000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:37.602000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:37.692000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:37.697000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:37.838000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:37.843000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:37.942000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:37.950000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:38.082000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:38.087000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:38.177000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:38.190000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:38.323000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:38.338000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:38.413000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:38.430000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:38.562000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:38.574000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:38.649000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:38.666000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:38.802000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:38.814000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:38.886000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:38.902000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:39.038000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:39.050000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:39.126000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:39.138000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:39.281000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:39.299000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:39.361000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:39.374000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:39.526000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:39.538000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:39.614000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:39.668000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:39.770000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:39.778000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:39.866000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:39.918000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:40.018000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:40.074000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:40.110000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:40.157000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:40.260000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:40.314000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:40.350000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:40.397000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:40.498000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:40.554000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:40.590000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:40.639000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:40.739000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:40.800000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:40.881000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:40.902000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:40.978000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:41.046000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:41.121000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:41.146000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:41.218000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:41.290000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:41.361000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:41.386000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:41.457000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:41.527000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:41.598000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:41.631000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:41.770000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:41.775000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:41.837000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_55 0.0296 ms 32.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0395 ms 24.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0451 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0489 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0583 ms 16.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0679 ms 14.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0685 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0689 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0705 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 9.4828 seconds and 0.0001 seconds precompiling for 27 choices
[rank2]:W1101 10:53:41.874000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:42.010000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:42.014000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:42.073000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:42.126000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:42.250000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:42.258000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0101 ms 100.0% 
  triton_mm_55 0.0314 ms 32.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0411 ms 24.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0453 ms 22.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0518 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0633 ms 16.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_74 0.0699 ms 14.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0707 ms 14.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_58 0.0715 ms 14.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0718 ms 14.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 9.7680 seconds and 0.0000 seconds precompiling for 27 choices
[rank0]:W1101 10:53:42.313000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:42.366000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:42.491000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:42.495000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:42.549000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:42.606000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:42.730000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:42.734000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:42.785000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:42.846000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_55 0.0299 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0395 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0452 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0490 ms 19.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0582 ms 16.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0679 ms 14.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0685 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0688 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0708 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 9.3529 seconds and 0.0001 seconds precompiling for 27 choices
[rank3]:W1101 10:53:42.972000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:42.977000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0097 ms 100.0% 
  triton_mm_55 0.0296 ms 32.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0399 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0453 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0492 ms 19.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0578 ms 16.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0677 ms 14.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0681 ms 14.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0688 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0705 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 9.7596 seconds and 0.0000 seconds precompiling for 27 choices
[rank0]:W1101 10:53:43.025000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:43.086000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:43.214000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:43.219000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:43.265000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:43.326000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:43.413000 290 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f902f00aeb0>
[rank7]:W1101 10:53:43.445000 290 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f902f00ad90>
[rank3]:W1101 10:53:43.454000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:43.459000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:53:43 TP7] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1101 10:53:43.570000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:43.581000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1101 10:53:43.682000 288 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15989867c0>
[rank3]:W1101 10:53:43.698000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:43.703000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:43.713000 288 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1598986700>
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1101 10:53:43.814000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:43.822000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:53:43 TP5] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1101 10:53:43.942000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1101 10:53:44.014000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:44.058000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:44.069000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:44.193000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:44.254000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:53:44.298000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:44.309000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:44.419000 287 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f14db3c3090>
[rank3]:W1101 10:53:44.434000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:44.450000 287 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f14da4338d0>
[rank6]:W1101 10:53:44.480000 289 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f835961f360>
[rank1]:W1101 10:53:44.494000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:44.511000 289 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f835811b5d0>
[rank2]:W1101 10:53:44.538000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:44.549000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:53:44 TP4] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1101 10:53:44.674000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:53:44 TP6] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1101 10:53:44.734000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1101 10:53:44.778000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:44.789000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1101 10:53:44.916000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:44.971000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:44.976000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:45.030000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:45.159000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:45.214000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:45.219000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:45.224000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:45.270000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:45.399000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:45.454000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:45.464000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:45.469000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:45.506000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:53:45.640000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:45.695000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:45.700000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:45.713000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:45.747000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:45.939000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:45.943000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:45.965000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:45.972000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:45.994000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:46.023000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:46.178000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:46.182000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:46.210000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:46.223000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:46.246000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:46.259000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:46.418000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:46.422000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:46.446000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:46.459000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:46.488000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:46.495000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:46.663000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:46.668000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:46.687000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:46.703000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:46.731000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:46.737000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:46.902000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:46.911000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:46.927000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:46.947000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:46.970000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:46.976000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:47.138000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:47.150000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:47.166000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:53:47.183000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:53:47.207000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:47.212000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:47.374000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:47.386000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:47.402000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:47.447000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:47.616000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:47.642000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:47.683000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:47.688000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:47.882000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:47.910000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:47.923000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:47.935000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:48.152000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:48.180000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:48.186000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:48.220000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:48.399000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:48.424000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:48.431000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:48.459000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:48.638000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:48.669000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:48.674000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:48.694000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:48.874000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:48.908000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:48.913000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:48.930000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:49.110000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:49.147000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:49.154000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:49.166000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:49.347000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:49.386000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:49.390000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:49.402000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:49.586000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:49.622000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:49.626000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:49.638000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:49.822000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:49.862000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:49.866000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:49.874000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:50.062000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:50.102000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:50.107000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:50.111000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:50.298000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:50.345000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:50.350000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:50.355000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:50.534000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:50.588000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:50.594000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:50.605000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:50.770000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:50.826000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:50.832000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:50.846000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:51.007000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:51.067000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:51.073000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:51.083000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:51.242000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:51.306000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:51.311000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:51.387000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:51.542000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:51.547000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:51.551000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:51.628000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_81 0.1035 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1057 ms 57.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1115 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1136 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1138 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1154 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1192 ms 50.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_90 0.1565 ms 38.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_98 0.1567 ms 38.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6568 seconds and 0.3380 seconds precompiling for 27 choices
[rank5]:W1101 10:53:51.782000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:51.850000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:51.858000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:51.870000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:52.022000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:52.091000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:52.102000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:52.111000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:52.262000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:52.330000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:52.342000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:52.355000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_81 0.1046 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1059 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1134 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1148 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1151 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1166 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1209 ms 50.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_90 0.1570 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_99 0.1571 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5984 seconds and 0.3349 seconds precompiling for 27 choices
[rank5]:W1101 10:53:52.506000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:52.571000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:52.582000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:52.599000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:52.750000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:52.806000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:52.822000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:52.839000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:52.986000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:53.043000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:53.062000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:53.078000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:53.222000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:53.282000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:53.306000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:53.318000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:53.458000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:53.522000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:53.551000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:53.556000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:53.694000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:53.764000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:53.790000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:53.796000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:53.930000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:53.998000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:54.026000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:54.039000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_81 0.1042 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1052 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1140 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_100 0.1153 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1159 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_91 0.1164 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1210 ms 50.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1570 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_98 0.1571 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6799 seconds and 0.3324 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_81 0.1037 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1058 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1106 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_100 0.1144 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1148 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_91 0.1158 ms 52.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1184 ms 51.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_90 0.1557 ms 38.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_99 0.1578 ms 38.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6818 seconds and 0.3377 seconds precompiling for 27 choices
[rank5]:W1101 10:53:54.166000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:54.234000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:54.262000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:54.279000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:54.402000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:54.470000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:54.499000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:54.515000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:54.638000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:54.706000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:54.738000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:54.755000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:54.874000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:54.942000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:54.974000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:54.995000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:55.111000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:55.179000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:55.215000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:55.308000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:55.419000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:55.424000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:55.459000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:55.552000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:55.667000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:55.727000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:55.771000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:55.799000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:55.906000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:55.967000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:56.011000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:56.039000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:56.146000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:56.207000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:56.251000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:56.279000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:56.386000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:56.447000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:56.491000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:56.519000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:56.626000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:56.683000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:56.731000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:56.759000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:56.866000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:56.919000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:56.971000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:56.999000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:57.103000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:57.156000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:57.212000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:57.240000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:57.339000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:57.390000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:57.450000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:57.478000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:57.578000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:57.625000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:57.685000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:57.714000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:57.815000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:57.863000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:57.923000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:57.951000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:58.055000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:58.098000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:58.162000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:58.191000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:58.294000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:58.334000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:58.402000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:58.429000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:58.529000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:58.569000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:58.637000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:53:58.665000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:53:58.765000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:58.807000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:58.875000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:59.043000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:59.114000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:59.283000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:59.359000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:53:59.527000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:53:59.603000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0607 ms 100.0% 
  triton_mm_81 0.1040 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1054 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1111 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1140 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1142 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1156 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1189 ms 51.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_90 0.1554 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_99 0.1565 ms 38.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 9.5043 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0612 ms 100.0% 
  triton_mm_81 0.1040 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1059 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_101 0.1145 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1151 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_80 0.1155 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1166 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1207 ms 50.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1566 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_98 0.1569 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 9.5122 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0612 ms 100.0% 
  triton_mm_81 0.1043 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1054 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1144 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1146 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1150 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1165 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1211 ms 50.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1568 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_98 0.1568 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 9.5390 seconds and 0.0000 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_81 0.1038 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1057 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1113 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1142 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1145 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1161 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1195 ms 51.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1563 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_90 0.1564 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 9.5999 seconds and 0.0001 seconds precompiling for 27 choices
Capturing batches (bs=32 avail_mem=42.07 GB):  87%| | 45/52 [02:11<03:48, 32.70s/it]Capturing batches (bs=24 avail_mem=41.45 GB):  87%| | 45/52 [02:11<03:48, 32.70s/it][rank4]:W1101 10:54:16.835000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:54:16.866000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:54:16.910000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:54:16.933000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:54:16.936000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:54:16.941000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:54:17.008000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:54:17.011000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:54:17.015000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:54:17.045000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:54:17.113000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:54:17.116000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:54:17.669000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:54:17.675000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:54:17.676000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:54:17.685000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:54:17.743000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:54:17.750000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:54:17.762000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:54:17.772000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:54:17.848000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:54:17.854000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:54:17.866000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:54:17.877000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:54:18.525000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:54:18.566000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:54:18.630000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:54:18.662000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:54:19.344000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:54:19.353000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:54:19.377000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:54:19.385000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:54:20.014000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:54:20.082000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:54:20.092000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:54:20.168000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:54:20.192000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:54:20.267000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:54:20.512000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:54:20.588000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:54:20.664000 290 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank3]:W1101 10:54:20.698000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:54:20.745000 285 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank4]:W1101 10:54:21.072000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:54:21.148000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:54:21.184000 286 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank0]:W1101 10:54:21.194000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:54:21.246000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:54:21.274000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:54:21.379000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:54:21.561000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:54:21.589000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:54:21.636000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:54:21.664000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:54:21.725000 287 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank5]:W1101 10:54:21.734000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:54:21.762000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:54:21.848000 283 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank1]:W1101 10:54:22.033000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:54:22.111000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:54:22.209000 288 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank1]:W1101 10:54:22.210000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:54:22.238000 289 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank1]:W1101 10:54:22.725000 284 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_115 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_119 0.0063 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_127 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_126 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_120 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_121 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_110 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_111 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_129 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3176 seconds and 0.3700 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_105 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_115 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_119 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_106 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_107 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_111 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_110 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0064 ms 98.7% 
  triton_bmm_120 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2500 seconds and 0.3727 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_119 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_107 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_115 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_106 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_111 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0065 ms 98.1% 
  triton_bmm_117 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_123 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_126 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_109 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2897 seconds and 0.3766 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_109 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_107 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_111 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_115 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_119 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_110 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_105 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_106 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_127 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2928 seconds and 0.3687 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_119 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_111 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_107 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_115 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_129 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0065 ms 97.5% 
  triton_bmm_110 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_120 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_126 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3027 seconds and 0.3790 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_119 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_107 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_111 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_115 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_110 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_106 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_109 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0065 ms 96.9% 
  triton_bmm_105 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.3345 seconds and 0.3719 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_119 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_111 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_110 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_129 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_107 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_105 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_109 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_113 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_115 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3526 seconds and 0.4064 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_115 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_106 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_119 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_107 0.0065 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_111 0.0065 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_127 0.0065 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_129 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_110 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3167 seconds and 0.3682 seconds precompiling for 27 choices
[rank7]:W1101 10:54:31.967000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:54:32.466000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:54:32.783000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:54:32.888000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:54:32.908000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:54:33.280000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:54:33.364000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:54:33.384000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:54:33.409000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:54:33.451000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:54:33.657000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:54:33.855000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:54:33.960000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:54:34.151000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:54:34.523000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:54:34.883000 290 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank5]:W1101 10:54:35.022000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:54:35.378000 286 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank4]:W1101 10:54:35.916000 287 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank1]:W1101 10:54:36.232000 284 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank0]:W1101 10:54:36.715000 283 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank6]:W1101 10:54:37.348000 289 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank2]:W1101 10:54:38.252000 285 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank5]:W1101 10:54:38.307000 288 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x24x512, 16x512x128)
  triton_bmm_133 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0068 ms 99.4% 
  triton_bmm_145 0.0073 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0078 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0083 ms 81.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0084 ms 80.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_141 0.0088 ms 76.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0089 ms 75.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_135 0.0093 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0093 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2596 seconds and 0.5196 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_133 0.0068 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0073 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0080 ms 82.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0083 ms 78.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0086 ms 76.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0090 ms 73.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_152 0.0090 ms 73.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0090 ms 72.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_155 0.0094 ms 69.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1516 seconds and 0.5165 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_133 0.0067 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0074 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0078 ms 84.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0083 ms 79.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0085 ms 77.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0087 ms 75.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0088 ms 74.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0089 ms 73.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_135 0.0094 ms 70.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2340 seconds and 0.5206 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_133 0.0069 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0076 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0079 ms 82.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_137 0.0084 ms 78.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_143 0.0085 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0087 ms 75.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_152 0.0089 ms 73.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0093 ms 70.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_155 0.0094 ms 69.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2286 seconds and 0.5308 seconds precompiling for 27 choices
[rank7]:W1101 10:54:42.186000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:54:42.261000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:54:42.309000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:54:42.457000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0067 ms 100.0% 
  triton_bmm_133 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0073 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0077 ms 86.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0083 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0086 ms 78.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_141 0.0090 ms 74.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0090 ms 74.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_150 0.0093 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_135 0.0093 ms 72.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2055 seconds and 0.5170 seconds precompiling for 27 choices
[rank3]:W1101 10:54:42.533000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:54:42.582000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:54:42.936000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:54:43.020000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:54:43.070000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:54:43.074000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:54:43.149000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0067 ms 100.0% 
  triton_bmm_133 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0073 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0078 ms 85.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0083 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0085 ms 78.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_141 0.0089 ms 75.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0089 ms 74.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_152 0.0090 ms 74.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_155 0.0094 ms 71.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2315 seconds and 0.5214 seconds precompiling for 27 choices
[rank4]:W1101 10:54:43.197000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:54:43.381000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:54:43.458000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:54:43.507000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:54:44.054000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_133 0.0068 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0072 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0079 ms 82.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0082 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0087 ms 75.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0088 ms 74.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_152 0.0090 ms 72.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0091 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_155 0.0093 ms 70.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2094 seconds and 0.5335 seconds precompiling for 27 choices
[rank6]:W1101 10:54:44.129000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_133 0.0069 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0075 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0079 ms 82.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0085 ms 77.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0088 ms 74.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0091 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_150 0.0092 ms 71.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_153 0.0096 ms 68.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_151 0.0097 ms 68.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2430 seconds and 0.5339 seconds precompiling for 27 choices
[rank6]:W1101 10:54:44.284000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:54:44.944000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:54:45.020000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:54:45.034000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:54:45.072000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:54:45.108000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:54:45.156000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:54:48.100000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:54:48.176000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:54:48.277000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:54:48.385000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:54:48.460000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:54:48.506000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:54:48.560000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:54:48.572000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:54:48.583000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:54:48.604000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:54:48.649000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:54:48.680000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:54:48.685000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:54:48.748000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:54:48.782000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:54:49.917000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:54:49.991000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:54:50.089000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:54:50.938000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:54:50.956000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:54:51.012000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:54:51.031000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:54:51.110000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:54:51.129000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:54:51.439000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:54:51.498000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:54:51.518000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:54:51.683000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:54:51.738000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:54:51.757000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:54:51.866000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:54:51.923000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:54:51.978000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:54:51.999000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:54:52.003000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:54:52.108000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:54:52.239000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:54:52.347000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:54:52.481000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:54:52.836000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:54:53.083000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:54:53.322000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:54:53.521000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:54:53.597000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:54:53.617000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:54:53.693000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:54:53.696000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:54:53.736000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:54:53.793000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:54:53.812000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:54:53.918000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:54:53.962000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:54:54.038000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:54:54.144000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:54:54.241000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:54:54.296000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:54:54.303000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:54:54.326000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:54:54.438000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:54:54.548000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:54:54.560000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:54:54.792000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:54:54.827000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:54:54.897000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:54:54.975000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:54:55.075000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:54:56.393000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:54:56.467000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:54:56.564000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:54:56.734000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:54:56.810000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:54:56.909000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_159 0.0284 ms 33.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0401 ms 23.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0450 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0489 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0589 ms 16.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0685 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_162 0.0691 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.0695 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0705 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5174 seconds and 0.3813 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0097 ms 100.0% 
  triton_mm_159 0.0307 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0404 ms 23.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0453 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0490 ms 19.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0596 ms 16.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0684 ms 14.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_178 0.0691 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_162 0.0696 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_179 0.0707 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5394 seconds and 0.3816 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_159 0.0315 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0402 ms 23.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0453 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0487 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0600 ms 15.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0684 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_178 0.0693 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_162 0.0697 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0708 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5320 seconds and 0.3842 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_159 0.0334 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0402 ms 23.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0451 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0510 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0658 ms 14.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_178 0.0693 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_181 0.0705 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_170 0.0707 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_179 0.0710 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5709 seconds and 0.3793 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_159 0.0281 ms 33.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0401 ms 23.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0452 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0487 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0585 ms 16.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0678 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_162 0.0689 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.0690 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0706 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.4970 seconds and 0.3763 seconds precompiling for 27 choices
[rank7]:W1101 10:55:01.394000 290 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f902f00aeb0>
[rank7]:W1101 10:55:01.416000 290 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f902f00ad90>
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_159 0.0310 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0406 ms 23.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0453 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0497 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0599 ms 15.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0681 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_178 0.0694 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0707 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_162 0.0708 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6412 seconds and 0.3767 seconds precompiling for 27 choices
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:55:01 TP7] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1101 10:55:01.520000 284 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eef01a8aee0>
[rank1]:W1101 10:55:01.682000 284 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eef01a8ad00>
[rank0]:W1101 10:55:01.691000 283 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3705d4b570>
[rank0]:W1101 10:55:01.715000 283 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3705d4b090>
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:55:01 TP1] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:55:01 TP0] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1101 10:55:01.783000 286 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7d9d213540>
[rank3]:W1101 10:55:01.807000 286 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7d9d213570>
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:55:01 TP3] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1101 10:55:01.981000 287 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f14db3c3090>
[rank4]:W1101 10:55:02.004000 287 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f14da4338d0>
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:55:02 TP4] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1101 10:55:02.788000 289 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f835961f360>
[rank6]:W1101 10:55:02.811000 289 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f835811b5d0>
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:55:02 TP6] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1101 10:55:03.030000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0097 ms 100.0% 
  triton_mm_159 0.0307 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0414 ms 23.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0453 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0517 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0654 ms 14.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_178 0.0705 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0708 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_179 0.0723 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0726 ms 13.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.7372 seconds and 0.3883 seconds precompiling for 27 choices
[rank3]:W1101 10:55:03.114000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:03.200000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_159 0.0332 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0399 ms 23.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0452 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0486 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0598 ms 15.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_178 0.0692 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_181 0.0695 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_170 0.0706 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_179 0.0709 ms 13.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5395 seconds and 0.3775 seconds precompiling for 27 choices
[rank1]:W1101 10:55:03.282000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:03.366000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:03.448000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:03.490000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:03.530000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:03.614000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:03.691000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:03.751000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:03.778000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:03.859000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:03.936000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:03.994000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:04.022000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:04.071000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:04.103000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:04.179000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:04.238000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:04.266000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:04.279000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:04.319000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:04.347000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:04.424000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:04.482000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:04.509000 285 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edfb1cc7c60>
[rank1]:W1101 10:55:04.511000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:04.523000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:04.532000 285 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edfb1cc75d0>
[rank4]:W1101 10:55:04.564000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:04.595000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:55:04 TP2] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1101 10:55:04.668000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:04.730000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:04.758000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:04.764000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:04.807000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:04.839000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:04.911000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:04.951000 288 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15989867c0>
[rank0]:W1101 10:55:04.974000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:04.974000 288 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1598986700>
[rank1]:W1101 10:55:05.002000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:05.009000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:55:05 TP5] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1101 10:55:05.047000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:05.086000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:05.151000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:05.218000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:05.246000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:05.253000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:05.287000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:05.327000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:05.395000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:05.462000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:05.490000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:05.496000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:05.534000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:05.574000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:05.640000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:05.706000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:05.735000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:05.742000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:05.775000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:05.832000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:05.876000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:05.883000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:05.951000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:05.975000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:05.987000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:06.019000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:06.076000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:06.123000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:06.131000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:06.191000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:06.218000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:06.227000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:06.262000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:06.311000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:06.319000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:06.363000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:06.375000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:06.431000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:06.459000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:06.467000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:06.507000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:06.555000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:06.575000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:06.615000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:06.623000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:06.675000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:06.699000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:06.707000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:06.751000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:06.796000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:06.819000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:06.854000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:06.872000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:06.914000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:06.939000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:06.950000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:06.995000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:07.039000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:07.062000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:07.098000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:07.120000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:07.159000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:07.179000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:07.196000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:07.239000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:07.279000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:07.307000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:07.343000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:07.367000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:07.402000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:07.428000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:07.438000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:07.483000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:07.519000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:07.550000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:07.582000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:07.612000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:07.642000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:07.671000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:07.682000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:07.727000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:07.759000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:07.794000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:07.827000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:07.855000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:07.887000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:07.915000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:07.927000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:07.971000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:07.999000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:08.040000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:08.071000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:08.127000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:08.163000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:08.171000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:08.219000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:08.231000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:08.242000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:08.284000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:08.312000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:08.371000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:08.407000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:08.464000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:08.480000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:08.486000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:08.554000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:08.560000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:08.618000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:08.679000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:08.712000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:08.732000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:08.738000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:08.780000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:08.807000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:08.813000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:08.932000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:08.954000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:08.971000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:08.983000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:09.003000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:09.023000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:09.055000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:09.064000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:09.180000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:09.199000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:09.210000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:09.231000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:09.247000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:09.267000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:09.300000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:09.308000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:09.428000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:09.456000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:09.484000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:09.490000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:09.516000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:09.543000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:09.550000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:09.584000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:09.675000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:09.699000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:09.731000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:09.736000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:09.759000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:09.786000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:09.798000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:09.831000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:09.924000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:09.943000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:09.975000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:09.983000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:10.003000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:10.031000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:10.047000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:10.075000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:10.171000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:10.186000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:10.223000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:10.228000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:10.246000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:10.275000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:10.291000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:10.323000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:10.419000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:10.430000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:10.472000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:10.476000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:10.492000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:10.514000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:10.534000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:10.572000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:10.670000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:10.722000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:10.728000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:10.739000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:10.758000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:10.778000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:10.804000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:10.823000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:10.920000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:10.971000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:10.979000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:10.984000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:11.003000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:11.023000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:11.047000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:11.067000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:11.168000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:11.219000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:11.227000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:11.232000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:11.285000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:11.291000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:11.314000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:11.391000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:11.420000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:11.467000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:11.472000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:11.479000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:11.534000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:11.540000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:11.562000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:11.639000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:11.664000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:11.711000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:11.719000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:11.727000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:11.778000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:11.787000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:11.810000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:11.887000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:11.907000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:11.962000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:11.968000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:11.975000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:12.023000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:12.043000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:12.058000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:12.131000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:12.152000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:12.206000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:12.211000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:12.223000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:12.266000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:12.287000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:12.307000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:12.375000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:12.395000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:12.450000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:12.456000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:12.471000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:12.512000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:12.531000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:12.555000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:12.623000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:12.639000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:12.695000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:12.700000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:12.720000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:12.755000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:12.778000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:12.803000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:12.870000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:12.886000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:12.939000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:12.947000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:12.963000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:12.999000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:13.027000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:13.047000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:13.119000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:13.136000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:13.183000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:13.191000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:13.211000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:13.242000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:13.275000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:13.294000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:13.379000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:13.392000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:13.427000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:13.447000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:13.459000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:13.487000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:13.519000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:13.542000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:13.635000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:13.647000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:13.675000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:13.691000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:13.707000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:13.730000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:13.763000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:13.790000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:13.879000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:13.891000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:13.922000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:13.935000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:13.955000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:13.974000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:14.011000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:14.038000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:14.123000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:14.136000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:14.171000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:14.179000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:14.203000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:14.219000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:14.259000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:14.286000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:14.367000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:14.379000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:14.419000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:14.424000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:14.451000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:14.462000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:14.507000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:14.535000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:14.611000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:14.623000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:14.666000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:14.673000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:14.712000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:14.754000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:14.788000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:14.856000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:14.861000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:14.871000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:14.914000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:14.922000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:14.956000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:15.036000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:15.108000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:15.116000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:15.160000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:15.167000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:15.203000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:15.280000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:15.291000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:15.315000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:15.363000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:15.369000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:15.416000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:15.451000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:15.536000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:15.543000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:15.562000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:15.571000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:15.616000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:15.621000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:15.666000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:15.695000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:15.787000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:15.793000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:15.812000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:15.818000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:15.866000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:15.874000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:15.914000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:15.940000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:16.040000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:16.063000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:16.071000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:16.115000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:16.131000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:16.167000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:16.187000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:16.193000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:16.291000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:16.315000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:16.320000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:16.359000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:16.383000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:16.415000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:16.431000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:16.443000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:16.544000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:16.563000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:16.568000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:16.603000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:16.635000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:16.675000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:16.681000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:16.695000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:16.795000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:16.811000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:16.816000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:16.847000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:16.887000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:16.918000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:16.931000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:16.947000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:17.047000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:17.060000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:17.065000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:17.091000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:17.144000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:17.183000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:17.204000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:17.298000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:17.311000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:17.317000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:17.322000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:17.338000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:17.400000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:17.456000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:17.559000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:17.565000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:17.571000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:17.586000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:17.703000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:17.807000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:17.819000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:17.960000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:17.989000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:18.056000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:18.072000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:18.224000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:18.257000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:18.316000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:18.332000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:18.521000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:18.591000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:18.774000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:18.840000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:19.027000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:19.089000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:19.287000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:19.344000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:19.542000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:19.601000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:19.789000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:19.843000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:20.038000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:20.091000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:20.334000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_185 0.1026 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1053 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1142 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1146 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1151 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1158 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1204 ms 50.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1563 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1565 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6597 seconds and 0.3609 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0610 ms 100.0% 
  triton_mm_185 0.1031 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1058 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1132 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1148 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1153 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1161 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1209 ms 50.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1562 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1565 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6498 seconds and 0.3706 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_185 0.1024 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1053 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1117 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1137 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1143 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1152 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1184 ms 51.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_194 0.1553 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_203 0.1558 ms 38.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6648 seconds and 0.3506 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_185 0.1026 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1057 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1097 ms 55.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1138 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1144 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1151 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1183 ms 51.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_194 0.1557 ms 38.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_203 0.1567 ms 38.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6397 seconds and 0.3683 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_185 0.1025 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1055 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1115 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_204 0.1144 ms 53.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1145 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.1155 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1190 ms 51.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1558 ms 39.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1561 ms 39.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6482 seconds and 0.3541 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_185 0.1027 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1053 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1139 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1149 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1150 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1156 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1209 ms 50.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1559 ms 39.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1560 ms 39.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6249 seconds and 0.3476 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_185 0.1025 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1057 ms 57.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1117 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1150 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_205 0.1150 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1160 ms 52.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.1203 ms 50.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1563 ms 38.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1565 ms 38.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6300 seconds and 0.3639 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_185 0.1025 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1057 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1146 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1160 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_205 0.1164 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1169 ms 52.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.1234 ms 49.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1566 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1571 ms 38.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6479 seconds and 0.3496 seconds precompiling for 27 choices
Capturing batches (bs=24 avail_mem=41.45 GB):  88%| | 46/52 [03:28<04:35, 45.99s/it]Capturing batches (bs=16 avail_mem=40.83 GB):  88%| | 46/52 [03:28<04:35, 45.99s/it][rank0]:W1101 10:55:33.847000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:33.923000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:34.028000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:34.096000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:34.172000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:34.280000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:34.352000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:34.427000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:34.484000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:34.529000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:34.532000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:34.559000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:34.605000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:34.664000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:34.677000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:34.713000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:34.753000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:34.807000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:34.858000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:34.882000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:34.989000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:35.010000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:35.086000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:35.193000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:35.562000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:35.815000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:36.051000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:36.165000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:36.240000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:36.374000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:36.506000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:36.725000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:37.375000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:37.453000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:37.495000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:37.553000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:37.573000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:55:37 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1101 10:55:37.675000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:55:37 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1101 10:55:37.961000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:38.030000 283 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank5]:W1101 10:55:38.037000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:38.139000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:38.162000 286 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:55:38 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1101 10:55:38.322000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:38.402000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:38.491000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:38.501000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:55:38 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1101 10:55:38.569000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:38.608000 288 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank4]:W1101 10:55:38.672000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:55:38 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1101 10:55:38.808000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:38.877000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:38.886000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:38.953000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:38.967000 284 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank2]:W1101 10:55:38.982000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:38.984000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:55:39 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1101 10:55:39.053000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:39.063000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:55:39 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1101 10:55:39.145000 287 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank2]:W1101 10:55:39.172000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:55:39 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1101 10:55:39.460000 290 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank6]:W1101 10:55:39.522000 289 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank2]:W1101 10:55:39.672000 285 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_220 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_226 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_222 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0063 ms 99.4% 
  triton_bmm_218 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_229 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_221 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_227 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_230 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_231 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7997 seconds and 0.1423 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_221 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_227 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 99.4% 
  triton_bmm_215 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_222 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_223 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_229 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_218 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_219 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_224 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8611 seconds and 0.1482 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  bmm 0.0061 ms 100.0% 
  triton_bmm_229 0.0063 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_226 0.0063 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_220 0.0064 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_230 0.0064 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_218 0.0064 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_221 0.0064 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_227 0.0064 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_228 0.0064 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_223 0.0064 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8462 seconds and 0.1500 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_218 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_221 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_212 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_213 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_219 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_220 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_230 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_211 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_214 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8253 seconds and 0.1521 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_213 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_210 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_215 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_220 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_221 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_219 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_226 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_214 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_224 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8709 seconds and 0.1479 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_219 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_213 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_220 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_214 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_218 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_226 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_227 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_224 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_212 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_215 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.7978 seconds and 0.1606 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_211 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_212 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_213 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_218 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_219 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_225 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_226 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_220 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_221 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_208 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9133 seconds and 0.1583 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_210 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_214 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_224 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_213 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_215 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_220 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_228 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_211 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_218 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_219 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7924 seconds and 0.1593 seconds precompiling for 25 choices
[rank3]:W1101 10:55:48.206000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:48.256000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:48.703000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:48.751000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:49.280000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:49.378000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:49.419000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:49.613000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:49.776000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:49.873000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:55:49.918000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:50.062000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:55:50.104000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:55:50.571000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:55:50.590000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:50.988000 283 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank2]:W1101 10:55:51.106000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:51.301000 286 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank5]:W1101 10:55:52.406000 288 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank1]:W1101 10:55:52.667000 284 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank7]:W1101 10:55:53.225000 290 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank4]:W1101 10:55:53.857000 287 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank2]:W1101 10:55:53.928000 285 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank6]:W1101 10:55:54.241000 289 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_234 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_235 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_245 0.0068 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_244 0.0069 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0072 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0072 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_238 0.0082 ms 79.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0082 ms 79.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_255 0.0083 ms 78.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8043 seconds and 0.4241 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_234 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_235 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_244 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_245 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_243 0.0072 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0073 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_238 0.0082 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0082 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_254 0.0084 ms 77.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8355 seconds and 0.4409 seconds precompiling for 25 choices
[rank0]:W1101 10:55:57.168000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:57.248000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:55:57.347000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:55:57 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1101 10:55:57.521000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:57.599000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:55:57.699000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:55:57 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_235 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_234 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_244 0.0070 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_245 0.0071 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0079 ms 83.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0079 ms 83.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_239 0.0082 ms 79.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_238 0.0083 ms 79.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_255 0.0087 ms 75.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9117 seconds and 0.4054 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_234 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_235 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_244 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_245 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0071 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0071 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_239 0.0080 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_238 0.0081 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_255 0.0085 ms 75.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8536 seconds and 0.4046 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x512, 16x512x128)
  triton_bmm_235 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_234 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0066 ms 97.6% 
  triton_bmm_244 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_245 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0072 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0072 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_238 0.0080 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0080 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_236 0.0085 ms 75.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8910 seconds and 0.4024 seconds precompiling for 25 choices
[rank5]:W1101 10:55:58.672000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:58.749000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:55:58.848000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:58.873000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:55:58 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1101 10:55:58.949000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:55:59.047000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:55:59 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_234 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_235 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_244 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_245 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0072 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0073 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_238 0.0082 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0083 ms 78.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_254 0.0083 ms 78.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8016 seconds and 0.4075 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x512, 16x512x128)
  triton_bmm_235 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_234 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0066 ms 98.8% 
  triton_bmm_244 0.0069 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_245 0.0069 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0071 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0072 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_239 0.0080 ms 81.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_238 0.0081 ms 80.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_255 0.0083 ms 77.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8790 seconds and 0.4162 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x512, 16x512x128)
  triton_bmm_235 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_234 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0065 ms 98.2% 
  triton_bmm_244 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_245 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0072 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0072 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_238 0.0081 ms 79.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0081 ms 79.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_237 0.0085 ms 75.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9069 seconds and 0.3930 seconds precompiling for 25 choices
[rank7]:W1101 10:56:00.043000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:00.120000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:00.218000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:00.227000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:00 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1101 10:56:00.303000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:00.403000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:00 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1101 10:56:00.510000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:00.589000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:00.613000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:00.689000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:00.690000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:00 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1101 10:56:00.790000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:00 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:01 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:02 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:02 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1101 10:56:02.809000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:02.886000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:02.935000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:02 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1101 10:56:03.303000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:03.377000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:03.426000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:03 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:03 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1101 10:56:04.247000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:04.322000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:04.370000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:04 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:04 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1101 10:56:04.745000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:04 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1101 10:56:04.822000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:04.870000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:04 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:04 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:05 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1101 10:56:05.801000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:05.877000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:05.927000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:05 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1101 10:56:06.164000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:06.410000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:06.418000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:06.478000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:06.484000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:06.553000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:06.559000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:06.601000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:06.607000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:06 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:06 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1101 10:56:06.663000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:06.668000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:06.900000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:06.916000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:06.975000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:07.024000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:07 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:07 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1101 10:56:07.115000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:07 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1101 10:56:07.363000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:07.608000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:07 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1101 10:56:08.030000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:08.268000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:08.283000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:08.346000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:08.381000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:08.447000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:08.459000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:08 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1101 10:56:08.539000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:08.559000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:08 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:08 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1101 10:56:09.211000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:09.287000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:09.384000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:09.411000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:09 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1101 10:56:09.658000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:09.810000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:09.817000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:09.921000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:10.069000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:10.076000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:10.165000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:10.214000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:10.240000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:10 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1101 10:56:10.319000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:10.331000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:10.339000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:10 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1101 10:56:10.463000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:10 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:10 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1101 10:56:10.708000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:11 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1101 10:56:11.389000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:11.465000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:11.565000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:11 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1101 10:56:11.934000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:12.016000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:12.124000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:12 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1101 10:56:12.322000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:12.399000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:12.499000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:12 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1101 10:56:12.829000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:12.910000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:13.020000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 10:56:13 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0099 ms 100.0% 
  triton_mm_259 0.0305 ms 32.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_269 0.0309 ms 32.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_268 0.0310 ms 31.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_258 0.0322 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_266 0.0467 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_267 0.0468 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_277 0.0511 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0517 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_263 0.0542 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0315 seconds and 0.2127 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_268 0.0309 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_258 0.0315 ms 30.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_269 0.0316 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_259 0.0324 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_267 0.0476 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0497 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0514 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0518 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_262 0.0569 ms 16.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0186 seconds and 0.2284 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0101 ms 100.0% 
  triton_mm_268 0.0328 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_259 0.0328 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_269 0.0330 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_258 0.0335 ms 30.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_266 0.0495 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_267 0.0497 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0512 ms 19.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0513 ms 19.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_263 0.0586 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0656 seconds and 0.2265 seconds precompiling for 25 choices
[rank0]:W1101 10:56:15.220000 283 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3705d4b570>
[rank0]:W1101 10:56:15.243000 283 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3705d4b090>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:56:15 TP0] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1101 10:56:15.697000 286 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7d9d213540>
[rank3]:W1101 10:56:15.720000 286 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7d9d213570>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:56:15 TP3] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0097 ms 100.0% 
  triton_mm_269 0.0312 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_268 0.0318 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_259 0.0354 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_258 0.0356 ms 27.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.0487 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0492 ms 19.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_276 0.0510 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0511 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_279 0.0623 ms 15.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0712 seconds and 0.2127 seconds precompiling for 25 choices
[rank5]:W1101 10:56:16.199000 288 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15989867c0>
[rank5]:W1101 10:56:16.222000 288 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1598986700>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:56:16 TP5] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1101 10:56:16.936000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:17.059000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:17.189000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_268 0.0309 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_269 0.0309 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_259 0.0348 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_258 0.0356 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.0467 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0469 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_276 0.0521 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0521 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_278 0.0625 ms 15.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0409 seconds and 0.2126 seconds precompiling for 25 choices
[rank3]:W1101 10:56:17.315000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:17.361000 284 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eef01a8aee0>
[rank1]:W1101 10:56:17.384000 284 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eef01a8ad00>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:56:17 TP1] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1101 10:56:17.455000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:17.568000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:17.707000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:17.755000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_259 0.0303 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_269 0.0306 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_268 0.0307 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_258 0.0319 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.0464 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0465 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0512 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0514 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_263 0.0540 ms 17.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0540 seconds and 0.2389 seconds precompiling for 25 choices
[rank3]:W1101 10:56:17.819000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:17.959000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:18.011000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:18.068000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_269 0.0307 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_268 0.0308 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_259 0.0334 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_258 0.0338 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.0465 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0465 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_276 0.0515 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0516 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_263 0.0554 ms 17.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0847 seconds and 0.2345 seconds precompiling for 25 choices
[rank0]:W1101 10:56:18.206000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:18.263000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:18.319000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:18.462000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:18.502000 285 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edfb1cc7c60>
[rank5]:W1101 10:56:18.516000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:18.525000 285 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edfb1cc75d0>
[rank3]:W1101 10:56:18.571000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:56:18 TP2] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_269 0.0307 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_268 0.0307 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_259 0.0309 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_258 0.0322 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.0462 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0464 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_276 0.0513 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0514 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_263 0.0542 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9980 seconds and 0.2371 seconds precompiling for 25 choices
[rank1]:W1101 10:56:18.690000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:18.714000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:18.772000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:18.824000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:18.947000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:18.971000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:19.024000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:19.079000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:19.198000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:19.226000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:19.275000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:19.313000 290 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f902f00aeb0>
[rank3]:W1101 10:56:19.331000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:19.336000 290 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f902f00ad90>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:56:19 TP7] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1101 10:56:19.451000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:19.478000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:19.524000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:19.549000 289 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f835961f360>
[rank6]:W1101 10:56:19.572000 289 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f835811b5d0>
[rank3]:W1101 10:56:19.583000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:56:19 TP6] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1101 10:56:19.702000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:19.734000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:19.776000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:19.835000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:19.954000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:19.986000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:20.028000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:20.087000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:20.207000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:20.238000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:20.276000 287 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f14db3c3090>
[rank5]:W1101 10:56:20.280000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:20.300000 287 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f14da4338d0>
[rank3]:W1101 10:56:20.339000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:56:20 TP4] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1101 10:56:20.458000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:20.490000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:20.511000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:20.532000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:20.591000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:20.710000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:20.742000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:20.767000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:20.784000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:20.843000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:20.916000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:20.962000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:20.994000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:21.019000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:21.036000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:21.095000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:21.140000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:21.168000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:21.214000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:21.246000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:21.271000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:21.288000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:21.347000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:21.392000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:21.416000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:21.466000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:21.498000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:21.525000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:21.540000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:21.599000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:21.644000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:21.664000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:21.718000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:21.750000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:21.779000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:21.792000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:21.851000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:21.896000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:21.912000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:21.970000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:22.002000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:22.031000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:22.044000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:22.089000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:22.103000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:22.148000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:22.159000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:22.236000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:22.267000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:22.296000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:22.302000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:22.339000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:22.356000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:22.399000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:22.407000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:22.496000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:22.527000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:22.555000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:22.561000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:22.591000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:22.616000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:22.651000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:22.657000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:22.748000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:22.775000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:22.807000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:22.813000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:22.843000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:22.868000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:22.903000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:22.909000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:23.000000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:23.023000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:23.059000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:23.065000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:23.095000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:23.120000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:23.155000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:23.161000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:23.252000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:23.271000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:23.311000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:23.317000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:23.347000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:23.372000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:23.407000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:23.413000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:23.504000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:23.519000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:23.563000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:23.569000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:23.600000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:23.623000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:23.661000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:23.667000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:23.755000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:23.770000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:23.816000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:23.821000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:23.852000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:23.875000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:23.912000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:23.919000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:24.010000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:24.022000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:24.070000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:24.078000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:24.100000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:24.131000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:24.162000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:24.169000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:24.276000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:24.287000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:24.319000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:24.336000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:24.347000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:24.384000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:24.412000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:24.418000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:24.532000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:24.539000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:24.571000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:24.588000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:24.599000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:24.636000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:24.664000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:24.669000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:24.785000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:24.792000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:24.823000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:24.841000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:24.851000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:24.916000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:24.921000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:25.044000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:25.051000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:25.075000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:25.100000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:25.107000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:25.114000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:25.168000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:25.173000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:25.300000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:25.306000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:25.327000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:25.352000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:25.359000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:25.368000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:25.420000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:25.426000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:25.552000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:25.559000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:25.587000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:25.604000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:25.615000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:25.622000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:25.676000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:25.682000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:25.804000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:25.811000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:25.856000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:25.872000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:25.881000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:25.932000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:25.938000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:26.072000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:26.083000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:26.124000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:26.130000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:26.144000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:26.187000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:26.193000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:26.315000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:26.340000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:26.385000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:26.391000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:26.403000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:26.441000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:26.447000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:26.575000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:26.587000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:26.596000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:26.644000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:26.649000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:26.659000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:26.692000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:26.699000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:26.834000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:26.842000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:26.852000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:26.892000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:26.903000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:26.915000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:26.944000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:26.951000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:27.091000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:27.098000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:27.104000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:27.144000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:27.159000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:27.171000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:27.196000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:27.203000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:27.350000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:27.355000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:27.362000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:27.392000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:27.415000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:27.427000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:27.448000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:27.455000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:27.607000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:27.611000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:27.618000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:27.640000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:27.671000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:27.683000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:27.700000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:27.707000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:27.863000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:27.867000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:27.874000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:27.892000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:27.927000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:27.939000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:27.952000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:27.959000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:28.122000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:28.127000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:28.133000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:28.144000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:28.183000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:28.195000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:28.205000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:28.212000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:28.388000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:28.394000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:28.399000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:28.406000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:28.452000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:28.461000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:28.467000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:28.474000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:28.644000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:28.651000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:28.657000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:28.663000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:28.703000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:28.717000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:28.722000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:28.731000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:28.899000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:28.908000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:28.913000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:28.922000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:28.959000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:28.975000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:29.159000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:29.168000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:29.174000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:29.181000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:29.233000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:29.240000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:29.247000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:29.420000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:29.426000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:29.435000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:29.441000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:29.460000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:29.484000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:29.491000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:29.504000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:29.676000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:29.683000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:29.691000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:29.697000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:29.716000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:29.741000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:29.749000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:29.759000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:29.931000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:29.940000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:29.948000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:29.953000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:29.971000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:29.996000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:30.004000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:30.011000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:30.187000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:30.200000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:30.210000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:30.231000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:30.257000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:30.265000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:30.270000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:30.435000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:30.444000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:30.459000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:30.467000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:30.488000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:30.516000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:30.521000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:30.529000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:30.691000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:30.700000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:30.715000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:30.723000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:30.744000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:30.771000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:30.778000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:30.785000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:30.947000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:30.956000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:30.971000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:30.979000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:31.008000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:31.027000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:31.033000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:31.048000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:31.207000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:31.213000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:31.227000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:31.233000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:31.264000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:31.283000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:31.289000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:31.300000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:31.467000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:31.474000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:31.484000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:31.489000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:31.519000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:31.541000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:31.547000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:31.555000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:31.732000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:31.737000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:31.743000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:31.789000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:31.798000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:31.804000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:31.987000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:31.996000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:32.002000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:32.044000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:32.053000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:32.059000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:32.247000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:32.255000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:32.261000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:32.301000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:32.312000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:32.318000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:32.516000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:32.522000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:32.569000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:32.577000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:32.584000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:32.776000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:32.782000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:32.827000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:32.836000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:32.842000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:33.033000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:33.043000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:33.087000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:33.095000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:33.103000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:33.296000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:33.319000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:33.357000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:33.363000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:33.373000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:33.551000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:33.612000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:33.618000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:33.634000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:33.804000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:33.864000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:33.876000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:33.886000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:34.056000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:34.116000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:34.127000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:34.141000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:34.319000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:34.380000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:34.386000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:34.404000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:34.579000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:34.642000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:34.647000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:34.655000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:34.839000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:34.899000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:34.904000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:34.914000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:35.095000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:35.155000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:35.161000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:35.167000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:35.351000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:35.411000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:35.417000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:35.608000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:35.668000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:35.861000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:36.120000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:36.384000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:36.648000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0612 ms 100.0% 
  triton_mm_282 0.1037 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1037 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_292 0.1047 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1048 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1093 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1094 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1128 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1128 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_287 0.1486 ms 41.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0399 seconds and 0.1870 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_282 0.1037 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1038 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_292 0.1049 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1049 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1096 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1097 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1136 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1137 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_286 0.1486 ms 41.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1039 seconds and 0.1828 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_283 0.1037 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_282 0.1037 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_292 0.1048 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1049 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1093 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1095 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1130 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1133 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_287 0.1482 ms 41.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1105 seconds and 0.1998 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_282 0.1035 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1037 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_292 0.1046 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1047 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1098 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1101 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_291 0.1135 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1138 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_287 0.1485 ms 41.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0307 seconds and 0.1851 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0610 ms 100.0% 
  triton_mm_283 0.1036 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_282 0.1037 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_293 0.1046 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_292 0.1049 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_300 0.1095 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1096 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_291 0.1130 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1131 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_287 0.1476 ms 41.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0284 seconds and 0.1881 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_283 0.1035 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_282 0.1035 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_292 0.1045 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1045 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1101 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1102 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_291 0.1138 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1140 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_286 0.1484 ms 41.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0627 seconds and 0.1892 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0612 ms 100.0% 
  triton_mm_283 0.1033 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_282 0.1034 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_293 0.1045 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_292 0.1045 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_300 0.1096 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1096 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1134 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1134 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_286 0.1482 ms 41.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0627 seconds and 0.1888 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_282 0.1037 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1038 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_292 0.1047 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1047 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1101 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1103 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1139 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1139 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_287 0.1486 ms 41.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0349 seconds and 0.1918 seconds precompiling for 25 choices
Capturing batches (bs=16 avail_mem=40.83 GB):  90%| | 47/52 [04:44<04:34, 54.81s/it]Capturing batches (bs=12 avail_mem=40.25 GB):  90%| | 47/52 [04:44<04:34, 54.81s/it][rank2]:W1101 10:56:49.207000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:49.239000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:49.285000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:49.315000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:49.381000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:49.393000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:49.420000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:49.459000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:49.566000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:49.702000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:49.760000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:49.777000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:49.802000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:49.838000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:49.845000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:49.879000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:49.886000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:49.921000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:49.947000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:49.985000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:50.026000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:50.217000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:50.294000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:50.404000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:50.938000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:50.949000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:51.101000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:51.411000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:51.485000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:51.502000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:51.526000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:51.927000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:52.464000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:52.544000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:52.646000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:52.787000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:52.864000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:52.885000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:52.963000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:56:52.965000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:53.061000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:53.133000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:53.210000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:53.248000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:53.311000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:53.326000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:56:53.333000 285 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank4]:W1101 10:56:53.426000 287 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank7]:W1101 10:56:53.431000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:53.497000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:56:53.521000 289 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank5]:W1101 10:56:53.579000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:53.628000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:53.685000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:53.712000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:56:53.755000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:56:53.825000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:56:53.827000 284 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank0]:W1101 10:56:53.843000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:56:53.916000 290 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank0]:W1101 10:56:53.951000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:56:54.151000 288 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank3]:W1101 10:56:54.304000 286 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank0]:W1101 10:56:54.436000 283 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_310 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_309 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_316 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_323 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_324 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_317 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_322 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_311 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_315 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_321 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7734 seconds and 0.1391 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_317 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 100.0% 
  triton_bmm_314 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_315 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_316 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_310 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_311 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_318 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_320 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_321 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8818 seconds and 0.1541 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_321 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_324 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_315 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_318 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_322 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_323 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_327 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_311 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_314 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_307 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8922 seconds and 0.1620 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_307 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_309 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_314 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_308 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_311 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_316 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_322 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_306 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_310 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8204 seconds and 0.1503 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_309 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_310 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_311 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_315 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_316 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_319 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_321 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_308 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_314 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_317 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8790 seconds and 0.1501 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_315 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_316 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_307 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_306 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_309 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_310 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_311 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_314 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_308 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_317 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8970 seconds and 0.1325 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_311 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_314 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_315 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_327 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_316 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0064 ms 99.4% 
  triton_bmm_306 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_307 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_310 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_324 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8072 seconds and 0.1299 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_305 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_306 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_307 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_315 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 99.4% 
  triton_bmm_311 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_316 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_309 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_304 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_308 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8328 seconds and 0.1552 seconds precompiling for 25 choices
[rank4]:W1101 10:57:03.699000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:03.830000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:04.055000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:04.064000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:04.197000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:04.328000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:04.433000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:04.553000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:04.559000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:04.716000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:04.741000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:04.884000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:04.936000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:05.231000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:05.249000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:05.385000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:06.702000 284 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank4]:W1101 10:57:06.754000 287 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank0]:W1101 10:57:07.160000 283 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank7]:W1101 10:57:07.747000 290 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank3]:W1101 10:57:08.085000 286 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank2]:W1101 10:57:08.165000 285 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank5]:W1101 10:57:08.426000 288 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank6]:W1101 10:57:09.939000 289 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_331 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_330 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_340 0.0069 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0069 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_339 0.0072 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0073 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_335 0.0080 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_334 0.0081 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_350 0.0085 ms 76.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8213 seconds and 0.4269 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  triton_bmm_331 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_330 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_340 0.0069 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0069 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0072 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0072 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0073 ms 90.0% 
  triton_bmm_334 0.0081 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0081 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_351 0.0083 ms 78.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9101 seconds and 0.4016 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  triton_bmm_330 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0067 ms 97.6% 
  triton_bmm_340 0.0070 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0070 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0072 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0073 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_334 0.0082 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0082 ms 79.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_350 0.0084 ms 77.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8143 seconds and 0.4201 seconds precompiling for 25 choices
[rank1]:W1101 10:57:12.904000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:12.981000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:13.030000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x12x512, 16x512x128)
  triton_bmm_330 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 99.4% 
  triton_bmm_340 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_339 0.0071 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0071 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_334 0.0079 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0079 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_351 0.0082 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8453 seconds and 0.4119 seconds precompiling for 25 choices
[rank4]:W1101 10:57:13.138000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:13.215000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:13.264000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_330 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_340 0.0069 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0070 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0073 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0073 ms 88.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_334 0.0081 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0081 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_351 0.0083 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8154 seconds and 0.4120 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_330 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_341 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_340 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0072 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0073 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_334 0.0081 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0081 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_351 0.0085 ms 76.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7704 seconds and 0.4198 seconds precompiling for 25 choices
[rank0]:W1101 10:57:13.468000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:13.555000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:13.604000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_330 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_340 0.0072 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0072 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0078 ms 84.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0079 ms 83.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_335 0.0082 ms 79.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_334 0.0083 ms 78.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_350 0.0087 ms 75.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7749 seconds and 0.4148 seconds precompiling for 25 choices
[rank7]:W1101 10:57:14.085000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:14.161000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:14.210000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:14.262000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:14.340000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:14.390000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:14.559000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:14.635000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:14.683000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:14.749000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:14.827000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:14.876000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x12x512, 16x512x128)
  triton_bmm_330 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 98.8% 
  triton_bmm_340 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0071 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0072 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_335 0.0080 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_334 0.0081 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_350 0.0084 ms 77.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8312 seconds and 0.4001 seconds precompiling for 25 choices
[rank6]:W1101 10:57:16.240000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:16.316000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:16.365000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:18.476000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:18.540000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:18.553000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:18.620000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:18.657000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:18.730000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:19.583000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:19.663000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:19.767000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:20.029000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:20.107000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:20.209000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:20.387000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:20.466000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:20.569000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:20.633000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:20.710000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:20.812000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:21.313000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:21.391000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:21.422000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:21.494000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:21.512000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:21.682000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:21.769000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:21.938000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:22.023000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:22.292000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:22.369000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:22.469000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:22.950000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:22.976000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:23.210000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:23.232000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:23.470000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:23.484000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:23.643000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:23.714000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:23.720000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:23.736000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:23.792000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:23.820000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:23.893000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:23.999000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:24.012000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:24.259000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:24.272000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:24.532000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:24.668000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:24.943000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:24.979000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:25.056000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:25.072000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:25.148000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:25.156000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:25.203000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:25.248000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:25.664000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:25.911000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:25.928000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:25.989000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:26.090000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:26.184000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:26.448000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:26.528000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:26.632000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:26.853000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:26.929000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:27.031000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:27.813000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:27.889000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:27.988000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_365 0.0308 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.0309 ms 30.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_355 0.0312 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_354 0.0336 ms 27.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_362 0.0460 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0460 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0514 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0519 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_359 0.0546 ms 17.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0823 seconds and 0.2281 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0099 ms 100.0% 
  triton_mm_365 0.0308 ms 32.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.0315 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_355 0.0357 ms 27.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_354 0.0358 ms 27.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_362 0.0475 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0479 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0506 ms 19.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0507 ms 19.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_359 0.0599 ms 16.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0771 seconds and 0.2299 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_355 0.0307 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_365 0.0307 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.0308 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_354 0.0323 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_362 0.0461 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0466 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0510 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0512 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.0539 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0898 seconds and 0.2171 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_365 0.0306 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.0308 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_355 0.0324 ms 29.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_354 0.0330 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_363 0.0462 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_362 0.0466 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_372 0.0507 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0509 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_359 0.0572 ms 16.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0408 seconds and 0.2233 seconds precompiling for 25 choices
[rank4]:W1101 10:57:31.051000 287 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f14db3c3090>
[rank4]:W1101 10:57:31.074000 287 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f14da4338d0>
[rank1]:W1101 10:57:31.102000 284 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eef01a8aee0>
[rank1]:W1101 10:57:31.124000 284 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eef01a8ad00>
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:57:31 TP4] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:57:31 TP1] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_365 0.0318 ms 29.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.0331 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_354 0.0339 ms 27.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_355 0.0342 ms 27.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_362 0.0468 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0487 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0515 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0515 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.0598 ms 15.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1059 seconds and 0.2320 seconds precompiling for 25 choices
[rank7]:W1101 10:57:32.124000 290 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f902f00aeb0>
[rank7]:W1101 10:57:32.147000 290 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f902f00ad90>
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:57:32 TP7] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0097 ms 100.0% 
  triton_mm_355 0.0327 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_365 0.0332 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_354 0.0332 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_364 0.0334 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_362 0.0497 ms 19.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0508 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0509 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0520 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_359 0.0594 ms 16.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1072 seconds and 0.2125 seconds precompiling for 25 choices
[rank1]:W1101 10:57:32.481000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_365 0.0306 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.0307 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_354 0.0347 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_355 0.0347 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_363 0.0459 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_362 0.0460 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0522 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0522 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_359 0.0548 ms 17.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0537 seconds and 0.2193 seconds precompiling for 25 choices
[rank1]:W1101 10:57:32.743000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:32.802000 283 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3705d4b570>
[rank0]:W1101 10:57:32.825000 283 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3705d4b090>
[rank4]:W1101 10:57:32.852000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:57:32 TP0] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1101 10:57:33.007000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:33.087000 286 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7d9d213540>
[rank3]:W1101 10:57:33.110000 286 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7d9d213570>
[rank4]:W1101 10:57:33.111000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:57:33 TP3] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1101 10:57:33.267000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:33.371000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:33.479000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:33.532000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_365 0.0308 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.0311 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_355 0.0336 ms 28.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_354 0.0343 ms 27.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_363 0.0462 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_362 0.0463 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_372 0.0517 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0518 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_359 0.0562 ms 16.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9793 seconds and 0.2137 seconds precompiling for 25 choices
[rank5]:W1101 10:57:33.614000 288 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15989867c0>
[rank4]:W1101 10:57:33.631000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:33.637000 288 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1598986700>
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:57:33 TP5] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1101 10:57:33.739000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:33.803000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:33.902000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:33.974000 285 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edfb1cc7c60>
[rank2]:W1101 10:57:33.997000 285 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edfb1cc75d0>
[rank7]:W1101 10:57:34.000000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:34.060000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:57:34 TP2] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1101 10:57:34.164000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:34.261000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:34.324000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:34.420000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:34.520000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:34.587000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:34.599000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:34.614000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:34.684000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:34.780000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:34.849000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:34.854000 289 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f835961f360>
[rank3]:W1101 10:57:34.861000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:34.875000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:34.876000 289 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f835811b5d0>
[rank4]:W1101 10:57:34.944000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:57:34 TP6] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1101 10:57:34.980000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:35.040000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:35.108000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:35.120000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:35.131000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:35.204000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:35.236000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:35.301000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:35.368000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:35.375000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:35.386000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:35.464000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:35.487000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:35.495000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:35.560000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:35.631000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:35.637000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:35.650000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:35.724000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:35.747000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:35.754000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:35.820000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:35.892000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:35.901000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:35.913000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:35.980000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:36.007000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:36.014000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:36.080000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:36.164000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:36.171000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:36.178000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:36.211000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:36.244000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:36.265000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:36.272000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:36.345000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:36.423000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:36.429000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:36.434000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:36.468000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:36.504000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:36.524000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:36.530000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:36.612000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:36.687000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:36.693000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:36.697000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:36.724000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:36.760000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:36.780000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:36.803000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:36.873000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:36.953000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:36.960000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:36.967000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:36.980000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:37.015000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:37.035000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:37.064000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:37.132000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:37.212000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:37.220000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:37.227000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:37.235000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:37.275000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:37.291000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:37.319000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:37.396000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:37.471000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:37.479000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:37.485000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:37.492000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:37.536000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:37.553000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:37.575000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:37.661000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:37.735000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:37.741000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:37.747000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:37.755000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:37.809000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:37.836000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:37.928000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:38.009000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:38.017000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:38.022000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:38.067000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:38.101000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:38.107000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:38.192000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:38.272000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:38.279000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:38.286000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:38.321000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:38.328000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:38.373000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:38.380000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:38.539000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:38.546000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:38.557000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:38.591000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:38.598000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:38.640000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:38.649000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:38.796000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:38.801000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:38.807000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:38.815000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:38.851000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:38.860000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:38.896000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:38.912000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:39.060000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:39.067000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:39.073000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:39.079000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:39.112000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:39.119000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:39.151000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:39.177000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:39.324000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:39.331000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:39.337000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:39.342000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:39.368000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:39.385000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:39.413000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:39.436000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:39.579000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:39.596000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:39.602000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:39.609000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:39.623000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:39.648000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:39.672000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:39.700000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:39.840000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:39.860000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:39.868000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:39.874000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:39.910000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:39.929000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:39.970000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:40.105000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:40.131000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:40.141000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:40.184000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:40.192000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:40.209000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:40.237000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:40.365000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:40.404000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:40.457000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:40.464000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:40.472000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:40.481000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:40.496000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:40.624000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:40.671000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:40.725000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:40.732000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:40.739000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:40.745000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:40.754000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:40.762000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:40.884000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:40.939000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:40.992000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:41.003000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:41.010000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:41.020000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:41.029000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:41.145000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:41.204000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:41.259000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:41.270000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:41.278000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:41.287000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:41.294000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:41.311000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:41.405000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:41.468000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:41.523000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:41.534000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:41.542000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:41.551000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:41.559000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:41.575000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:41.740000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:41.799000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:41.808000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:41.813000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:41.824000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:41.830000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:41.847000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:41.991000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:42.007000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:42.065000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:42.072000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:42.080000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:42.087000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:42.096000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:42.112000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:42.255000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:42.271000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:42.328000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:42.334000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:42.345000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:42.351000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:42.360000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:42.372000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:42.519000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:42.535000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:42.592000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:42.598000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:42.608000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:42.617000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:42.625000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:42.633000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:42.783000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:42.799000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:42.856000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:42.862000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:42.870000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:42.879000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:42.889000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:42.897000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:43.047000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:43.063000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:43.120000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:43.127000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:43.133000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:43.144000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:43.153000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:43.160000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:43.311000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:43.327000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:43.384000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:43.391000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:43.397000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:43.408000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:43.417000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:43.424000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:43.577000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:43.592000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:43.652000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:43.658000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:43.666000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:43.674000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:43.687000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:43.695000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:43.841000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:43.856000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:43.912000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:43.923000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:43.931000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:43.938000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:43.951000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:43.959000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:44.100000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:44.120000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:44.172000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:44.187000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:44.194000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:44.202000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:44.216000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:44.223000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:44.360000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:44.384000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:44.431000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:44.451000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:44.458000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:44.466000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:44.481000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:44.490000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:44.620000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:44.647000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:44.691000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:44.717000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:44.724000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:44.732000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:44.746000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:44.754000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:44.883000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:44.915000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:44.951000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:44.980000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:44.987000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:44.995000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:45.008000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:45.017000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:45.149000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:45.184000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:45.212000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:45.243000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:45.248000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:45.261000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:45.268000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:45.276000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:45.413000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:45.448000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:45.472000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:45.511000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:45.516000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:45.526000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:45.533000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:45.539000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:45.673000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:45.712000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:45.732000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:45.778000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:45.785000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:45.793000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:45.801000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:45.808000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:45.933000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:45.976000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:45.992000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:46.042000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:46.051000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:46.058000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:46.067000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:46.073000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:46.193000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:46.240000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:46.252000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:46.307000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:46.315000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:46.322000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:46.331000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:46.337000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:46.453000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:46.506000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:46.515000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:46.572000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:46.581000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:46.588000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:46.598000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:46.605000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:46.712000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:46.771000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:46.777000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:46.836000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:46.848000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:46.855000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:46.865000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:46.872000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:46.976000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:47.039000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:47.044000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:47.100000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:47.112000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:47.120000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:47.130000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:47.137000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:47.236000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:47.307000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:47.313000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:47.363000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:47.376000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:47.387000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:47.396000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:47.403000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:47.496000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:47.571000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:47.579000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:47.628000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:57:47.640000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:47.651000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:47.661000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:47.668000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:47.755000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:47.835000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:47.847000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:47.895000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:57:47.915000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:47.924000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:47.932000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:48.020000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:48.099000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:48.115000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:48.156000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:48.188000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:48.196000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:48.288000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:48.363000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:48.383000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:48.420000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:48.452000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:48.460000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:48.552000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:48.629000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:57:48.654000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:48.687000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:48.716000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:48.728000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:48.822000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:48.901000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:48.959000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:48.988000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:48.995000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:49.096000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:49.164000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:49.228000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:49.256000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:49.264000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:49.356000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:49.423000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:49.500000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:49.525000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:49.533000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:49.620000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:49.691000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:57:49.768000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:57:49.796000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:49.803000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:49.883000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:57:49.955000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:50.064000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:50.143000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:50.320000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:50.405000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:57:50.582000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:50.670000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:50.938000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:57:51.206000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_379 0.1043 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_378 0.1043 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_389 0.1045 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_388 0.1045 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1094 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1095 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1132 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1132 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_383 0.1455 ms 42.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1474 seconds and 0.1967 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_379 0.1043 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_378 0.1045 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_388 0.1047 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1047 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1091 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1092 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1126 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1128 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_383 0.1449 ms 42.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0848 seconds and 0.1854 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_378 0.1041 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_379 0.1042 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_388 0.1045 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1045 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_397 0.1089 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1089 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_386 0.1127 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1127 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_383 0.1446 ms 42.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0064 seconds and 0.1866 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_378 0.1044 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_379 0.1045 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_389 0.1047 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_388 0.1047 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_396 0.1094 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1094 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1126 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1127 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_382 0.1455 ms 42.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0231 seconds and 0.1998 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_378 0.1045 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_379 0.1045 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_388 0.1047 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1049 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_397 0.1100 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1101 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1131 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1131 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_383 0.1454 ms 42.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0535 seconds and 0.1981 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_379 0.1044 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_378 0.1045 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_388 0.1047 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1047 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1096 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1097 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_387 0.1128 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1129 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_382 0.1452 ms 42.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0765 seconds and 0.2235 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_379 0.1045 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_378 0.1046 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_388 0.1047 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1048 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1094 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1096 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_387 0.1125 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1126 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_382 0.1443 ms 42.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0765 seconds and 0.1909 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_388 0.1041 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1041 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_379 0.1042 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_378 0.1043 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_397 0.1102 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1103 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1132 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1133 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_383 0.1453 ms 42.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0702 seconds and 0.1895 seconds precompiling for 25 choices
Capturing batches (bs=12 avail_mem=40.25 GB):  92%|| 48/52 [05:58<04:03, 60.78s/it]Capturing batches (bs=8 avail_mem=39.65 GB):  92%|| 48/52 [05:58<04:03, 60.78s/it] [rank2]:W1101 10:58:04.008000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:04.087000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:04.093000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:04.170000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:04.196000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:04.215000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:04.221000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:04.235000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:04.277000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:04.282000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:04.293000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:04.299000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:04.312000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:04.360000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:04.402000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:04.406000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:04.419000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:04.468000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:04.617000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:04.696000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:04.708000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:04.786000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:04.805000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:04.896000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:05.770000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:05.805000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:05.934000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:05.939000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:05.945000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:06.001000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:06.339000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:06.410000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:07.640000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:07.707000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:07.720000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:07.740000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:07.765000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:07.786000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:07.821000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:07.822000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:07.839000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:07.844000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:07.888000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:07.918000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:07.925000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:07.945000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:07.966000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:08.019000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:08.045000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:08.147000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:08.159000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:08.238000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:08.290000 286 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank1]:W1101 10:58:08.340000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:08.356000 285 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank7]:W1101 10:58:08.395000 290 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank6]:W1101 10:58:08.408000 289 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank4]:W1101 10:58:08.487000 287 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank0]:W1101 10:58:08.645000 283 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank5]:W1101 10:58:08.760000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:08.843000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:08.859000 284 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank5]:W1101 10:58:08.948000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:09.417000 288 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_411 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_410 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_421 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_407 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_412 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_403 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_413 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_419 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_406 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0064 ms 98.1% 
SingleProcess AUTOTUNE benchmarking takes 4.8382 seconds and 0.1599 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_407 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_411 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_410 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_404 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_406 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_402 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_401 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_412 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_413 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_421 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7978 seconds and 0.1564 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_419 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_413 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_423 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_411 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_412 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_418 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_402 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_416 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8888 seconds and 0.1543 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_414 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_422 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_417 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_418 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_419 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_413 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_421 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_411 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9153 seconds and 0.1496 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_400 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_402 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_401 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_403 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_405 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_414 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_418 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8821 seconds and 0.1445 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_407 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_419 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_405 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_406 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_420 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_416 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  bmm 0.0064 ms 98.1% 
  triton_bmm_400 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_412 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8412 seconds and 0.1599 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_405 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_407 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_418 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_406 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_413 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_419 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_404 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_416 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8820 seconds and 0.1322 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_411 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_412 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_407 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_410 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_413 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 99.4% 
  triton_bmm_406 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_402 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_403 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9154 seconds and 0.1474 seconds precompiling for 25 choices
[rank3]:W1101 10:58:18.492000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:18.543000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:18.644000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:19.008000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:19.041000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:19.145000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:19.312000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:19.383000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:19.562000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:19.602000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:19.690000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:19.809000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:19.886000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:20.066000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:20.118000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:20.195000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:21.164000 284 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank3]:W1101 10:58:21.348000 286 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank6]:W1101 10:58:21.503000 289 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank4]:W1101 10:58:22.163000 287 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank2]:W1101 10:58:22.833000 285 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank7]:W1101 10:58:23.870000 290 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank0]:W1101 10:58:24.057000 283 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank5]:W1101 10:58:25.925000 288 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_427 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_426 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_436 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0069 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0072 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0072 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_431 0.0080 ms 80.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_430 0.0081 ms 79.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_447 0.0083 ms 78.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8113 seconds and 0.3978 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_426 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_427 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_437 0.0070 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_436 0.0070 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_434 0.0074 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0074 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_430 0.0079 ms 82.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0080 ms 81.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_446 0.0084 ms 77.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8268 seconds and 0.4015 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_426 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_427 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_436 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0071 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0071 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_430 0.0080 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0080 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_446 0.0085 ms 75.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8624 seconds and 0.4147 seconds precompiling for 25 choices
[rank1]:W1101 10:58:27.335000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:27.411000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x8x512, 16x512x128)
  triton_bmm_426 0.0066 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0066 ms 99.4% 
  triton_bmm_427 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_436 0.0069 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0069 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_435 0.0071 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0072 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_430 0.0080 ms 82.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0080 ms 82.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_447 0.0083 ms 79.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8557 seconds and 0.4240 seconds precompiling for 25 choices
[rank1]:W1101 10:58:27.520000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:27.529000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:27.607000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:27.707000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:27.725000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:27.802000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:27.902000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_426 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_427 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_436 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0068 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_435 0.0071 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0073 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_431 0.0079 ms 82.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_430 0.0079 ms 81.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_447 0.0082 ms 79.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8064 seconds and 0.4287 seconds precompiling for 25 choices
[rank4]:W1101 10:58:28.528000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:28.606000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:28.707000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:29.024000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:29.102000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x8x512, 16x512x128)
  triton_bmm_427 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 98.8% 
  triton_bmm_426 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_436 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_435 0.0073 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0073 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_430 0.0079 ms 81.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0079 ms 81.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_447 0.0084 ms 76.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7935 seconds and 0.4091 seconds precompiling for 25 choices
[rank2]:W1101 10:58:29.203000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_426 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_427 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_436 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0072 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0073 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_430 0.0080 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0080 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_446 0.0083 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8439 seconds and 0.4018 seconds precompiling for 25 choices
[rank7]:W1101 10:58:30.587000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:30.666000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:30.699000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:30.768000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:30.777000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:30.877000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_426 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_427 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_436 0.0071 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0072 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0078 ms 83.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0079 ms 83.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_430 0.0081 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0081 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_447 0.0086 ms 75.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7698 seconds and 0.4004 seconds precompiling for 25 choices
[rank5]:W1101 10:58:32.045000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:32.123000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:32.223000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:33.200000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:33.278000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:33.328000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:33.386000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:33.463000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:33.513000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:33.552000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:33.628000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:33.678000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:34.297000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:34.384000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:34.386000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:34.436000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:34.462000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:34.512000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:36.092000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:36.292000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:36.356000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:36.369000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:36.419000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:36.522000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:36.599000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:36.619000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:36.648000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:36.705000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:36.875000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:36.969000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:37.139000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:37.237000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:37.399000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:37.680000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:37.905000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:37.948000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:38.160000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:38.173000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:38.215000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:38.235000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:38.285000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:38.440000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:38.670000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:38.748000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:38.851000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:39.335000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:39.419000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:39.478000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:39.524000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:39.556000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:39.621000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:39.657000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:39.819000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:39.885000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:40.091000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:40.158000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:40.377000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:40.384000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:40.462000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:40.494000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:40.575000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:40.684000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:40.978000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:41.494000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:41.766000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:42.038000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:42.186000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:42.264000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:42.366000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:42.490000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:42.569000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:42.671000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:43.675000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:44.141000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:44.243000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_461 0.0301 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0303 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_451 0.0347 ms 27.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_450 0.0350 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_459 0.0466 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.0467 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0497 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0504 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_455 0.0592 ms 15.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0594 seconds and 0.2128 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_460 0.0305 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_461 0.0309 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_451 0.0332 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_450 0.0334 ms 27.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_459 0.0481 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.0491 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0507 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0512 ms 17.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_455 0.0561 ms 16.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0795 seconds and 0.2290 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_460 0.0299 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_461 0.0300 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_451 0.0321 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_450 0.0329 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_458 0.0458 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.0459 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.0508 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0509 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_455 0.0548 ms 16.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0442 seconds and 0.2187 seconds precompiling for 25 choices
[rank1]:W1101 10:58:45.849000 284 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eef01a8aee0>
[rank1]:W1101 10:58:45.872000 284 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eef01a8ad00>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:58:45 TP1] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_451 0.0298 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_461 0.0301 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0301 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_450 0.0318 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_459 0.0457 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.0457 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_468 0.0507 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0511 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_455 0.0545 ms 16.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0665 seconds and 0.2169 seconds precompiling for 25 choices
[rank6]:W1101 10:58:46.459000 289 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f835961f360>
[rank6]:W1101 10:58:46.482000 289 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f835811b5d0>
[rank3]:W1101 10:58:46.531000 286 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7d9d213540>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:58:46 TP6] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1101 10:58:46.555000 286 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7d9d213570>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:58:46 TP3] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_461 0.0302 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0304 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_451 0.0344 ms 27.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_450 0.0348 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_458 0.0463 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.0465 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0512 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0514 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_455 0.0572 ms 16.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1710 seconds and 0.2160 seconds precompiling for 25 choices
[rank4]:W1101 10:58:47.518000 287 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f14db3c3090>
[rank4]:W1101 10:58:47.541000 287 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f14da4338d0>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:58:47 TP4] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1101 10:58:47.644000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:47.916000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:47.964000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_451 0.0291 ms 31.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_461 0.0299 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0301 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_450 0.0307 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_459 0.0457 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.0458 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_468 0.0500 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0503 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_455 0.0538 ms 17.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0531 seconds and 0.2162 seconds precompiling for 25 choices
[rank3]:W1101 10:58:48.053000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:48.092000 285 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edfb1cc7c60>
[rank2]:W1101 10:58:48.116000 285 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edfb1cc75d0>
[rank1]:W1101 10:58:48.180000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:58:48 TP2] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1101 10:58:48.229000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:48.317000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_461 0.0300 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0304 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_451 0.0306 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_450 0.0327 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_458 0.0458 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.0465 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0507 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0509 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_454 0.0544 ms 17.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0994 seconds and 0.2126 seconds precompiling for 25 choices
[rank1]:W1101 10:58:48.449000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:48.493000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:48.592000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:48.715000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:48.759000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:48.859000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:48.986000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:49.021000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:49.127000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:49.255000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:49.292000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:49.395000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:49.489000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:49.523000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:49.534000 283 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3705d4b570>
[rank6]:W1101 10:58:49.553000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:49.557000 283 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3705d4b090>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:58:49 TP0] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1101 10:58:49.663000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:49.757000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:49.792000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:49.818000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:49.914000 290 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f902f00aeb0>
[rank3]:W1101 10:58:49.936000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:49.937000 290 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f902f00ad90>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:58:50 TP7] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1101 10:58:50.025000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:50.035000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:50.059000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:50.081000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:50.208000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:50.293000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:50.319000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:50.345000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:50.354000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:50.480000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:50.561000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:50.599000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:50.617000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:50.627000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:50.752000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:50.829000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:50.872000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:50.881000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:50.895000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:51.023000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:51.059000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:51.096000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:51.145000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:51.152000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:51.171000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:51.293000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:51.324000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:51.364000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:51.417000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:51.424000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:51.436000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:51.444000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_450 0.0315 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_451 0.0321 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_460 0.0329 ms 28.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_461 0.0330 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.0499 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.0499 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.0507 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0508 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_455 0.0592 ms 15.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.9083 seconds and 0.0000 seconds precompiling for 25 choices
[rank3]:W1101 10:58:51.561000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:51.588000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:51.636000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:51.684000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:51.692000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:51.699000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:51.714000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:51.836000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:51.851000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:51.909000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:51.961000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:51.966000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:51.975000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:51.984000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:52.117000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:52.124000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:52.176000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:52.220000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:52.241000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:52.249000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:52.257000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:52.389000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:52.396000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:52.448000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:52.484000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:52.521000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:52.529000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:52.537000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:52.657000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:52.664000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:52.720000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:52.752000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:52.785000 288 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15989867c0>
[rank2]:W1101 10:58:52.790000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:52.798000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:52.809000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:52.809000 288 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1598986700>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 10:58:52 TP5] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1101 10:58:52.925000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:52.933000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:52.992000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:53.016000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:53.061000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:53.069000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:53.078000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:53.193000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:53.200000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:53.264000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:53.280000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:53.329000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:53.338000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:53.348000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:53.461000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:53.468000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:53.536000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:53.544000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:53.598000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:53.605000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:53.617000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:53.729000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:53.736000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:53.808000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:53.814000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:53.865000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:53.874000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:53.888000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:54.009000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:54.016000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:54.080000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:54.086000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:54.136000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:54.144000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:54.162000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:54.288000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:54.293000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:54.313000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:54.353000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:54.360000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:54.407000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:54.414000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:54.430000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:54.559000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:54.565000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:54.581000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:54.617000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:54.629000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:54.679000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:54.686000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:54.697000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:54.833000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:54.840000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:54.848000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:54.880000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:54.896000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:54.953000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:54.961000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:54.971000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:55.105000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:55.112000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:55.119000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:55.149000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:55.169000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:55.219000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:55.227000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:55.241000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:55.375000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:55.382000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:55.389000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:55.417000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:55.441000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:55.491000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:55.498000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:55.509000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:55.648000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:55.653000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:55.660000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:55.681000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:55.709000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:55.765000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:55.775000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:55.783000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:55.921000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:55.928000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:55.935000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:55.944000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:55.976000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:56.038000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:56.046000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:56.056000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:56.197000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:56.203000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:56.211000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:56.219000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:56.249000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:56.307000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:56.314000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:56.326000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:56.465000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:56.475000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:56.482000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:56.490000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:56.517000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:56.579000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:56.587000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:56.596000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:56.732000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:56.749000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:56.755000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:56.763000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:56.785000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:56.863000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:56.872000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:56.878000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:56.997000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:57.023000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:57.030000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:57.038000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:57.053000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:57.135000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:57.145000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:57.151000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:57.261000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:57.296000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:57.305000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:57.311000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:57.321000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:57.407000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:57.416000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:57.423000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:57.525000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:57.567000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:57.575000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:57.584000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:57.591000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:57.679000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:57.688000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:57.695000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:57.789000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:57.841000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:57.846000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:57.855000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:57.863000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:57.951000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:57.959000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:57.967000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:58.053000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:58.105000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:58.119000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:58.127000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:58.135000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:58.223000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:58.231000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:58.239000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:58.317000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:58.373000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:58.387000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:58.399000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:58.407000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:58.495000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:58.504000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:58.511000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:58.581000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:58.642000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:58.659000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:58.672000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:58.679000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:58.774000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:58.780000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:58.795000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:58.844000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:58.908000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:58.928000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:58.945000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:58.952000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:59.044000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:59.057000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:59.069000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:59.108000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:59.176000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:59.196000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:59.218000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:59.225000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:59.316000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:59.329000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:59.340000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:59.376000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:59.444000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:59.476000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:59.500000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:59.507000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:59.588000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:59.605000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:58:59.614000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:59.644000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:58:59.712000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:58:59.745000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:58:59.772000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:58:59.780000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:58:59.861000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:58:59.878000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:58:59.913000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:00.021000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:00.052000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:00.060000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:00.136000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:00.157000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:00.184000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:00.297000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:00.328000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:00.342000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:00.412000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:00.437000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:00.448000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:00.460000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:00.573000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:00.621000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:00.688000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:00.713000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:00.725000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:00.732000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:00.785000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:00.852000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:00.905000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:00.964000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:00.997000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:01.004000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:01.010000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:01.057000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:01.068000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:01.120000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:01.181000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:01.240000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:01.269000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:01.275000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:01.283000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:01.331000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:01.344000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:01.388000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:01.465000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:01.516000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:01.543000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:01.553000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:01.559000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:01.614000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:01.621000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:01.656000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:01.737000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:01.792000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:01.811000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:01.826000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:01.832000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:01.885000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:01.896000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:01.924000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:02.009000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:02.067000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:02.079000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:02.097000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:02.104000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:02.157000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:02.172000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:02.192000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:02.281000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:02.340000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:02.347000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:02.369000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:02.376000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:02.429000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:02.448000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:02.460000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:02.553000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:02.616000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:02.621000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:02.642000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:02.648000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:02.701000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:02.724000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:02.731000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:02.832000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:02.892000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:02.924000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:02.994000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:03.005000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:03.126000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:03.164000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:03.200000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:03.278000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:03.284000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:03.348000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:03.417000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:03.436000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:03.457000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:03.472000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:03.492000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:03.553000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:03.564000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:03.624000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:03.704000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:03.733000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:03.764000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:03.829000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:03.840000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:03.900000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:03.972000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:04.009000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:04.036000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:04.116000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:04.176000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:04.244000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:04.291000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:04.313000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:04.406000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:04.462000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:04.521000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:04.575000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:04.595000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:04.686000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:04.742000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:04.793000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:04.855000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:04.875000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:04.968000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:05.020000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:05.071000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:05.145000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:05.160000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:05.244000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:05.296000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:05.339000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:05.424000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:05.436000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:05.568000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:05.607000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:05.692000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:05.708000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:05.844000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:05.875000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:05.956000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:05.972000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:06.130000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:06.145000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:06.243000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:06.410000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:06.514000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:06.689000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:06.783000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:06.833000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:06.978000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:07.114000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:07.255000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:07.383000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:07.658000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:07.932000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:08.208000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:08.480000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:08.747000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:09.020000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:09.288000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0610 ms 100.0% 
  triton_mm_475 0.1040 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_474 0.1041 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_485 0.1042 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_484 0.1044 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_492 0.1090 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1092 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1128 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1129 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_478 0.1408 ms 43.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1064 seconds and 0.1981 seconds precompiling for 25 choices
[rank5]:W1101 10:59:09.567000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_474 0.1039 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_475 0.1039 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_485 0.1041 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_484 0.1041 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_492 0.1086 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1087 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1124 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1124 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_479 0.1405 ms 43.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1010 seconds and 0.2011 seconds precompiling for 25 choices
[rank5]:W1101 10:59:09.839000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0612 ms 100.0% 
  triton_mm_475 0.1043 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_474 0.1045 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_485 0.1045 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_484 0.1046 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1089 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1090 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_482 0.1128 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1129 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_478 0.1409 ms 43.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1084 seconds and 0.1811 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0609 ms 100.0% 
  triton_mm_474 0.1044 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_475 0.1045 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_484 0.1046 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_485 0.1046 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1091 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1091 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1123 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1125 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_479 0.1405 ms 43.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0907 seconds and 0.2092 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_475 0.1044 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_474 0.1045 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_484 0.1046 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_485 0.1046 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1088 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1088 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1119 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1120 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_478 0.1398 ms 43.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0910 seconds and 0.1917 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0609 ms 100.0% 
  triton_mm_474 0.1044 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_475 0.1044 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_485 0.1047 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_484 0.1047 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_492 0.1090 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1091 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1126 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1128 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_478 0.1407 ms 43.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0454 seconds and 0.1948 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0608 ms 100.0% 
  triton_mm_474 0.1039 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_475 0.1039 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_485 0.1043 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_484 0.1043 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1089 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1090 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1128 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1128 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_478 0.1401 ms 43.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0898 seconds and 0.1834 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0616 ms 100.0% 
  triton_mm_485 0.1042 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_474 0.1045 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_475 0.1046 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_484 0.1047 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1087 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1088 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_482 0.1135 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1135 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_478 0.1415 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.9959 seconds and 0.0001 seconds precompiling for 25 choices
Capturing batches (bs=8 avail_mem=39.65 GB):  94%|| 49/52 [07:19<03:20, 66.68s/it]Capturing batches (bs=4 avail_mem=39.03 GB):  94%|| 49/52 [07:19<03:20, 66.68s/it][rank6]:W1101 10:59:24.372000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:24.449000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:24.509000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:24.523000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:24.539000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:24.557000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:24.588000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:24.602000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:24.616000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:24.697000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:24.711000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:24.724000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:25.036000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:25.116000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:25.227000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:25.289000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:25.369000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:25.478000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:25.541000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:25.619000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:25.728000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:25.883000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:25.962000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:26.070000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:26.074000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:26.227000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:26.250000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:26.266000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:26.738000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:27.039000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:27.262000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:27.602000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:28.166000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:28.248000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:28.300000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:28.380000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:28.459000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:28.509000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:28.601000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:28.646000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:28.679000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:28.720000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:28.726000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:28.729000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:28.777000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:28.798000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:28.848000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:29.017000 285 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank7]:W1101 10:59:29.092000 290 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank1]:W1101 10:59:29.096000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:29.184000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:29.193000 289 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank1]:W1101 10:59:29.241000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:29.252000 283 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank4]:W1101 10:59:29.325000 287 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank3]:W1101 10:59:29.373000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:29.388000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:29.462000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:29.472000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:29.517000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:29.524000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:29.736000 284 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank5]:W1101 10:59:29.992000 288 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank3]:W1101 10:59:29.997000 286 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_506 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_518 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_519 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_517 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_502 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_503 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_507 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_508 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_509 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_497 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.7733 seconds and 0.1540 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_510 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_512 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_514 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_516 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_508 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_511 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_513 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_515 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_503 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_509 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8253 seconds and 0.1488 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_508 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_514 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_499 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_507 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_511 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_496 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_509 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_513 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_501 0.0063 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_502 0.0063 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8574 seconds and 0.1491 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_501 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 99.4% 
  triton_bmm_502 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_503 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_516 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_500 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_515 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_497 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_499 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8072 seconds and 0.1591 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_499 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_502 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_508 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_509 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_511 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_513 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_507 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_510 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_501 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9030 seconds and 0.1473 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_498 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_499 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_514 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_515 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_502 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_509 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_500 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_501 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_503 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_513 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7683 seconds and 0.1408 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_501 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_512 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_498 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_513 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_514 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0063 ms 98.8% 
  triton_bmm_499 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_515 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8350 seconds and 0.1480 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_499 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_498 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_496 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_507 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_497 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_500 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_501 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_509 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_506 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9401 seconds and 0.1410 seconds precompiling for 25 choices
[rank6]:W1101 10:59:39.143000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:39.170000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:39.644000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:39.678000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:40.202000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:40.229000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:40.517000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:40.674000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:40.702000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:40.726000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:40.962000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:41.029000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:41.182000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:41.360000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:41.484000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:41.850000 289 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank1]:W1101 10:59:41.869000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:42.227000 285 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank3]:W1101 10:59:43.024000 286 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank7]:W1101 10:59:43.316000 290 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank0]:W1101 10:59:43.492000 283 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank4]:W1101 10:59:43.677000 287 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank5]:W1101 10:59:43.698000 288 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank1]:W1101 10:59:45.354000 284 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_523 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_522 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_533 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0071 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0071 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_526 0.0079 ms 81.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0079 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_542 0.0082 ms 78.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8893 seconds and 0.4032 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  triton_bmm_523 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0064 ms 98.7% 
  triton_bmm_522 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_532 0.0066 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0066 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0070 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0070 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0079 ms 79.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0080 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_542 0.0082 ms 77.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7924 seconds and 0.4248 seconds precompiling for 25 choices
[rank6]:W1101 10:59:48.096000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:48.175000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:48.225000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x4x512, 16x512x128)
  triton_bmm_522 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0065 ms 98.1% 
  triton_bmm_523 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_533 0.0069 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0069 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0073 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0074 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_527 0.0082 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_526 0.0082 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_542 0.0083 ms 76.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8482 seconds and 0.4097 seconds precompiling for 25 choices
[rank2]:W1101 10:59:48.533000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:48.623000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:48.674000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x4x512, 16x512x128)
  triton_bmm_522 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_523 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 97.5% 
  triton_bmm_532 0.0067 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0067 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0071 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0071 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0079 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0079 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_524 0.0081 ms 77.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8700 seconds and 0.4335 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  triton_bmm_522 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0065 ms 99.4% 
  triton_bmm_523 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_533 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0071 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0071 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_527 0.0081 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_526 0.0081 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_543 0.0083 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8154 seconds and 0.4035 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0063 ms 100.0% 
  triton_bmm_523 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_522 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_532 0.0067 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0067 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0071 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0071 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_527 0.0081 ms 78.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_526 0.0082 ms 77.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_542 0.0082 ms 77.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8508 seconds and 0.4102 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_522 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_523 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_532 0.0070 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0070 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0079 ms 82.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0079 ms 81.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0083 ms 78.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0083 ms 78.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_543 0.0085 ms 76.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8940 seconds and 0.3972 seconds precompiling for 25 choices
[rank3]:W1101 10:59:49.335000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:49.413000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:49.473000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:49.701000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:49.778000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:49.786000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:49.828000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:49.864000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:49.900000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:49.914000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:49.922000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:49.978000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:49.999000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:50.028000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:50.048000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_522 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_523 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_532 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0069 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_531 0.0075 ms 86.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0075 ms 85.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_526 0.0079 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0079 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_542 0.0085 ms 75.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8546 seconds and 0.4046 seconds precompiling for 25 choices
[rank1]:W1101 10:59:51.572000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:51.650000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:51.700000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:53.900000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:53.977000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:54.078000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:54.444000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:54.522000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:54.626000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:55.161000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:55.248000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:55.358000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:55.494000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:55.571000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:55.676000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:55.748000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:55.757000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:55.826000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:55.834000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:55.929000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:55.936000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:56.511000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:56.589000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:56.690000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:57.270000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:57.394000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:57.471000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:57.539000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 10:59:57.582000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:57.807000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:57.814000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:58.088000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:58.132000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 10:59:58.356000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:58.407000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 10:59:58.672000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:58.713000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:58.864000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:58.985000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:59.091000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:59.137000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 10:59:59.252000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:59.376000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 10:59:59.408000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:59.459000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:59.537000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 10:59:59.638000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 10:59:59.661000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 10:59:59.901000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:00.000000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:00.079000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:00.179000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:00.182000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:00.329000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:00.414000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:00.461000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:00.517000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:00.751000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:00.781000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:00.831000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:00.927000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:00.936000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:01.013000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:01.058000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:01.122000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:01.176000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:01.256000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:01.336000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:01.366000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:02.131000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:02.210000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:02.314000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:02.997000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:03.079000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:03.181000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_556 0.0294 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_557 0.0297 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_547 0.0324 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_546 0.0329 ms 27.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_555 0.0449 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0449 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_564 0.0479 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0479 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_551 0.0542 ms 16.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1544 seconds and 0.2210 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_557 0.0307 ms 29.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_556 0.0309 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_547 0.0331 ms 27.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_546 0.0340 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_555 0.0450 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0464 ms 19.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0481 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0482 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_551 0.0578 ms 15.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0617 seconds and 0.2278 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_557 0.0304 ms 30.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_556 0.0310 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_547 0.0345 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_546 0.0348 ms 26.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_554 0.0482 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_564 0.0489 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_555 0.0493 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_565 0.0494 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_551 0.0587 ms 15.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0903 seconds and 0.2203 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_557 0.0317 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_547 0.0319 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_556 0.0320 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_546 0.0330 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_554 0.0479 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_564 0.0484 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0485 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_555 0.0489 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_551 0.0595 ms 15.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1678 seconds and 0.2250 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_547 0.0294 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_556 0.0296 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_557 0.0296 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_546 0.0313 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_554 0.0450 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_555 0.0451 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0481 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0481 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_550 0.0538 ms 17.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0795 seconds and 0.2204 seconds precompiling for 25 choices
[rank6]:W1101 11:00:07.096000 289 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f835961f360>
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_547 0.0293 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_557 0.0295 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_556 0.0296 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_546 0.0312 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_554 0.0449 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_555 0.0449 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_565 0.0479 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0481 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_551 0.0538 ms 17.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1119 seconds and 0.2297 seconds precompiling for 25 choices
[rank6]:W1101 11:00:07.118000 289 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f835811b5d0>
[rank2]:W1101 11:00:07.168000 285 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edfb1cc7c60>
[rank2]:W1101 11:00:07.191000 285 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edfb1cc75d0>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:00:07 TP2] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1101 11:00:07.518000 286 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7d9d213540>
[rank3]:W1101 11:00:07.541000 286 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7d9d213570>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:00:07 TP3] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:00:07 TP6] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_556 0.0296 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_557 0.0297 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_547 0.0307 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_546 0.0320 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_554 0.0451 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_555 0.0453 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0478 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0480 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_550 0.0540 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0063 seconds and 0.2195 seconds precompiling for 25 choices
[rank5]:W1101 11:00:08.012000 288 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15989867c0>
[rank5]:W1101 11:00:08.034000 288 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1598986700>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:00:08 TP5] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1101 11:00:08.123000 290 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f902f00aeb0>
[rank7]:W1101 11:00:08.146000 290 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f902f00ad90>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:00:08 TP7] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1101 11:00:08.417000 287 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f14db3c3090>
[rank4]:W1101 11:00:08.440000 287 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f14da4338d0>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:00:08 TP4] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1101 11:00:08.695000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_557 0.0296 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_556 0.0304 ms 30.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_547 0.0347 ms 26.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_546 0.0349 ms 26.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_555 0.0461 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0470 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_564 0.0482 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0483 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_551 0.0599 ms 15.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0924 seconds and 0.2242 seconds precompiling for 25 choices
[rank6]:W1101 11:00:09.142000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:09.241000 283 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3705d4b570>
[rank0]:W1101 11:00:09.264000 283 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3705d4b090>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:00:09 TP0] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1101 11:00:09.421000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:09.544000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:09.593000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:09.694000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:09.832000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:09.880000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:09.966000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:10.077000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:10.116000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:10.160000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:10.198000 284 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eef01a8aee0>
[rank1]:W1101 11:00:10.221000 284 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eef01a8ad00>
[rank6]:W1101 11:00:10.242000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:10.258000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:00:10 TP1] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1101 11:00:10.353000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:10.395000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:10.440000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:10.518000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:10.542000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:10.558000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:10.628000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:10.680000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:10.721000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:10.794000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:10.822000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:10.837000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:10.901000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:10.972000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:11.012000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:11.072000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:11.098000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:11.114000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:11.173000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:11.255000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:11.292000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:11.339000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:11.353000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:11.373000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:11.390000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:11.449000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:11.536000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:11.572000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:11.615000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:11.629000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:11.655000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:11.666000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:11.721000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:11.828000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:11.864000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:11.903000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:11.912000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:11.934000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:11.943000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:12.001000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:12.112000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:12.144000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:12.179000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:12.187000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:12.210000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:12.218000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:12.265000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:12.274000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:12.396000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:12.424000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:12.455000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:12.463000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:12.490000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:12.497000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:12.544000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:12.552000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:12.680000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:12.704000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:12.735000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:12.743000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:12.769000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:12.777000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:12.820000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:12.828000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:12.962000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:12.986000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:13.013000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:13.020000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:13.044000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:13.052000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:13.097000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:13.105000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:13.242000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:13.261000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:13.289000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:13.296000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:13.325000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:13.332000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:13.373000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:13.382000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:13.522000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:13.538000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:13.565000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:13.572000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:13.604000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:13.612000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:13.649000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:13.660000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:13.801000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:13.814000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:13.841000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:13.849000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:13.885000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:13.892000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:13.925000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:13.936000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:14.080000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:14.088000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:14.115000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:14.126000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:14.166000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:14.174000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:14.200000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:14.213000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:14.368000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:14.376000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:14.399000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:14.407000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:14.445000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:14.453000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:14.485000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:14.492000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:14.650000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:14.659000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:14.677000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:14.684000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:14.721000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:14.728000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:14.756000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:14.769000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:14.930000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:14.938000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:14.953000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:14.960000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:15.000000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:15.007000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:15.028000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:15.045000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:15.206000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:15.214000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:15.225000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:15.236000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:15.280000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:15.287000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:15.300000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:15.321000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:15.482000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:15.490000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:15.501000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:15.512000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:15.560000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:15.567000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:15.576000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:15.593000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:15.758000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:15.769000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:15.777000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:15.788000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:15.840000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:15.847000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:15.855000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:15.873000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:16.036000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:16.044000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:16.051000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:16.065000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:16.122000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:16.130000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:16.138000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:16.151000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:16.328000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:16.336000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:16.343000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:16.349000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:16.401000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:16.410000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:16.418000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:16.431000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:16.608000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:16.617000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:16.623000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:16.630000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:16.678000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:16.687000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:16.695000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:16.707000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:16.887000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:16.895000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:16.904000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:16.909000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:16.957000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:16.965000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:16.975000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:16.984000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:17.167000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:17.175000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:17.184000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:17.189000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:17.238000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:17.245000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:17.255000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:17.263000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:17.449000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:17.455000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:17.464000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:17.469000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:17.518000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:17.527000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:17.535000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:17.543000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:17.725000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:17.736000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:17.744000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:17.749000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:17.798000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:17.807000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:17.815000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:17.823000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:18.001000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:18.027000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:18.036000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:18.041000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:18.078000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:18.087000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:18.095000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:18.107000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:18.277000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:18.308000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:18.316000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:18.321000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:18.358000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:18.366000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:18.375000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:18.383000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:18.553000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:18.588000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:18.596000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:18.601000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:18.638000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:18.646000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:18.656000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:18.664000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:18.828000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:18.869000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:18.879000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:18.886000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:18.917000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:18.923000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:18.931000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:18.941000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:19.108000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:19.149000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:19.158000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:19.165000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:19.200000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:19.209000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:19.216000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:19.223000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:19.388000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:19.429000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:19.439000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:19.447000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:19.480000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:19.489000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:19.497000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:19.505000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:19.668000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:19.718000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:19.726000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:19.734000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:19.760000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:19.769000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:19.777000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:19.785000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:19.948000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:20.005000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:20.014000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:20.022000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:20.045000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:20.052000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:20.059000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:20.069000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:20.230000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:20.283000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:20.292000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:20.297000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:20.330000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:20.338000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:20.348000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:20.355000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:20.509000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:20.564000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:20.571000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:20.577000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:20.611000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:20.618000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:20.628000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:20.636000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:20.784000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:20.850000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:20.858000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:20.865000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:20.889000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:20.896000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:20.904000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:20.914000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:21.060000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:21.129000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:21.138000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:21.146000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:21.173000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:21.180000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:21.187000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:21.196000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:21.340000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:21.410000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:21.418000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:21.426000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:21.457000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:21.464000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:21.471000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:21.480000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:21.620000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:21.690000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:21.698000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:21.706000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:21.741000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:21.748000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:21.756000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:21.764000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:21.900000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:21.969000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:21.978000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:21.985000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:22.024000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:22.032000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:22.039000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:22.048000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:22.180000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:22.250000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:22.258000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:22.266000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:22.308000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:22.316000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:22.324000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:22.332000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:22.460000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:22.541000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:22.551000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:22.558000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:22.592000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:22.599000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:22.607000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:22.617000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:22.740000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:22.820000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:22.828000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:22.835000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:22.879000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:22.887000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:22.895000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:22.904000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:23.022000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:23.104000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:23.110000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:23.116000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:23.162000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:23.171000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:23.179000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:23.187000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:23.302000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:23.387000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:23.394000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:23.400000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:23.443000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:23.451000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:23.461000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:23.472000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:23.582000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:23.672000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:23.678000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:23.684000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:23.730000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:23.739000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:23.747000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:23.756000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:23.861000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:23.956000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:23.962000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:23.968000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:24.010000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:24.019000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:24.028000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:24.040000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:24.138000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:24.240000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:24.246000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:24.252000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:24.294000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:24.302000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:24.311000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:24.320000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:24.414000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:24.523000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:24.530000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:24.536000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:24.578000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:24.586000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:24.595000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:24.603000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:24.690000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:24.808000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:24.814000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:24.820000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:24.861000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:24.869000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:24.879000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:24.887000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:25.091000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:25.098000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:25.103000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:25.143000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:25.152000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:25.168000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:25.176000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:25.380000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:25.387000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:25.426000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:25.434000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:25.445000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:25.456000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:25.667000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:25.706000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:25.715000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:25.725000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:25.736000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:25.956000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:26.003000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:26.011000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:26.026000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:26.240000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:26.298000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:26.313000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:26.528000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:26.598000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:26.823000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:26.882000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:27.113000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:27.178000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:27.461000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:27.743000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:28.026000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0607 ms 100.0% 
  triton_mm_570 0.1041 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_571 0.1042 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_580 0.1042 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_581 0.1042 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_588 0.1082 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1083 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1117 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1119 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_574 0.1333 ms 45.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0673 seconds and 0.2043 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0601 ms 100.0% 
  triton_mm_570 0.1043 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_571 0.1043 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_581 0.1043 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_580 0.1043 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_588 0.1079 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1080 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1112 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1114 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_575 0.1324 ms 45.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1173 seconds and 0.1843 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0607 ms 100.0% 
  triton_mm_570 0.1044 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_571 0.1045 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_581 0.1046 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_580 0.1047 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1085 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_588 0.1086 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_578 0.1123 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1124 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_574 0.1337 ms 45.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1061 seconds and 0.1855 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_570 0.1044 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_580 0.1044 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_581 0.1045 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_571 0.1045 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_589 0.1083 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_588 0.1083 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_578 0.1118 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1120 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_575 0.1332 ms 45.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1151 seconds and 0.1885 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_570 0.1039 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_571 0.1040 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_581 0.1041 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_580 0.1042 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_588 0.1081 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1083 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_579 0.1115 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1115 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_575 0.1328 ms 45.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0906 seconds and 0.1857 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_570 0.1043 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_571 0.1044 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_581 0.1045 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_580 0.1045 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_588 0.1086 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1087 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_579 0.1117 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1118 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_575 0.1333 ms 45.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0898 seconds and 0.1844 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0605 ms 100.0% 
  triton_mm_580 0.1043 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_581 0.1043 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_570 0.1045 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_571 0.1045 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_588 0.1087 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1088 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_579 0.1116 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1118 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_574 0.1333 ms 45.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0649 seconds and 0.1917 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0607 ms 100.0% 
  triton_mm_570 0.1039 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_571 0.1040 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_581 0.1041 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_580 0.1041 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_588 0.1094 ms 55.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1096 ms 55.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1121 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1124 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_575 0.1336 ms 45.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0739 seconds and 0.2027 seconds precompiling for 25 choices
Capturing batches (bs=4 avail_mem=39.03 GB):  96%|| 50/52 [08:36<02:19, 69.74s/it]Capturing batches (bs=2 avail_mem=38.41 GB):  96%|| 50/52 [08:36<02:19, 69.74s/it][rank7]:W1101 11:00:41.393000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:41.400000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:41.474000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:41.479000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:41.587000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:41.587000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:41.926000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:42.005000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:42.014000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:42.019000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:42.093000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:42.100000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:42.116000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:42.125000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:42.151000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:42.202000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:42.204000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:42.212000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:42.230000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:42.272000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:42.315000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:42.339000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:42.351000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:42.461000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:43.143000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:43.158000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:43.649000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:43.754000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:43.790000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:43.847000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:43.855000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:43.983000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:44.559000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:44.640000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:44.692000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:44.838000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:44.919000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:44.972000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:45.333000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:45.410000 284 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank0]:W1101 11:00:45.410000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:45.413000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:45.433000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:45.462000 290 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank4]:W1101 11:00:45.465000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:45.494000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:45.516000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:45.553000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:45.573000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:45.805000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:45.824000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:45.885000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:45.904000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:45.935000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:45.955000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:45.962000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:45.964000 287 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank0]:W1101 11:00:46.040000 283 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank2]:W1101 11:00:46.045000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:46.052000 289 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank2]:W1101 11:00:46.117000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:46.422000 288 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank3]:W1101 11:00:46.468000 286 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank2]:W1101 11:00:46.614000 285 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_607 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_615 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_603 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_605 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_606 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_608 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_609 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_610 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_611 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_612 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8449 seconds and 0.1307 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  bmm 0.0062 ms 100.0% 
  triton_bmm_599 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_603 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_605 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_611 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_613 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_615 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_604 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_606 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_607 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9175 seconds and 0.1385 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_594 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_595 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_597 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_602 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_605 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_610 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_614 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_596 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_598 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_599 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9280 seconds and 0.1551 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_597 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_598 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_603 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_604 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_607 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_610 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_595 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_594 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_596 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_599 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9109 seconds and 0.1395 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_605 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_602 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_611 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_598 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_604 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_606 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_607 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_609 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_610 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0064 ms 98.8% 
SingleProcess AUTOTUNE benchmarking takes 4.9426 seconds and 0.1381 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_603 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 100.0% 
  triton_bmm_594 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_595 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_596 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_597 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_598 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_599 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_604 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_605 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8862 seconds and 0.1639 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_599 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_604 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_596 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_597 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_598 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_602 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_603 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_605 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_607 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 99.4% 
SingleProcess AUTOTUNE benchmarking takes 4.8944 seconds and 0.1570 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_604 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_599 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_602 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_603 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_595 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_597 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_598 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_605 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_614 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_607 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9113 seconds and 0.1315 seconds precompiling for 25 choices
[rank1]:W1101 11:00:55.494000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:55.770000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:55.994000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:55.994000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:56.055000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:00:56.282000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:56.397000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:00:56.505000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:00:56.558000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:56.891000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:00:56.904000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:56.958000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:57.096000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:00:57.409000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:00:57.457000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:00:57.624000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:00:58.521000 284 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank7]:W1101 11:00:58.541000 290 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank4]:W1101 11:00:58.809000 287 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank0]:W1101 11:00:58.954000 283 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank6]:W1101 11:00:59.168000 289 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank5]:W1101 11:00:59.901000 288 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank3]:W1101 11:00:59.942000 286 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank2]:W1101 11:01:00.391000 285 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_618 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0064 ms 100.0% 
  triton_bmm_619 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_628 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_627 0.0071 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0072 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_622 0.0079 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0080 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_638 0.0083 ms 77.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7999 seconds and 0.3971 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_618 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_619 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 96.3% 
  triton_bmm_628 0.0066 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0066 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_627 0.0069 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0069 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_622 0.0077 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0077 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_620 0.0081 ms 77.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8952 seconds and 0.4003 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_618 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_619 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_629 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_626 0.0071 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0071 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_622 0.0080 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0080 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_639 0.0083 ms 77.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8697 seconds and 0.4361 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_618 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_619 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_628 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_627 0.0070 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0071 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_622 0.0080 ms 79.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0080 ms 79.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_638 0.0083 ms 77.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7788 seconds and 0.4177 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_618 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_619 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0068 ms 96.5% 
  triton_bmm_628 0.0069 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0069 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_627 0.0072 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0072 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_622 0.0080 ms 81.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0080 ms 81.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_638 0.0083 ms 78.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9274 seconds and 0.4091 seconds precompiling for 25 choices
[rank7]:W1101 11:01:04.922000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:05.002000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:05.053000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:05.074000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:05.109000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:05.153000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:05.187000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x2x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_619 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_618 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_628 0.0069 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0071 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0077 ms 83.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0077 ms 83.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_622 0.0081 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0081 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_639 0.0086 ms 75.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7836 seconds and 0.4360 seconds precompiling for 25 choices
[rank4]:W1101 11:01:05.204000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:05.235000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:05.237000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:05.314000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:05.364000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_618 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_619 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 99.4% 
  triton_bmm_628 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0072 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0072 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_622 0.0079 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0079 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_639 0.0083 ms 78.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9381 seconds and 0.4361 seconds precompiling for 25 choices
[rank6]:W1101 11:01:05.548000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:05.626000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:05.677000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x2x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_619 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_618 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_628 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_627 0.0070 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0071 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_623 0.0079 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_622 0.0080 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_638 0.0083 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9157 seconds and 0.4001 seconds precompiling for 25 choices
[rank3]:W1101 11:01:06.376000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:06.454000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:06.503000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:06.510000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:06.590000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:06.640000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:06.673000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:06.751000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:06.801000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:10.215000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:10.294000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:10.406000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:10.897000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:10.964000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:10.978000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:11.044000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:11.084000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:11.084000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:11.150000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:11.163000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:11.256000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:11.267000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:11.335000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:11.440000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:11.471000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:11.551000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:11.654000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:12.298000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:12.376000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:12.479000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:12.538000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:12.616000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:12.719000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:13.171000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:13.451000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:13.721000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:14.244000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:14.359000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:14.473000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:14.524000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:14.632000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:14.645000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:14.757000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:14.812000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:14.912000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:14.928000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:15.033000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:15.139000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:15.188000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:15.222000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:15.300000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:15.401000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:15.422000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:15.644000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:15.698000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:15.902000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:15.924000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:16.194000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:16.209000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:16.349000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:16.430000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:16.482000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:16.535000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:16.547000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:16.611000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:16.626000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:16.691000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:16.728000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:16.795000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:16.864000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:16.946000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:17.056000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:17.252000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:17.337000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:17.442000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:17.717000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:17.795000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:17.901000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:18.009000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:18.090000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:18.192000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_643 0.0291 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_652 0.0301 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_653 0.0301 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_642 0.0313 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_650 0.0440 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_651 0.0443 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_661 0.0448 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0450 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_647 0.0540 ms 16.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1293 seconds and 0.2405 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0090 ms 100.0% 
  triton_mm_652 0.0291 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_653 0.0291 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_643 0.0302 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0313 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_650 0.0440 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_651 0.0443 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0448 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0453 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_647 0.0538 ms 16.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0985 seconds and 0.2173 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_653 0.0294 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0295 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_643 0.0350 ms 26.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0352 ms 26.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_650 0.0442 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_651 0.0444 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0447 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0452 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_662 0.0595 ms 15.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0576 seconds and 0.2368 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0090 ms 100.0% 
  triton_mm_652 0.0292 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_653 0.0293 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_643 0.0305 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0321 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_650 0.0443 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_651 0.0444 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_661 0.0444 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0444 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_647 0.0543 ms 16.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0247 seconds and 0.2205 seconds precompiling for 25 choices
[rank0]:W1101 11:01:22.533000 283 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3705d4b570>
[rank0]:W1101 11:01:22.556000 283 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3705d4b090>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:01:22 TP0] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0089 ms 100.0% 
  triton_mm_652 0.0292 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_653 0.0295 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_643 0.0326 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0335 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_660 0.0443 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_651 0.0445 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_650 0.0447 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0448 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_647 0.0556 ms 16.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1567 seconds and 0.2163 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0090 ms 100.0% 
  triton_mm_653 0.0303 ms 29.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0318 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_643 0.0346 ms 26.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0349 ms 25.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_660 0.0451 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_650 0.0453 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0469 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_651 0.0477 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_663 0.0601 ms 14.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1324 seconds and 0.2341 seconds precompiling for 25 choices
[rank4]:W1101 11:01:23.544000 287 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f14db3c3090>
[rank4]:W1101 11:01:23.567000 287 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f14da4338d0>
[rank1]:W1101 11:01:23.580000 284 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eef01a8aee0>
[rank7]:W1101 11:01:23.599000 290 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f902f00aeb0>
[rank1]:W1101 11:01:23.603000 284 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eef01a8ad00>
[rank7]:W1101 11:01:23.622000 290 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f902f00ad90>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:01:23 TP4] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:01:23 TP1] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_642 0.0323 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_652 0.0323 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_653 0.0326 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_643 0.0327 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_660 0.0467 ms 19.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0470 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_650 0.0484 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_651 0.0497 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_647 0.0594 ms 15.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1714 seconds and 0.2203 seconds precompiling for 25 choices
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:01:23 TP7] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_653 0.0295 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0296 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_643 0.0347 ms 26.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0349 ms 26.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_650 0.0440 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_660 0.0445 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0446 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_651 0.0453 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_663 0.0592 ms 15.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0738 seconds and 0.2394 seconds precompiling for 25 choices
[rank6]:W1101 11:01:23.951000 289 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f835961f360>
[rank6]:W1101 11:01:23.974000 289 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f835811b5d0>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:01:24 TP6] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1101 11:01:24.077000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:24.367000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:24.540000 286 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7d9d213540>
[rank3]:W1101 11:01:24.562000 286 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7d9d213570>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:01:24 TP3] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1101 11:01:24.647000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:24.833000 288 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15989867c0>
[rank5]:W1101 11:01:24.856000 288 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1598986700>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:01:24 TP5] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1101 11:01:24.925000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:24.977000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:25.061000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:25.065000 285 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edfb1cc7c60>
[rank2]:W1101 11:01:25.088000 285 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edfb1cc75d0>
[rank1]:W1101 11:01:25.126000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:01:25 TP2] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1101 11:01:25.210000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:25.267000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:25.322000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:25.346000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:25.408000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:25.494000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:25.554000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:25.601000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:25.625000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:25.706000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:25.782000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:25.841000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:25.881000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:25.909000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:26.006000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:26.066000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:26.074000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:26.129000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:26.160000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:26.192000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:26.288000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:26.348000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:26.355000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:26.370000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:26.418000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:26.442000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:26.478000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:26.572000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:26.596000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:26.632000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:26.639000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:26.654000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:26.701000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:26.721000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:26.761000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:26.859000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:26.883000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:26.919000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:26.937000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:26.985000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:27.001000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:27.044000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:27.150000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:27.174000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:27.206000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:27.225000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:27.274000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:27.281000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:27.330000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:27.436000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:27.460000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:27.492000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:27.514000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:27.562000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:27.572000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:27.578000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:27.614000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:27.726000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:27.746000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:27.779000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:27.801000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:27.837000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:27.857000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:27.866000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:27.897000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:28.047000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:28.071000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:28.085000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:28.117000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:28.158000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:28.337000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:28.361000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:28.379000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:28.448000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:28.625000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:28.670000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:28.680000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:28.742000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:28.857000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:28.889000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:28.918000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:28.952000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:28.970000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:29.033000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:29.061000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:29.149000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:29.176000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:29.206000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:29.236000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:29.258000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:29.298000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:29.321000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:29.347000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:29.439000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:29.466000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:29.522000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:29.549000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:29.590000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:29.612000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:29.631000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:29.731000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:29.755000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:29.852000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:29.880000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:29.900000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:29.919000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:30.031000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:30.050000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:30.144000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:30.164000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:30.171000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:30.187000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:30.206000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:30.327000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:30.342000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:30.438000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:30.454000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:30.463000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:30.472000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:30.481000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:30.492000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:30.621000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:30.633000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:30.730000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:30.742000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:30.750000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:30.763000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:30.772000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:30.781000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:30.914000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:30.924000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:31.018000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:31.027000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:31.037000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:31.051000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:31.059000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:31.069000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:31.210000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:31.217000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:31.306000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:31.315000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:31.324000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:31.339000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:31.348000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:31.357000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:31.502000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:31.509000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:31.594000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:31.603000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:31.612000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:31.626000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:31.636000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:31.645000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:31.794000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:31.800000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:31.882000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:31.891000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:31.899000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:31.914000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:31.924000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:31.933000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:32.085000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:32.092000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:32.170000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:32.180000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:32.187000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:32.202000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:32.212000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:32.221000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:32.377000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:32.383000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:32.456000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:32.464000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:32.474000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:32.489000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:32.497000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:32.507000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:32.670000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:32.678000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:32.744000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:32.752000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:32.762000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:32.777000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:32.785000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:32.794000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:32.961000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:32.967000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:33.038000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:33.047000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:33.054000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:33.067000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:33.077000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:33.086000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:33.257000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:33.264000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:33.338000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:33.346000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:33.354000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:33.368000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:33.376000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:33.383000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:33.554000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:33.562000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:33.624000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:33.634000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:33.641000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:33.652000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:33.663000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:33.670000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:33.846000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:33.854000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:33.912000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:33.920000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:33.928000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:33.937000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:33.947000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:33.955000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:34.137000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:34.145000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:34.205000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:34.212000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:34.221000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:34.229000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:34.238000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:34.246000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:34.426000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:34.434000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:34.489000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:34.504000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:34.511000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:34.520000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:34.529000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:34.537000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:34.718000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:34.726000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:34.773000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:34.792000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:34.799000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:34.807000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:34.818000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:34.825000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:35.006000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:35.014000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:35.057000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:35.080000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:35.087000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:35.096000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:35.105000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:35.113000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:35.294000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:35.302000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:35.341000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:35.380000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:35.387000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:35.397000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:35.404000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:35.412000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:35.585000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:35.592000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:35.624000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:35.670000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:35.681000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:35.688000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:35.699000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:35.707000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:35.877000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:35.883000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:35.908000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:35.966000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:35.975000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:35.984000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:35.993000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:36.001000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:36.168000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:36.175000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:36.192000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:36.254000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:36.263000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:36.273000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:36.281000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:36.291000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:36.461000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:36.467000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:36.476000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:36.542000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:36.551000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:36.560000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:36.570000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:36.579000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:36.752000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:36.759000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:36.765000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:36.830000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:36.839000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:36.849000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:36.858000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:36.866000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:37.045000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:37.051000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:37.058000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:37.130000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:37.138000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:37.146000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:37.156000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:37.165000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:37.337000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:37.343000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:37.350000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:37.418000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:37.428000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:37.436000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:37.445000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:37.455000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:37.628000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:37.635000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:37.641000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:37.706000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:37.715000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:37.724000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:37.735000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:37.743000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:37.920000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:37.927000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:37.933000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:37.994000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:38.003000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:38.012000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:38.022000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:38.031000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:38.212000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:38.218000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:38.225000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:38.282000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:38.291000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:38.301000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:38.309000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:38.319000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:38.508000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:38.515000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:38.522000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:38.570000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:38.580000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:38.588000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:38.597000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:38.607000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:38.800000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:38.807000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:38.813000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:38.858000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:38.867000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:38.877000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:38.885000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:38.895000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:39.088000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:39.100000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:39.106000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:39.145000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:39.156000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:39.164000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:39.173000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:39.184000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:39.376000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:39.392000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:39.398000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:39.446000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:39.454000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:39.462000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:39.472000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:39.482000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:39.664000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:39.684000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:39.690000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:39.734000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:39.743000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:39.752000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:39.761000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:39.771000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:39.952000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:39.976000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:39.982000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:40.022000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:40.031000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:40.039000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:40.049000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:40.059000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:40.240000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:40.268000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:40.274000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:40.310000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:40.318000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:40.327000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:40.337000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:40.347000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:40.528000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:40.560000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:40.566000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:40.598000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:40.606000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:40.616000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:40.625000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:40.635000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:40.816000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:40.852000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:40.858000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:40.886000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:40.895000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:40.904000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:40.913000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:40.923000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:41.104000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:41.144000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:41.151000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:41.174000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:41.184000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:41.193000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:41.211000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:41.392000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:41.436000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:41.443000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:41.463000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:41.472000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:41.480000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:41.498000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:41.680000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:41.729000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:41.736000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:41.750000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:41.760000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:41.768000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:41.782000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:41.969000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:42.026000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:42.033000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:42.043000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:42.053000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:42.062000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:42.075000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:42.261000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:42.354000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:42.366000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:42.548000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:42.638000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:42.650000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:42.838000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:42.926000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:42.956000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:43.141000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:43.249000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:43.533000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0601 ms 100.0% 
  triton_mm_676 0.1043 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_677 0.1043 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_667 0.1044 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_666 0.1045 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_684 0.1067 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1068 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1114 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1115 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_671 0.1292 ms 46.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0390 seconds and 0.2023 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_676 0.1042 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_677 0.1043 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_666 0.1044 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1044 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_684 0.1068 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1068 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1115 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1116 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_670 0.1289 ms 47.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0685 seconds and 0.1937 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0602 ms 100.0% 
  triton_mm_676 0.1040 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_677 0.1041 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_666 0.1042 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1042 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_684 0.1067 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1068 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1116 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1117 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_670 0.1292 ms 46.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1343 seconds and 0.1778 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0605 ms 100.0% 
  triton_mm_667 0.1039 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_666 0.1039 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_676 0.1040 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_677 0.1041 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_684 0.1068 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1069 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1118 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1118 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_670 0.1294 ms 46.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0956 seconds and 0.2462 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0601 ms 100.0% 
  triton_mm_676 0.1039 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_666 0.1040 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1040 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_677 0.1040 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_684 0.1066 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1067 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1112 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1113 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_671 0.1286 ms 46.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1505 seconds and 0.2166 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_676 0.1044 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_666 0.1045 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1045 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_677 0.1046 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_684 0.1071 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1078 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1121 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1121 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_671 0.1298 ms 46.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0918 seconds and 0.1881 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0605 ms 100.0% 
  triton_mm_677 0.1042 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_666 0.1043 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_676 0.1043 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_667 0.1045 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_685 0.1074 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_684 0.1075 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1116 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1117 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_670 0.1291 ms 46.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1377 seconds and 0.1867 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0600 ms 100.0% 
  triton_mm_666 0.1043 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1044 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_676 0.1044 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_677 0.1044 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_685 0.1070 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_684 0.1070 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1111 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1112 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_671 0.1283 ms 46.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0854 seconds and 0.1943 seconds precompiling for 25 choices
Capturing batches (bs=2 avail_mem=38.41 GB):  98%|| 51/52 [09:51<01:11, 71.43s/it]Capturing batches (bs=1 avail_mem=37.83 GB):  98%|| 51/52 [09:51<01:11, 71.43s/it][rank0]:W1101 11:01:56.185000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:56.268000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:56.381000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:56.435000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:56.515000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:56.570000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:56.624000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:56.643000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:56.649000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:56.668000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:56.694000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:56.696000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:56.722000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:56.748000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:56.757000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:56.775000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:56.775000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:56.802000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:56.835000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:56.858000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:56.881000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:56.886000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:56.894000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:56.990000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:57.890000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:58.118000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:58.243000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:58.314000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:58.327000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:58.335000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:58.351000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:01:58.456000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:59.465000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:59.521000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:59.548000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:59.583000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:59.604000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:01:59.656000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:59.667000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:01:59.712000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:59.744000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:01:59.774000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:59.802000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:59.825000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:59.884000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:59.906000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:59.914000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:01:59.930000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:01:59.988000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:01:59.988000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:01:59.996000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:00.011000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:00.092000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:00.092000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:00.099000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:00.196000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:05.256000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:05.303000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:05.337000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:05.345000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:05.399000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:05.477000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:05.769000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:05.773000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:05.811000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:05.835000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:05.850000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:05.900000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:05.924000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:05.978000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:06.280000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:06.461000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:08.841000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:08.920000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:08.971000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:09.042000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:09.063000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:09.122000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:09.145000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:09.173000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:09.196000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:09.964000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:09.973000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:10.045000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:10.053000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:10.095000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:10.105000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:10.465000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:10.478000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:10.545000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:10.558000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:10.596000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:10.610000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:10.893000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:10.974000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:11.027000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:13.109000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:13.190000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:13.296000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:13.781000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:13.862000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:13.968000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:14.156000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:14.236000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:14.343000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:14.857000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:14.869000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:14.879000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:14.936000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:14.948000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:14.961000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:15.039000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:15.051000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:15.066000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:15.114000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:15.194000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:15.298000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:15.376000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:15.455000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:15.558000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:15.923000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:16.211000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:16.499000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:16.809000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:16.835000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:17.093000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:17.130000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:17.296000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:17.370000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:17.420000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:17.510000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:17.588000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:17.773000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:17.794000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:17.871000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:17.881000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:17.952000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:18.056000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:18.058000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:18.066000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:18.076000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:18.290000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:18.352000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:18.361000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:18.582000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:18.645000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:18.789000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:18.870000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:18.874000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:18.974000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:19.053000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:19.134000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:19.237000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:19.259000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:19.342000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:19.447000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:19.718000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:19.798000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:19.902000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:20.034000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:20.114000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:20.216000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:20.316000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:20.397000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:20.501000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:20.558000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:20.638000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:20.741000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:22.167000 283 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3705d4b570>
[rank0]:W1101 11:02:22.191000 283 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3705d4b090>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:02:22 TP0] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1101 11:02:22.679000 287 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f14db3c3090>
[rank4]:W1101 11:02:22.702000 287 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f14da4338d0>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:02:22 TP4] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1101 11:02:22.891000 284 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eef01a8aee0>
[rank1]:W1101 11:02:22.914000 284 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eef01a8ad00>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:02:22 TP1] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1101 11:02:23.203000 286 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7d9d213540>
[rank3]:W1101 11:02:23.226000 286 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7d9d213570>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:02:23 TP3] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1101 11:02:23.622000 289 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f835961f360>
[rank5]:W1101 11:02:23.644000 288 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15989867c0>
[rank6]:W1101 11:02:23.645000 289 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f835811b5d0>
[rank5]:W1101 11:02:23.667000 288 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1598986700>
[rank0]:W1101 11:02:23.705000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:02:23 TP6] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:02:23 TP5] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1101 11:02:24.003000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:24.210000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:24.296000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:24.370000 285 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edfb1cc7c60>
[rank2]:W1101 11:02:24.394000 285 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edfb1cc75d0>
[rank1]:W1101 11:02:24.420000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:02:24 TP2] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1101 11:02:24.498000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:24.589000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:24.647000 290 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f902f00aeb0>
[rank7]:W1101 11:02:24.670000 290 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f902f00ad90>
[rank1]:W1101 11:02:24.712000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:02:24 TP7] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1101 11:02:24.748000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:24.790000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:24.884000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:25.004000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:25.014000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:25.040000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:25.082000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:25.171000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:25.177000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:25.303000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:25.313000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:25.333000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:25.374000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:25.467000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:25.475000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:25.596000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:25.614000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:25.625000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:25.666000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:25.736000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:25.754000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:25.771000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:25.888000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:25.907000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:25.917000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:25.958000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:26.033000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:26.043000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:26.064000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:26.177000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:26.186000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:26.197000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:26.210000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:26.253000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:26.330000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:26.338000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:26.357000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:26.473000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:26.481000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:26.490000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:26.502000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:26.549000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:26.622000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:26.632000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:26.645000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:26.769000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:26.777000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:26.787000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:26.796000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:26.849000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:26.914000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:26.923000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:26.933000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:27.065000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:27.074000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:27.082000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:27.092000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:27.142000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:27.204000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:27.216000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:27.223000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:27.359000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:27.366000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:27.376000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:27.384000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:27.438000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:27.497000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:27.507000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:27.516000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:27.650000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:27.660000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:27.670000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:27.678000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:27.730000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:27.793000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:27.802000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:27.812000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:27.941000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:27.954000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:27.963000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:27.974000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:28.021000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:28.081000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:28.099000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:28.108000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:28.239000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:28.248000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:28.258000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:28.267000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:28.318000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:28.370000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:28.388000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:28.397000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:28.536000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:28.544000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:28.553000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:28.562000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:28.614000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:28.658000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:28.696000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:28.703000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:28.830000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:28.842000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:28.850000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:28.860000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:28.909000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:28.945000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:28.994000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:29.005000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:29.125000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:29.138000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:29.145000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:29.156000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:29.205000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:29.233000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:29.286000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:29.297000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:29.421000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:29.430000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:29.438000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:29.450000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:29.505000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:29.521000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:29.590000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:29.600000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:29.717000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:29.725000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:29.733000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:29.744000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:29.800000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:29.809000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:29.886000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:29.895000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:30.013000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:30.021000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:30.029000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:30.039000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:30.097000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:30.104000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:30.177000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:30.185000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:30.310000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:30.317000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:30.328000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:30.336000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:30.394000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:30.403000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:30.473000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:30.481000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:30.606000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:30.614000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:30.625000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:30.633000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:30.690000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:30.699000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:30.768000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:30.777000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:30.902000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:30.912000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:30.921000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:30.930000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:30.986000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:30.995000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:31.068000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:31.076000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:31.198000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:31.208000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:31.217000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:31.226000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:31.282000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:31.291000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:31.364000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:31.371000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:31.504000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:31.514000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:31.523000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:31.530000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:31.584000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:31.593000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:31.660000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:31.667000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:31.800000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:31.814000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:31.824000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:31.831000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:31.882000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:31.891000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:31.956000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:31.963000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:32.096000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:32.110000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:32.119000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:32.129000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:32.178000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:32.187000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:32.258000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:32.268000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:32.394000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:32.405000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:32.413000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:32.426000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:32.477000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:32.484000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:32.553000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:32.566000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:32.690000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:32.701000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:32.709000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:32.722000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:32.773000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:32.780000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:32.844000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:32.860000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:32.992000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:33.002000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:33.011000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:33.029000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:33.070000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:33.079000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:33.136000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:33.156000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:33.290000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:33.299000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:33.307000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:33.334000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:33.365000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:33.372000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:33.441000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:33.466000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:33.586000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:33.597000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:33.604000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:33.630000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:33.661000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:33.668000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:33.733000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:33.762000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:33.881000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:33.899000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:33.908000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:33.925000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:33.967000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:33.976000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:34.062000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:34.185000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:34.195000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:34.204000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:34.229000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:34.262000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:34.272000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:34.365000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:34.484000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:34.494000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:34.505000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:34.529000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:34.571000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:34.665000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:34.795000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:34.807000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:34.828000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:34.837000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:34.867000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:34.973000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:35.112000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:35.133000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:35.281000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:35.383000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:35.423000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:35.436000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:35.586000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:35.596000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:35.687000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:35.727000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:35.736000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:35.897000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:35.919000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:35.933000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:35.975000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:35.987000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:36.040000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:36.201000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:36.223000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:36.233000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:36.274000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:36.295000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:36.342000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:36.507000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:36.521000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:36.535000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:36.569000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:36.597000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:36.641000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:36.702000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:36.806000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:36.814000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:36.821000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:36.834000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:36.865000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:36.897000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:36.936000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:37.001000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:37.100000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:37.114000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:37.123000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:37.133000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:37.162000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:37.198000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:37.233000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:37.300000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:37.411000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:37.419000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:37.426000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:37.446000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:37.457000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:37.497000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:37.534000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:37.602000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:37.710000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:37.721000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:37.728000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:37.747000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:37.755000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:37.797000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:37.834000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:37.906000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:38.005000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:38.022000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:38.031000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:38.046000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:38.054000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:38.098000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:38.128000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:38.209000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:38.304000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:38.322000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:38.331000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:38.344000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:38.353000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:38.398000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:38.428000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:38.513000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:38.608000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:38.622000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:38.631000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:38.645000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:38.653000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:38.698000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:38.728000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:38.812000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:38.912000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:38.922000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:38.931000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:38.945000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:38.953000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:38.998000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:39.028000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:39.112000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:39.216000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:39.226000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:39.234000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:39.245000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:39.253000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:39.298000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:39.328000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:39.423000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:39.522000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:39.530000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:39.537000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:39.548000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:39.558000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:39.597000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:39.625000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:39.726000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:39.822000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:39.833000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:39.840000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:39.848000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:39.858000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:39.901000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:39.921000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:40.029000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:40.120000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:40.134000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:40.145000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:40.153000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:40.162000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:40.202000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:40.216000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:40.332000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:40.427000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:40.434000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:40.445000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:40.452000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:40.466000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:40.501000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:40.513000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:40.639000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:40.726000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:40.734000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:40.749000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:40.756000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:40.766000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:40.802000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:40.810000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:40.943000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:41.024000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:41.034000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:41.046000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:41.059000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:41.067000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1101 11:02:41.104000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:41.114000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:41.249000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:41.324000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:41.335000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:41.343000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:41.358000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:41.368000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:41.410000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:41.558000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:41.626000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:41.634000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:41.642000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:41.657000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:41.674000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1101 11:02:41.709000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:41.863000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1101 11:02:41.927000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:41.935000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:41.943000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:41.957000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:41.983000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:42.169000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:42.230000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:42.240000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:42.259000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1101 11:02:42.280000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:42.472000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1101 11:02:42.522000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1101 11:02:42.534000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:42.566000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:42.780000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:42.867000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:43.085000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:43.175000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1101 11:02:43.391000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:43.478000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1101 11:02:43.778000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
Capturing batches (bs=1 avail_mem=37.83 GB): 100%|| 52/52 [10:48<00:00, 67.07s/it]Capturing batches (bs=1 avail_mem=37.83 GB): 100%|| 52/52 [10:48<00:00, 12.47s/it]
[2025-11-01 11:02:49 TP0] Registering 6396 cuda graph addresses
[2025-11-01 11:02:51 TP5] Capture cuda graph end. Time elapsed: 650.95 s. mem usage=6.17 GB. avail mem=37.13 GB.
[2025-11-01 11:02:51 TP6] Capture cuda graph end. Time elapsed: 649.94 s. mem usage=6.15 GB. avail mem=37.14 GB.
[2025-11-01 11:02:51 TP0] Capture cuda graph end. Time elapsed: 650.74 s. mem usage=6.16 GB. avail mem=37.41 GB.
[2025-11-01 11:02:51 TP3] Capture cuda graph end. Time elapsed: 650.97 s. mem usage=6.10 GB. avail mem=37.07 GB.
[2025-11-01 11:02:51 TP7] Capture cuda graph end. Time elapsed: 650.94 s. mem usage=6.18 GB. avail mem=37.12 GB.
[2025-11-01 11:02:51 TP4] Capture cuda graph end. Time elapsed: 650.94 s. mem usage=6.16 GB. avail mem=37.06 GB.
[2025-11-01 11:02:51 TP1] Capture cuda graph end. Time elapsed: 651.07 s. mem usage=6.12 GB. avail mem=37.05 GB.
[2025-11-01 11:02:51 TP2] Capture cuda graph end. Time elapsed: 651.18 s. mem usage=6.20 GB. avail mem=36.95 GB.
[2025-11-01 11:02:51 TP0] max_total_num_tokens=971639, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=1024, context_len=163840, available_gpu_mem=37.41 GB
[2025-11-01 11:02:51] INFO:     Started server process [43]
[2025-11-01 11:02:51] INFO:     Waiting for application startup.
[2025-11-01 11:02:51] INFO:     Application startup complete.
[2025-11-01 11:02:51] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-11-01 11:02:52] INFO:     127.0.0.1:47004 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-01 11:02:52 TP0] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:02:54 TP5] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:02:54 TP4] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:02:54 TP0] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:02:54 TP7] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:02:54 TP6] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:02:54 TP2] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:02:54 TP1] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:02:54 TP3] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:02:55] INFO:     127.0.0.1:47360 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-01 11:02:56] INFO:     127.0.0.1:47018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:02:56] The server is fired up and ready to roll!
[2025-11-01 11:03:03] INFO:     127.0.0.1:47362 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-01 11:03:03 TP0] Prefill batch, #new-seq: 1, #new-token: 666, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP0] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP3] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP2] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP1] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP5] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP6] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP7] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP4] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03] INFO:     127.0.0.1:47368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:03 TP0] Prefill batch, #new-seq: 1, #new-token: 67, #cached-token: 667, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP5] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP6] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP4] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP1] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP2] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP0] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP3] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP7] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP0] Prefill batch, #new-seq: 40, #new-token: 2407, #cached-token: 26680, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP0] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP2] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP1] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP3] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP5] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP7] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP6] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP4] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:03 TP0] Prefill batch, #new-seq: 127, #new-token: 7833, #cached-token: 84964, token usage: 0.00, #running-req: 41, #queue-req: 0, 
[2025-11-01 11:03:04 TP0] Prefill batch, #new-seq: 148, #new-token: 8663, #cached-token: 99056, token usage: 0.01, #running-req: 168, #queue-req: 0, 
[2025-11-01 11:03:05 TP0] Prefill batch, #new-seq: 276, #new-token: 16372, #cached-token: 184773, token usage: 0.02, #running-req: 316, #queue-req: 41, 
[2025-11-01 11:03:06 TP0] Prefill batch, #new-seq: 275, #new-token: 16332, #cached-token: 184172, token usage: 0.04, #running-req: 592, #queue-req: 116, 
[2025-11-01 11:03:07 TP0] Prefill batch, #new-seq: 157, #new-token: 9797, #cached-token: 105158, token usage: 0.05, #running-req: 867, #queue-req: 295, 
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP3] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP3] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP3] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP3] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP3] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP3] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP2] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP2] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP2] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP2] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP2] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP2] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP1] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP1] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP1] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP1] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP1] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP1] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP0] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP0] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP0] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP0] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP0] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP0] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP7] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP7] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP7] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP7] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP7] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP7] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP5] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP5] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP5] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP5] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP5] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP5] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP6] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP6] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP6] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP6] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP6] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP6] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP4] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP4] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP4] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-01 11:03:09 TP4] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP4] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:09 TP4] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-01 11:03:12] INFO:     127.0.0.1:50270 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:12 TP6] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:12 TP4] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:12 TP1] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:12 TP2] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:12 TP7] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:12 TP0] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:12 TP3] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:12 TP5] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:12 TP0] Prefill batch, #new-seq: 1, #new-token: 42, #cached-token: 669, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:12 TP2] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:12 TP4] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:12 TP6] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:12 TP7] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:12 TP0] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:12 TP3] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:12 TP1] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:12 TP5] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:13 TP0] Decode batch, #running-req: 1024, #token: 95502, token usage: 0.10, cuda graph: False, gen throughput (token/s): 1513.12, #queue-req: 294, 
[2025-11-01 11:03:13] INFO:     127.0.0.1:50334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:13 TP0] Prefill batch, #new-seq: 1, #new-token: 69, #cached-token: 669, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:13 TP1] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:13 TP6] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:13 TP2] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:13 TP5] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:13 TP4] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:13 TP7] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:13 TP0] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:13 TP3] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:13] INFO:     127.0.0.1:47996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:13] INFO:     127.0.0.1:49080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:13] INFO:     127.0.0.1:50976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:13] INFO:     127.0.0.1:51852 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:13 TP6] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:13 TP4] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:13 TP1] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:13 TP2] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:13 TP7] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:13 TP3] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:13 TP0] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:13 TP5] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14] INFO:     127.0.0.1:47410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:48650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:51714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14 TP0] Prefill batch, #new-seq: 4, #new-token: 303, #cached-token: 2678, token usage: 0.11, #running-req: 1020, #queue-req: 289, 
[aiter] [fused_moe] using default for (303, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP6] [fused_moe] using default for (303, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (303, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP2] [fused_moe] using default for (303, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (303, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP4] [fused_moe] using default for (303, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (303, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP7] [fused_moe] using default for (303, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (303, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP0] [fused_moe] using default for (303, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (303, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP3] [fused_moe] using default for (303, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (303, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP5] [fused_moe] using default for (303, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (303, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP1] [fused_moe] using default for (303, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14] INFO:     127.0.0.1:48194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:49030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:49812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:50152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:51644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:52948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:55866 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP6] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP2] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP4] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP0] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP3] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP7] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP1] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP5] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP0] Prefill batch, #new-seq: 10, #new-token: 691, #cached-token: 6702, token usage: 0.11, #running-req: 1014, #queue-req: 279, 
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP6] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP4] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP1] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP2] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP7] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP3] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP5] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP0] [fused_moe] using default for (691, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14] INFO:     127.0.0.1:48428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:50296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:51666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:52992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:55722 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP6] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP2] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP0] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP4] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP3] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP7] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP1] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP5] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP0] Prefill batch, #new-seq: 5, #new-token: 273, #cached-token: 3353, token usage: 0.11, #running-req: 1019, #queue-req: 274, 
[aiter] [fused_moe] using default for (273, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP2] [fused_moe] using default for (273, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (273, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (273, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP6] [fused_moe] using default for (273, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (273, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP4] [fused_moe] using default for (273, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP1] [fused_moe] using default for (273, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (273, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP7] [fused_moe] using default for (273, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (273, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP3] [fused_moe] using default for (273, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (273, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP0] [fused_moe] using default for (273, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (273, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP5] [fused_moe] using default for (273, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14] INFO:     127.0.0.1:47654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:47814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:47834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:48530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:49530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:51590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:52028 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP3] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP6] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP2] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP4] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP0] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP7] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP1] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP5] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP0] Prefill batch, #new-seq: 7, #new-token: 339, #cached-token: 4692, token usage: 0.11, #running-req: 1017, #queue-req: 267, 
[aiter] [fused_moe] using default for (339, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (339, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP6] [fused_moe] using default for (339, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP2] [fused_moe] using default for (339, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (339, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (339, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (339, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP4] [fused_moe] using default for (339, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP1] [fused_moe] using default for (339, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP7] [fused_moe] using default for (339, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (339, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP3] [fused_moe] using default for (339, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (339, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP0] [fused_moe] using default for (339, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (339, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP5] [fused_moe] using default for (339, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14] INFO:     127.0.0.1:48988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:52284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:55854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:55990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14 TP0] Prefill batch, #new-seq: 4, #new-token: 263, #cached-token: 2679, token usage: 0.11, #running-req: 1020, #queue-req: 263, 
[aiter] [fused_moe] using default for (263, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (263, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (263, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP1] [fused_moe] using default for (263, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP6] [fused_moe] using default for (263, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP2] [fused_moe] using default for (263, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (263, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP4] [fused_moe] using default for (263, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (263, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP7] [fused_moe] using default for (263, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (263, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (263, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP3] [fused_moe] using default for (263, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP0] [fused_moe] using default for (263, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (263, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP5] [fused_moe] using default for (263, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14] INFO:     127.0.0.1:47736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:50568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:51142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:52462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:54134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:14] INFO:     127.0.0.1:55210 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP0] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP6] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP4] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP2] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP3] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP7] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP5] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:14 TP1] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP0] Prefill batch, #new-seq: 6, #new-token: 307, #cached-token: 4018, token usage: 0.11, #running-req: 1018, #queue-req: 257, 
[aiter] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP2] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP6] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP1] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP7] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP4] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP3] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP0] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP5] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15] INFO:     127.0.0.1:50944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:51682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:51782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:53268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:54606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:55388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15 TP0] Prefill batch, #new-seq: 6, #new-token: 287, #cached-token: 4021, token usage: 0.11, #running-req: 1018, #queue-req: 251, 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP6] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP2] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP4] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP1] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP7] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP3] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP0] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP5] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15] INFO:     127.0.0.1:47884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:48092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:50806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:51928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:55012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:55316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:55926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15 TP0] Prefill batch, #new-seq: 7, #new-token: 478, #cached-token: 4689, token usage: 0.11, #running-req: 1017, #queue-req: 244, 
[aiter] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP2] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP1] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP6] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP4] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP7] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP3] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP0] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP5] [fused_moe] using default for (478, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15] INFO:     127.0.0.1:47396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:49102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:49590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:49598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:50266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:53578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:54808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:54866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:55156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:56094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15 TP0] Prefill batch, #new-seq: 10, #new-token: 659, #cached-token: 6701, token usage: 0.11, #running-req: 1014, #queue-req: 234, 
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP2] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP6] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP7] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP1] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP4] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP3] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP0] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP5] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15] INFO:     127.0.0.1:48080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:48352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:49318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:50694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:52070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:54418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:54856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15 TP0] Prefill batch, #new-seq: 7, #new-token: 461, #cached-token: 4692, token usage: 0.11, #running-req: 1017, #queue-req: 227, 
[aiter] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP7] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP6] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP4] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP2] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP1] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP5] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP0] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15 TP3] [fused_moe] using default for (461, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:15] INFO:     127.0.0.1:48498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:52132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:52172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:52854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:53640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:15] INFO:     127.0.0.1:56272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16 TP0] Prefill batch, #new-seq: 6, #new-token: 401, #cached-token: 4020, token usage: 0.11, #running-req: 1018, #queue-req: 221, 
[aiter] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP6] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP2] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP7] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP1] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP4] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP3] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP0] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP5] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16] INFO:     127.0.0.1:49948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:51470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:52368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:52746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:53046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:53548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:53746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:55220 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP6] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP7] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP3] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP4] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP0] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP2] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP1] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP5] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP0] Prefill batch, #new-seq: 8, #new-token: 453, #cached-token: 5356, token usage: 0.12, #running-req: 1016, #queue-req: 213, 
[aiter] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP6] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP2] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP1] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP4] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP7] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP5] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP3] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP0] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16] INFO:     127.0.0.1:48260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:48634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:49162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:50468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:50728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:51278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:53164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:55170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:55268 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP0] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP6] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP7] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP4] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP2] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP3] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP1] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP5] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP0] Prefill batch, #new-seq: 9, #new-token: 397, #cached-token: 6028, token usage: 0.12, #running-req: 1015, #queue-req: 204, 
[aiter] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP2] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP6] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP1] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP7] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP4] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP3] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP0] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP5] [fused_moe] using default for (397, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16] INFO:     127.0.0.1:48328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:49368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:50690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:51412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:52448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:53366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:55700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16 TP0] Prefill batch, #new-seq: 7, #new-token: 412, #cached-token: 4690, token usage: 0.12, #running-req: 1017, #queue-req: 197, 
[aiter] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP6] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP7] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP1] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP2] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP4] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP3] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP5] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP0] [fused_moe] using default for (412, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16] INFO:     127.0.0.1:48348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:51766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:52044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:53628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:53634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:54106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16 TP0] Prefill batch, #new-seq: 6, #new-token: 396, #cached-token: 4019, token usage: 0.12, #running-req: 1018, #queue-req: 191, 
[aiter] [fused_moe] using default for (396, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (396, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (396, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP1] [fused_moe] using default for (396, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (396, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP7] [fused_moe] using default for (396, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (396, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP2] [fused_moe] using default for (396, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP6] [fused_moe] using default for (396, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP4] [fused_moe] using default for (396, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (396, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (396, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP3] [fused_moe] using default for (396, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP0] [fused_moe] using default for (396, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (396, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP5] [fused_moe] using default for (396, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16] INFO:     127.0.0.1:47380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:48364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:49136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:49888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:50764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:51876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:53450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:53778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:54280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:55466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:16] INFO:     127.0.0.1:56040 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP7] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP0] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP3] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP6] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP4] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP2] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP1] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:16 TP5] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP0] Prefill batch, #new-seq: 12, #new-token: 707, #cached-token: 8037, token usage: 0.12, #running-req: 1012, #queue-req: 179, 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP2] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP7] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP6] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP4] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP1] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP3] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP5] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP0] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17] INFO:     127.0.0.1:47816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:48686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:48732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:49302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:49352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:49744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:49976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:52776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:55028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17 TP0] Prefill batch, #new-seq: 9, #new-token: 495, #cached-token: 6030, token usage: 0.12, #running-req: 1015, #queue-req: 170, 
[aiter] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP7] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP2] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP4] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP6] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP1] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP3] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP5] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP0] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17] INFO:     127.0.0.1:48820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:48886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:49674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:50366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:50708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:51798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:53236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:53284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:53706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:54538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17 TP0] Prefill batch, #new-seq: 10, #new-token: 480, #cached-token: 6700, token usage: 0.12, #running-req: 1014, #queue-req: 160, 
[2025-11-01 11:03:17] INFO:     127.0.0.1:48048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:51886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:51912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:53144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:54544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:54834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17 TP0] Prefill batch, #new-seq: 6, #new-token: 310, #cached-token: 4017, token usage: 0.12, #running-req: 1018, #queue-req: 154, 
[aiter] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP6] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP7] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP2] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP1] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP4] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP3] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP5] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP0] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17] INFO:     127.0.0.1:49248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:50856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:51916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:51946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:52256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:53876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:53942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:54722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:55346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:55502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:17] INFO:     127.0.0.1:56330 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP6] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP0] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP3] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP2] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP7] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP4] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP1] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP5] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP0] Prefill batch, #new-seq: 11, #new-token: 734, #cached-token: 7371, token usage: 0.12, #running-req: 1013, #queue-req: 143, 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP2] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP7] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP4] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP1] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP6] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP3] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP5] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:17 TP0] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18] INFO:     127.0.0.1:49122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:49398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:51832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:52938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:54040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:54396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:55408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18 TP0] Prefill batch, #new-seq: 7, #new-token: 408, #cached-token: 4693, token usage: 0.12, #running-req: 1017, #queue-req: 136, 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP7] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP2] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP6] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP1] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP4] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP0] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP5] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP3] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18] INFO:     127.0.0.1:47492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:48126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:48398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:48898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:49414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:50104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:52248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:52788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:52874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:53558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:54618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18 TP0] Prefill batch, #new-seq: 11, #new-token: 680, #cached-token: 7368, token usage: 0.12, #running-req: 1013, #queue-req: 125, 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP7] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP2] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP6] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP1] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP4] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP5] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP0] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP3] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18] INFO:     127.0.0.1:47688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:48448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:48856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:48996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:49646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:49682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:52580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:52728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:53654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:54266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:55230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18 TP0] Prefill batch, #new-seq: 11, #new-token: 763, #cached-token: 7371, token usage: 0.12, #running-req: 1013, #queue-req: 114, 
[aiter] [fused_moe] using default for (763, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP2] [fused_moe] using default for (763, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (763, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (763, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (763, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP7] [fused_moe] using default for (763, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP6] [fused_moe] using default for (763, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP1] [fused_moe] using default for (763, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (763, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP4] [fused_moe] using default for (763, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (763, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP3] [fused_moe] using default for (763, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (763, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP0] [fused_moe] using default for (763, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (763, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP5] [fused_moe] using default for (763, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18] INFO:     127.0.0.1:48614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:49468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:50108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:50816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:53312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:55252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:55744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18 TP0] Prefill batch, #new-seq: 7, #new-token: 484, #cached-token: 4691, token usage: 0.12, #running-req: 1017, #queue-req: 107, 
[aiter] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP2] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP7] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP6] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP4] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP1] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP3] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP0] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP5] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18] INFO:     127.0.0.1:48008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:48538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:50400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:50426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:50914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:51366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:52224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:52380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:52682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:54626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:55020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:55096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:18] INFO:     127.0.0.1:55256 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP2] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP6] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP0] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP7] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP3] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP4] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP1] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP5] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP0] Prefill batch, #new-seq: 13, #new-token: 834, #cached-token: 8715, token usage: 0.12, #running-req: 1011, #queue-req: 94, 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP2] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP4] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP7] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP6] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP1] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP3] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP0] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:18 TP5] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19] INFO:     127.0.0.1:47758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:47942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:48954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:49630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:50276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:50392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:51106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:51838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:51878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:52342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:53010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:54172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:55546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19 TP0] Prefill batch, #new-seq: 13, #new-token: 692, #cached-token: 8709, token usage: 0.12, #running-req: 1011, #queue-req: 81, 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP6] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP2] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP7] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP1] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP4] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP3] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP5] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP0] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19] INFO:     127.0.0.1:48900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:50028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:50068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:52974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19 TP0] Prefill batch, #new-seq: 4, #new-token: 234, #cached-token: 2679, token usage: 0.12, #running-req: 1020, #queue-req: 77, 
[aiter] [fused_moe] using default for (234, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP2] [fused_moe] using default for (234, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (234, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (234, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP7] [fused_moe] using default for (234, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP6] [fused_moe] using default for (234, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (234, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (234, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP4] [fused_moe] using default for (234, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP1] [fused_moe] using default for (234, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (234, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP3] [fused_moe] using default for (234, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (234, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP0] [fused_moe] using default for (234, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (234, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP5] [fused_moe] using default for (234, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19] INFO:     127.0.0.1:47524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:48568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:48774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:49074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:49764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:50146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:50416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:51914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:52820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:53034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:53168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:53618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:54210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:54318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:54634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:55238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:55304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:55622 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP2] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP6] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP0] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP4] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP3] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP7] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP1] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP5] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP0] Prefill batch, #new-seq: 18, #new-token: 1209, #cached-token: 12053, token usage: 0.12, #running-req: 1006, #queue-req: 59, 
[2025-11-01 11:03:19] INFO:     127.0.0.1:47750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:48970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:49064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:49494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:50770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:50878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:51908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:52316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:52762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:52932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:54150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:55054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:55070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:55368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:55550 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP2] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP0] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP3] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP1] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP6] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP7] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP4] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP5] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP0] Prefill batch, #new-seq: 15, #new-token: 1036, #cached-token: 10049, token usage: 0.12, #running-req: 1009, #queue-req: 44, 
[2025-11-01 11:03:19] INFO:     127.0.0.1:49270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:49312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:51246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:51454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:52136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:52394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:52858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:53088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:53244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19] INFO:     127.0.0.1:55636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:19 TP0] Prefill batch, #new-seq: 10, #new-token: 582, #cached-token: 6698, token usage: 0.12, #running-req: 1014, #queue-req: 34, 
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP2] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP7] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP6] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP4] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP1] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP3] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP5] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:19 TP0] [fused_moe] using default for (582, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20] INFO:     127.0.0.1:49522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:50224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:51102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:52614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:52668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:54076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:54358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:54494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:54588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:54978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:55174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:55822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:56056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:56238 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP6] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP0] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP4] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP2] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP3] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP7] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP1] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP5] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP0] Prefill batch, #new-seq: 14, #new-token: 846, #cached-token: 9384, token usage: 0.12, #running-req: 1010, #queue-req: 20, 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP2] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP6] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP1] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP7] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP4] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP3] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP5] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP0] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20] INFO:     127.0.0.1:47720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:48302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:48760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:48826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:49106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:50476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:50740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:50884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:51486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:52104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:52180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:53800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:54204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:56414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20 TP0] Prefill batch, #new-seq: 14, #new-token: 851, #cached-token: 9376, token usage: 0.12, #running-req: 1010, #queue-req: 6, 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP2] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP6] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP4] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP1] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP7] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP3] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP5] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP0] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP0] Decode batch, #running-req: 1010, #token: 120713, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5576.64, #queue-req: 6, 
[2025-11-01 11:03:20] INFO:     127.0.0.1:47676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:49178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:49346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:50588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:51152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:51290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:51688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:53474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:54030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:55120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:56162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:56320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20 TP0] Prefill batch, #new-seq: 6, #new-token: 302, #cached-token: 4024, token usage: 0.13, #running-req: 1012, #queue-req: 0, 
[aiter] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP2] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP6] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP4] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP7] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP1] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP5] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP3] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP0] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20] INFO:     127.0.0.1:47596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:48198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:48274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:48872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:50880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:50886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:53374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:54420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:47562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:50854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:53060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:53908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:54438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:54950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:55840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:55960 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP6] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP1] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP2] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP4] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP7] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP3] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP5] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP0] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20] INFO:     127.0.0.1:48972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:49856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:51236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:51758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:51864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:51982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:53462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:55450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:56032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:20] INFO:     127.0.0.1:56264 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP7] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP4] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP6] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP0] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP5] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP1] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP2] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:20 TP3] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21] INFO:     127.0.0.1:49824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:50100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:51550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:51872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:51904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:52344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:53080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:53344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:53892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:54254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:54378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:54640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:55190 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP4] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP5] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP3] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP1] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP2] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP7] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP6] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP0] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21] INFO:     127.0.0.1:48300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:48670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:49054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:49242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:49392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:51772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:52544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:53420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:55572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:55690 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP7] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP6] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP1] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP2] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP4] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP3] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP5] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP0] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21] INFO:     127.0.0.1:48036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:48094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:48150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:50508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:52192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:52292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:52326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:55136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:55380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:55652 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP7] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP2] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP4] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP6] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP3] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP0] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP5] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP1] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21] INFO:     127.0.0.1:47664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:47912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:48214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:49902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:51976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:52484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:52550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:54238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:54504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:54584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:54972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:55382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:55968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:56304 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP4] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP2] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP3] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP6] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP0] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP7] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP1] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP5] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21] INFO:     127.0.0.1:47500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:47626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:48060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:48606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:49434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:50320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:50502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:51262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:51652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:51728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:52426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:53970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:55330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:55348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:56124 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP6] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP3] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP4] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP2] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP7] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP0] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP1] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP5] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21] INFO:     127.0.0.1:48420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:48468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:50376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:52120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:53018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:53726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:54758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:55074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:55606 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP2] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP6] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP4] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP0] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP7] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP3] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP1] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP5] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21] INFO:     127.0.0.1:47852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:48140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:48464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:48916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:49286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:49934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:50678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:50780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:51370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:52830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:53404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:54452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:54642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:54792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:54840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:54906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:55364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:55424 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP1] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP5] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP6] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP2] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP3] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP0] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP7] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP4] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21] INFO:     127.0.0.1:47578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:48022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:48116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:48478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:49756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:49982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:51268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:52620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:53530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:53922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:54228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:21] INFO:     127.0.0.1:54302 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP6] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP7] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP5] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP1] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP3] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP2] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP4] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:21 TP0] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22] INFO:     127.0.0.1:47642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:48120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:49190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:49540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:50992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:52296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:52302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:54600 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP4] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP0] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP6] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP2] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP1] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP5] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP7] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP3] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22] INFO:     127.0.0.1:47818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:47862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:47902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:49724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:50420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:51204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:51442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:51750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:52024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:53154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:54876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:55198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:55260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:56500 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP6] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP4] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP7] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP0] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP3] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP2] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP1] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP5] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22] INFO:     127.0.0.1:49478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:50304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:50796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:50810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:51936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:52058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:52354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:52566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:53824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:55620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:55984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:56398 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP4] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP2] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP6] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP0] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP3] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP7] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP1] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP5] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22] INFO:     127.0.0.1:47452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:47888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:48668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:49810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:50870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:51154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:53960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:54454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:56004 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP6] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP2] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP4] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP3] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP0] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP7] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP1] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP5] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22] INFO:     127.0.0.1:47582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:48102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:49860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:50960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:51312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:51638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:51952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:52654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:54164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:55296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:55596 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP4] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP6] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP2] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP3] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP0] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP7] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP1] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP5] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22] INFO:     127.0.0.1:47432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:50212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:52442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:55486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:55574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:56150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:56176 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP4] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP6] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP2] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP0] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP3] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP7] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP1] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP5] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22] INFO:     127.0.0.1:48388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:48918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:50632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:50792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:51376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:53698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:55008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:55038 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP4] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP3] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP6] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP7] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP2] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP0] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP1] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP5] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22] INFO:     127.0.0.1:47844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:47956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:48170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:49418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:50238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:50642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:51284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:52748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:53848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:53958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:55598 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP4] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP6] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP2] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP7] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP3] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP0] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP1] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP5] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22] INFO:     127.0.0.1:47430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:48710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:51534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:51940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:52470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:53128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:54264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:54398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:56138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:56384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:56468 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP4] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP6] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP2] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP3] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP7] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP0] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP1] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP5] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22] INFO:     127.0.0.1:48594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:52584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:52808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:53336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:53358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:54120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:54198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:54246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:55004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:55224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:56440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:56944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:22] INFO:     127.0.0.1:57120 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP4] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP6] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP2] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP0] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP3] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP7] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP1] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:22 TP5] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23] INFO:     127.0.0.1:47420 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP4] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP6] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP2] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP3] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP0] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP7] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP1] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP5] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23] INFO:     127.0.0.1:47616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:47782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:48088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:50364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:50560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:51300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:51316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:51338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:53388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:54742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP2] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP4] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP6] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP3] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP0] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP7] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP1] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP5] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23] INFO:     127.0.0.1:51438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:51558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:52904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:53264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:54520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:54682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:55780 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP6] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP4] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP2] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP0] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP3] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP7] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP1] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP5] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23] INFO:     127.0.0.1:48510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:48620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:49234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:49322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:49454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:49886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:52038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:52156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:52996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:53224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:53722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:54372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:55534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:56036 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP2] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP4] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP6] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP0] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP3] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP7] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP1] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP5] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23] INFO:     127.0.0.1:47556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:47978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:48134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:50178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:50520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:52702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:52922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:53754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:54990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:55178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:57088 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP4] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP2] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP6] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP3] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP0] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP7] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP1] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP5] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP3] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP6] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP4] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP2] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP7] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP0] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP1] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP5] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23] INFO:     127.0.0.1:48332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:50896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:51944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:52056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:54770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:55954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:57626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:48158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:49930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:50550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:52166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:54764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:55924 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP4] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP3] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP6] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP7] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP2] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP0] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP1] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP5] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23] INFO:     127.0.0.1:48728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:48790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:51600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:51812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:56340 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP2] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP6] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP4] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP0] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP7] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP3] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP1] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP5] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23] INFO:     127.0.0.1:47466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:48414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:49348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:49952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:49964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:50584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:51168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:51198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:52020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:52912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:53864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:53954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:56198 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP4] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP2] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP6] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP0] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP3] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP7] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP1] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP5] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23] INFO:     127.0.0.1:47644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:47872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:48086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:50004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:52598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:53792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:54308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:55104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:55360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:55806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:56624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:57162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:23] INFO:     127.0.0.1:57314 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP4] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP6] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP2] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP0] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP7] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP3] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP1] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:23 TP5] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24] INFO:     127.0.0.1:50778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:50842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:50890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:51052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:51100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:51206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:51450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:53216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:53596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:53736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:54328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:54738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:57540 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP4] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP2] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP6] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP7] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP0] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP3] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP1] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP5] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24] INFO:     127.0.0.1:48848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:49002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:51578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:52092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:53594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:53988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:54082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:54356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:54812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:54936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:56356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:56696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:57214 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP2] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP4] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP6] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP3] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP7] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP0] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP1] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP5] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24] INFO:     127.0.0.1:50180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:51328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:51588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:51822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:52268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:52332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:52444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:52734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:53830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:55232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:55888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:57268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:57592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:57612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:57768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:58336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:58774 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP2] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP4] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP6] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP3] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP0] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP7] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP1] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP5] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24] INFO:     127.0.0.1:47540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:48948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:49448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:50398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:51068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:51670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:54880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:55792 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP2] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP4] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP6] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP0] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP3] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP7] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP1] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP5] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24] INFO:     127.0.0.1:48580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:50492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:52218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:52636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:53498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:54826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:56188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:56436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:57044 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP2] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP4] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP6] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP0] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP3] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP7] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP1] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP5] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24] INFO:     127.0.0.1:47600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:47632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:48836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:49406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:49456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:49460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:51848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:53256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:53298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:53610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:55664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:55748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:56208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:57298 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP4] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP2] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP6] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP3] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP0] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP7] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP1] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP5] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24] INFO:     127.0.0.1:47498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:47724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:48636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:49712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:50140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:51964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:52760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:52882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:53976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:55088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:56528 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP6] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP4] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP2] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP7] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP5] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP0] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP1] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP3] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24] INFO:     127.0.0.1:48320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:48804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:50018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:53184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:56106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:56164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:56514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:58148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:58900 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP1] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP4] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP2] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP7] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP6] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP0] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP3] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP5] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP0] Decode batch, #running-req: 613, #token: 92468, token usage: 0.10, cuda graph: False, gen throughput (token/s): 7471.60, #queue-req: 0, 
[2025-11-01 11:03:24] INFO:     127.0.0.1:47662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:47976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:53200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:55826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:55902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:56400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:57720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:57918 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP2] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP4] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP3] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP6] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP0] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP7] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP1] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP5] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24] INFO:     127.0.0.1:50578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:50748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:51386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:51998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:54754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:54862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:55216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:56038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:24] INFO:     127.0.0.1:57186 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP4] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP2] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP3] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP6] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP0] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP7] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP1] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:24 TP5] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25] INFO:     127.0.0.1:47488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:47706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:49044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:49664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:49854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:52634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:53856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:54258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:54268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:54340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:56408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:58732 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP4] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP2] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP6] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP3] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP7] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP0] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP1] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP5] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25] INFO:     127.0.0.1:47680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:48180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:49506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:50458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:50804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:51890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:53542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:54784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:57714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:58160 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP4] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP2] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP6] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP3] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP7] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP0] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP1] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP5] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25] INFO:     127.0.0.1:47454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:48482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:48928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:50602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:50666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:50930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:51508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:52720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:54188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:54586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:54736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:55398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:55832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:56422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:56770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:56848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:57014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:58804 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP4] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP2] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP6] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP7] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP0] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP3] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP1] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP5] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25] INFO:     127.0.0.1:48946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:49082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:49554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:51834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:52784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:54002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:56370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:56616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:58370 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP2] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP6] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP4] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP3] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP0] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP7] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP1] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP5] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25] INFO:     127.0.0.1:51022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:52796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:53112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:53324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:53566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:57134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:57350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:57674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:58304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:59066 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP6] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP2] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP0] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP3] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP4] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP7] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP1] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP5] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25] INFO:     127.0.0.1:49086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:49536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:50336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:50428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:50928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:51224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:54380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:57284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:57392 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP2] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP4] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP6] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP3] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP7] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP0] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP1] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25 TP5] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:03:25] INFO:     127.0.0.1:48484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:48524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:48782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:49068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:49222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:49990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:51216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:52234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:55378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:55914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:56710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:56840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:57356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:57548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:57874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:47438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:48066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:49838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:50020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:51244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:52392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:53678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:57230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:51500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:52042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:53314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:54658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:55438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:55894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:50162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:51742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:53586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:54408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:54474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:54922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:57482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:57652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:48746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:49870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:51012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:51402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:54962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:55540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:57132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:58394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:47798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:53416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:54074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:54596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:54700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:55580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:56766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:57384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:25] INFO:     127.0.0.1:57688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:51422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:52008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:54018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:49380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:49560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:53538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:54824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:54892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:56292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:56896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:57600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:50228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:51570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:52692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:54650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:56110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:57934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:58268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:49192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:49736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:51172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:52202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:53070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:56604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:57036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:57070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:57376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:58782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:47402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:47624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:50288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:54864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:56148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:56750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:57066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:58574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:51356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:52708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:54854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:58164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:58634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:58696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:48694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:50540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:51804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:54894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:55942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:58856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:59060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:50650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:50720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:50734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:54054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:57402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:58278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:58902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:50954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:51624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:52764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:53016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:55274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:55764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:56794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:57642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:57926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:58136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:58182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:58518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:58548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:50246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:50256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:52348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:52898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:55472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:58240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:59040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:52480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:53512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:53812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:54528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:57864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:48718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:49148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:51050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:51084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:55042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:57480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:57998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:58380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:58392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:49142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:50206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:50596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:54130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:57948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:50442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:54464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:55882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:56680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:57252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:58320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:58640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:49266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:51124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:51134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:52508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:54688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:55048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:57148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:57344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:57474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:58100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:26] INFO:     127.0.0.1:58944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:48438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:49662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:53930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:56568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:56722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:47516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:53836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:57216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:50052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:51520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:54500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:55714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:56986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:57266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:57452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:48230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:49800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:51026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:56590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:57116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:57174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:59118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:49898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:55332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:56490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:56782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:57212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:57892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:48552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:50526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:56736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:56776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:49208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:53434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:56910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:57390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:59016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:59078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:59132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:49696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:50912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:51560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:52080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:52520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:56090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:56486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:56646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:57622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:59162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:47436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:49012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:51042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:51368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:51446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:54306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:54326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:55836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:51392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:55730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:57020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:57090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:57792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:59116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:47972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:52960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:57574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:59148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27 TP0] Decode batch, #running-req: 275, #token: 51938, token usage: 0.05, cuda graph: True, gen throughput (token/s): 6111.32, #queue-req: 0, 
[2025-11-01 11:03:27] INFO:     127.0.0.1:50124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:53662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:56802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:56880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:49250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:49914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:53518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:53554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:56974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:47434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:50618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:57888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:48240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:57256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:48942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:49620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:53796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:54546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:56540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:57640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:53214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:54712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:57932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:47998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:50466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:52880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:52984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:58986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:27] INFO:     127.0.0.1:59102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:53486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:58908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:50350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:51580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:51324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:54066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:56016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:52428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:56058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:57662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:58690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:49610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:57678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:57972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:58422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:58490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:58884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:48462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:49946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:58722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:49022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:53694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:56448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:56824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:58378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:58610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:58700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:49330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:50830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:52370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:57808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:58820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:58876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:58932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:47468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:51538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:57464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:59028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:49578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:51114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:57040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:57554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:57788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:57848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:58350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:47694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:53828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:54162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:54490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:48380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:51416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:56620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:58258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:49780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:51930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:56818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:49786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:52750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:56640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:57588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:58562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:51006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:51608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:57904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:48654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:57526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:58382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:51274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:55986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:56864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:57496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:57430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:58194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:47768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:49882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:56992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:57094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:55448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:28] INFO:     127.0.0.1:58024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:53104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:55078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:57000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:58014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:58434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:58854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:58604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:48250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:48892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:53318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:57558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:57780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:57818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:58838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:56850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:56858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:49174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:49498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:50084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:50356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:57078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:56926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:58200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:58676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:58922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:59086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:52666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:57510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:58534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:47904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:53502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:57242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:57732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:52846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:56280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:56654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:56658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:56906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:49296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:55142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:58448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:56070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:57996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29 TP0] Decode batch, #running-req: 111, #token: 25872, token usage: 0.03, cuda graph: True, gen throughput (token/s): 3790.53, #queue-req: 0, 
[2025-11-01 11:03:29] INFO:     127.0.0.1:47650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:47928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:53208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:57702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:58888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:50040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:52410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:56460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:57460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:57816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:58688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:56930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:57534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:57980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:53006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:56248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:56554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:51352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:55566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:55678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:51330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:56478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:56582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:58316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:48288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:52214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:47990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:48480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:58788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:29] INFO:     127.0.0.1:58956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:52638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:54092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:54394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:55288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:59120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:52532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:57834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:58214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:51702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:48466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:57054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:58224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:49566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:54426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:56714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:58590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:57744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:58600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:55578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:57198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:57508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:58480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:48304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:51182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:52500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:53668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:58418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:59046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:54562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:54666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:59156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:55518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:57330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:52150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:57096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:58654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:58152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:58978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:57418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:47482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:58360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:30] INFO:     127.0.0.1:58660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31 TP0] Decode batch, #running-req: 36, #token: 11114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1768.29, #queue-req: 0, 
[2025-11-01 11:03:31] INFO:     127.0.0.1:54444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:57890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:48570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:58090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:51774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:58058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:53764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:51066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:57958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:58456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:56222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:50194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:48356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:59002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:57680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:48964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:57852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:58880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:50388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:57776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:58506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:56076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:57362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:58668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:57164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:57048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:31] INFO:     127.0.0.1:58712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:32 TP0] Decode batch, #running-req: 9, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 822.44, #queue-req: 0, 
[2025-11-01 11:03:32] INFO:     127.0.0.1:54568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:32] INFO:     127.0.0.1:57110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:32] INFO:     127.0.0.1:56960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:32] INFO:     127.0.0.1:54296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:32] INFO:     127.0.0.1:58042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:32] INFO:     127.0.0.1:56674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:32] INFO:     127.0.0.1:56130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:32] INFO:     127.0.0.1:56318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:32] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:03:45] INFO:     127.0.0.1:50324 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-01 11:03:45 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-01 11:03:46 TP4] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-01 11:03:46 TP7] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-01 11:03:46 TP1] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-01 11:03:46 TP2] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-01 11:03:46 TP0] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-01 11:03:46 TP5] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] start build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-01 11:03:46 TP6] start build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-01 11:03:46 TP3] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for gfx942.
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for host.
[92mSuccessfully preprocessed all matching files.[0m
[aiter] finish build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip], cost 53.58823147s
[2025-11-01 11:04:40 TP6] finish build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip], cost 53.58823147s
[2025-11-01 11:04:40] INFO:     127.0.0.1:50334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:40 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-01 11:04:40 TP0] Prefill batch, #new-seq: 42, #new-token: 42, #cached-token: 30439, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-11-01 11:04:40 TP0] Prefill batch, #new-seq: 48, #new-token: 48, #cached-token: 35013, token usage: 0.01, #running-req: 43, #queue-req: 0, 
[2025-11-01 11:04:40 TP0] Prefill batch, #new-seq: 54, #new-token: 54, #cached-token: 39540, token usage: 0.01, #running-req: 91, #queue-req: 0, 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:40 TP5] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:40 TP4] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:40 TP7] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:40 TP1] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:40 TP2] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:40 TP6] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:40 TP0] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:40 TP3] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:40 TP0] Prefill batch, #new-seq: 56, #new-token: 56, #cached-token: 40750, token usage: 0.01, #running-req: 145, #queue-req: 0, 
[2025-11-01 11:04:40 TP0] Prefill batch, #new-seq: 61, #new-token: 61, #cached-token: 44273, token usage: 0.02, #running-req: 201, #queue-req: 0, 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:40 TP7] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:40 TP5] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:40 TP4] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:40 TP2] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:40 TP1] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:40 TP3] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:40 TP0] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:40 TP6] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:40 TP0] Prefill batch, #new-seq: 66, #new-token: 66, #cached-token: 47991, token usage: 0.02, #running-req: 262, #queue-req: 0, 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP5] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP7] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP6] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP4] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP2] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP1] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP0] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP3] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP0] Prefill batch, #new-seq: 65, #new-token: 65, #cached-token: 47326, token usage: 0.02, #running-req: 328, #queue-req: 0, 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP4] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP5] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP6] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP7] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP1] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP2] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP0] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP3] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP0] Prefill batch, #new-seq: 68, #new-token: 68, #cached-token: 49796, token usage: 0.03, #running-req: 393, #queue-req: 0, 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP4] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP5] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP7] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP2] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP6] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP1] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP3] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP0] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP0] Prefill batch, #new-seq: 71, #new-token: 71, #cached-token: 51401, token usage: 0.03, #running-req: 461, #queue-req: 0, 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP7] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP4] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP6] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP5] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP2] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP0] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP3] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP1] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP0] Prefill batch, #new-seq: 76, #new-token: 76, #cached-token: 55229, token usage: 0.04, #running-req: 532, #queue-req: 0, 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP5] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP6] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP4] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP7] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP2] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP1] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP0] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP3] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP0] Prefill batch, #new-seq: 76, #new-token: 76, #cached-token: 55223, token usage: 0.04, #running-req: 608, #queue-req: 0, 
[2025-11-01 11:04:41 TP0] Prefill batch, #new-seq: 83, #new-token: 83, #cached-token: 60404, token usage: 0.05, #running-req: 684, #queue-req: 0, 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP6] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP7] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP4] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP0] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP3] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP1] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP2] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP5] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:41 TP0] Prefill batch, #new-seq: 82, #new-token: 82, #cached-token: 59977, token usage: 0.05, #running-req: 767, #queue-req: 0, 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP6] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP5] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP7] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP4] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP0] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP2] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP1] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP3] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP0] Prefill batch, #new-seq: 92, #new-token: 92, #cached-token: 66910, token usage: 0.06, #running-req: 849, #queue-req: 0, 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP5] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP7] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP4] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP6] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP2] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP1] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP0] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP3] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP0] Prefill batch, #new-seq: 59, #new-token: 59, #cached-token: 43217, token usage: 0.06, #running-req: 941, #queue-req: 0, 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP5] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP7] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP6] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP4] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP1] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP2] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP0] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP3] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP6] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP5] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP4] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP7] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP0] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP1] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP3] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP2] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:42 TP0] Prefill batch, #new-seq: 24, #new-token: 24, #cached-token: 17707, token usage: 0.07, #running-req: 1000, #queue-req: 21, 
[2025-11-01 11:04:44 TP0] Decode batch, #running-req: 1024, #token: 82205, token usage: 0.08, cuda graph: False, gen throughput (token/s): 268.77, #queue-req: 295, 
[2025-11-01 11:04:45] INFO:     127.0.0.1:52584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:45 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 711, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-11-01 11:04:45] INFO:     127.0.0.1:58204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:46 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 710, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-11-01 11:04:46] INFO:     127.0.0.1:52626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:46 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 735, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-11-01 11:04:46] INFO:     127.0.0.1:50224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:46] INFO:     127.0.0.1:53282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:46] INFO:     127.0.0.1:53990 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:46 TP2] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:46 TP7] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:46 TP5] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:46 TP6] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:46 TP4] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:46 TP1] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:46 TP0] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:46 TP3] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:47] INFO:     127.0.0.1:49726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:51100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:54380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2256, token usage: 0.11, #running-req: 1021, #queue-req: 289, 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:47 TP1] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:47 TP5] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:47 TP0] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:47 TP4] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:47 TP6] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:47 TP2] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:47 TP7] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:47 TP3] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:47] INFO:     127.0.0.1:51066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:52108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:52446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:55398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:58344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5906, token usage: 0.11, #running-req: 1016, #queue-req: 281, 
[2025-11-01 11:04:47] INFO:     127.0.0.1:51016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:52588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:55440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:58188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2927, token usage: 0.11, #running-req: 1020, #queue-req: 277, 
[2025-11-01 11:04:47] INFO:     127.0.0.1:50090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:50150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:50218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:50790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:54234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:54522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:54624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5069, token usage: 0.11, #running-req: 1017, #queue-req: 270, 
[2025-11-01 11:04:47] INFO:     127.0.0.1:51306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:54756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:58342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:58430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2863, token usage: 0.11, #running-req: 1020, #queue-req: 266, 
[2025-11-01 11:04:47] INFO:     127.0.0.1:51206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:51666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:52860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:53454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:54938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:56486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:47] INFO:     127.0.0.1:57618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5057, token usage: 0.11, #running-req: 1017, #queue-req: 259, 
[2025-11-01 11:04:48] INFO:     127.0.0.1:50286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:51964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:54296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:54468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:55710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:57848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4325, token usage: 0.11, #running-req: 1018, #queue-req: 253, 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:48 TP2] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:48 TP6] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:48 TP7] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:48 TP1] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:48 TP5] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:48 TP4] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:48 TP0] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:48 TP3] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:48] INFO:     127.0.0.1:49944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:50018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:53110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:54518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:57424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:57580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:57750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:58392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5877, token usage: 0.11, #running-req: 1016, #queue-req: 245, 
[2025-11-01 11:04:48] INFO:     127.0.0.1:49696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:49986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:51764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:51878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:52560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:55988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:56988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:57306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:58518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6639, token usage: 0.11, #running-req: 1015, #queue-req: 236, 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:48 TP2] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:48 TP6] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:48 TP1] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:48 TP5] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:48 TP7] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:48 TP4] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:48 TP0] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:48 TP3] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:48] INFO:     127.0.0.1:50618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:52974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:53742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:54154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:56792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:57288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4405, token usage: 0.11, #running-req: 1018, #queue-req: 230, 
[2025-11-01 11:04:48] INFO:     127.0.0.1:50564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:51432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:53574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:54128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:54206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:54648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:55312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:56064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:48] INFO:     127.0.0.1:57220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6597, token usage: 0.12, #running-req: 1015, #queue-req: 221, 
[2025-11-01 11:04:49] INFO:     127.0.0.1:52232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:53838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:54854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:55202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:55522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:55948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:57484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:58530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5794, token usage: 0.12, #running-req: 1016, #queue-req: 213, 
[2025-11-01 11:04:49] INFO:     127.0.0.1:49846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:50056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:50976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:51002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:52022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:52774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:56050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:57594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:57636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:58876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7151, token usage: 0.12, #running-req: 1014, #queue-req: 203, 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:49 TP2] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:49 TP5] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:49 TP6] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:49 TP1] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:49 TP7] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:49 TP4] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:49 TP0] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:49 TP3] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:49] INFO:     127.0.0.1:50138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:50594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:51410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:52964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:53268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:54538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:54958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:55598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:56146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7295, token usage: 0.12, #running-req: 1014, #queue-req: 193, 
[2025-11-01 11:04:49] INFO:     127.0.0.1:50752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:51350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:54074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:56472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2900, token usage: 0.12, #running-req: 1020, #queue-req: 189, 
[2025-11-01 11:04:49] INFO:     127.0.0.1:49686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:51448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:51492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:52178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:53040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:54144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:55850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:56154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:49] INFO:     127.0.0.1:56578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6571, token usage: 0.12, #running-req: 1015, #queue-req: 180, 
[2025-11-01 11:04:50] INFO:     127.0.0.1:51030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:51626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:52050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:54740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:55274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:55590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:55696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:57448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:57614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6549, token usage: 0.12, #running-req: 1015, #queue-req: 171, 
[2025-11-01 11:04:50] INFO:     127.0.0.1:51190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:51706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:52410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:52664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:52984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:55612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:55658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:56094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:56950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:57780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:58718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7872, token usage: 0.12, #running-req: 1013, #queue-req: 160, 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:50 TP6] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:50 TP5] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:50 TP4] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:50 TP7] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:50 TP2] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:50 TP1] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:50 TP3] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:50 TP0] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:50] INFO:     127.0.0.1:50078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:54158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:55784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:55980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:56044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:56230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:56786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:56962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:57390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:58318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7258, token usage: 0.12, #running-req: 1014, #queue-req: 150, 
[2025-11-01 11:04:50 TP0] Decode batch, #running-req: 1014, #token: 115870, token usage: 0.12, cuda graph: False, gen throughput (token/s): 6726.79, #queue-req: 150, 
[2025-11-01 11:04:50] INFO:     127.0.0.1:51570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:53170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:54088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:54214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:56278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:57942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4410, token usage: 0.12, #running-req: 1018, #queue-req: 144, 
[2025-11-01 11:04:50] INFO:     127.0.0.1:50770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:50792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:50902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:51460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:52012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:53400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:54066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:55396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:56396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:57034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:50] INFO:     127.0.0.1:57878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8058, token usage: 0.12, #running-req: 1013, #queue-req: 133, 
[2025-11-01 11:04:51] INFO:     127.0.0.1:50420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:51088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:51274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:51744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:53162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:53190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:54258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:55284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:55314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:57010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:57936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8037, token usage: 0.12, #running-req: 1013, #queue-req: 122, 
[2025-11-01 11:04:51] INFO:     127.0.0.1:51158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:51910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:52260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:54724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:56646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:57232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:57660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5163, token usage: 0.12, #running-req: 1017, #queue-req: 115, 
[2025-11-01 11:04:51] INFO:     127.0.0.1:50562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:51794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:51956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:52394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:53012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:53138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:55134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:55722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:56662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:57110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:58006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8868, token usage: 0.12, #running-req: 1012, #queue-req: 103, 
[2025-11-01 11:04:51] INFO:     127.0.0.1:50714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:51282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:52714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:52742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:53702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:54704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:54892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:56058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:57530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:57694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7297, token usage: 0.12, #running-req: 1014, #queue-req: 93, 
[2025-11-01 11:04:51] INFO:     127.0.0.1:50364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:50456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:51442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:51860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:52566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:52696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:53428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:54110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:54176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:54250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:54812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:55458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:56802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:56928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:58020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:51] INFO:     127.0.0.1:58588 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:51 TP0] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:51 TP5] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:51 TP6] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:51 TP4] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:51 TP2] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:51 TP1] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:51 TP7] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:51 TP3] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP0] Prefill batch, #new-seq: 16, #new-token: 16, #cached-token: 11579, token usage: 0.12, #running-req: 1008, #queue-req: 77, 
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP2] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP6] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP1] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP5] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP4] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP7] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP0] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP3] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:04:52] INFO:     127.0.0.1:50650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:51212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:51400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:52366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:57714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3641, token usage: 0.12, #running-req: 1019, #queue-req: 72, 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP6] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP2] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP1] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP4] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP5] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP7] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP0] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP3] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52] INFO:     127.0.0.1:49908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:49938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:50532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:51116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:52070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:52334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:52722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:53202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:54616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:55520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:56036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:56568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:57680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:57836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10354, token usage: 0.12, #running-req: 1010, #queue-req: 58, 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP2] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP5] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP7] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP6] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP4] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP1] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP0] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP3] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52] INFO:     127.0.0.1:49998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:50664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:51386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:51814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:53056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:53946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:54786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:55238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:55300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:55392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:55424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:55800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:56518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:56708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:56826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:56904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:57040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:57430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:57478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:57502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:58634 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP5] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP1] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP4] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP3] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP7] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP6] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP0] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP2] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP0] Prefill batch, #new-seq: 21, #new-token: 21, #cached-token: 15378, token usage: 0.12, #running-req: 1003, #queue-req: 37, 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP7] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP6] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP1] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP2] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP4] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP5] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP0] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52 TP3] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:52] INFO:     127.0.0.1:53562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:53826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:54554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:54876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:55320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:55554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:55672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:58102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5870, token usage: 0.13, #running-req: 1016, #queue-req: 29, 
[2025-11-01 11:04:52] INFO:     127.0.0.1:51844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:52426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:52500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:52996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:53420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:55068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:56740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:56986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52] INFO:     127.0.0.1:58580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:52 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6557, token usage: 0.13, #running-req: 1015, #queue-req: 20, 
[2025-11-01 11:04:53] INFO:     127.0.0.1:50392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:50466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:50580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:50886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:52058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:52788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:53650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:53852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:54432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:55192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:56174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:56446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:56556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:56908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:58598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 10915, token usage: 0.13, #running-req: 1009, #queue-req: 5, 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP2] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP1] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP3] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP0] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP6] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP4] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP5] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP7] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53] INFO:     127.0.0.1:51522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:51638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:54190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:54262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:54422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:55868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:56336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:56380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:57572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:58130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3618, token usage: 0.13, #running-req: 1014, #queue-req: 0, 
[2025-11-01 11:04:53] INFO:     127.0.0.1:50724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:51182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:53184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:53584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:58118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:58288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:58742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:58780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:49924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:51222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:51586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:52894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:54040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:54282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:55528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:56248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:58426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:58868 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP3] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP2] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP1] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP0] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP5] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP4] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP7] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP6] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53] INFO:     127.0.0.1:50862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:51292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:51362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:52132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:53532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:54058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:54364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:55030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:56216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:56266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:56502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:57914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:58474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:53] INFO:     127.0.0.1:58670 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP3] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP2] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP1] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP0] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP6] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP7] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP4] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:53 TP5] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54] INFO:     127.0.0.1:50304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:52102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:52374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:52904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:53912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:53974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:54672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:55736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:56620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:56764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:57042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:57624 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP1] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP3] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP2] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP0] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP6] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP5] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP4] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP7] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54] INFO:     127.0.0.1:50684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:50742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:51568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:52000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:54020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:55542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:55844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:58146 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP3] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP1] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP2] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP0] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP6] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP5] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP7] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP4] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54] INFO:     127.0.0.1:50324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:50780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:52456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:52810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:54796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:57542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:57730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:57820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:50334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:50426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:51052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:52198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:54768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:54832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:55012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:56610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:57264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:57398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:57864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:58042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:58428 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP2] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP1] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP6] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP0] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP3] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP4] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP5] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP7] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54] INFO:     127.0.0.1:49870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:50002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:50106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:50162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:50342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:51790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:52798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:54542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:54916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:57764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:57790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:58556 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP2] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP6] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP3] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP1] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP0] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP4] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP7] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP5] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54] INFO:     127.0.0.1:50492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:50608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:54978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:55860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:56132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:56324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:57160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:57388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:58540 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP3] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP2] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP1] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP0] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP5] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP6] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP4] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP7] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54] INFO:     127.0.0.1:51614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:52220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:52472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:52524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:53694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:55292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:55816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:56856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:57060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:57206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:57370 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP3] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP2] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP1] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP0] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP5] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP6] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP4] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP7] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54] INFO:     127.0.0.1:49988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:50206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:52036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:52280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:52962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:53554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:55922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:56670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:57024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:54] INFO:     127.0.0.1:57886 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (904, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP3] [fused_moe] using default for (904, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (904, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP2] [fused_moe] using default for (904, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (904, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP1] [fused_moe] using default for (904, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (904, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP0] [fused_moe] using default for (904, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (904, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP6] [fused_moe] using default for (904, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (904, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP4] [fused_moe] using default for (904, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (904, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP5] [fused_moe] using default for (904, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (904, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:54 TP7] [fused_moe] using default for (904, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:55] INFO:     127.0.0.1:49940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:50322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:50940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:51118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:51526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:51682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:53212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:53300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:54310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:54784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:56780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:57802 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:55 TP2] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:55 TP6] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:55 TP0] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:55 TP4] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:55 TP3] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:55 TP1] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:55 TP7] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:55 TP5] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:55] INFO:     127.0.0.1:50050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:50384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:53780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:53936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:54390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:54764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:55298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:55602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:57308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:58820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:58950 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:55 TP3] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:55 TP1] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:55 TP2] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:55 TP0] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:55 TP6] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:55 TP5] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:55 TP4] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:55 TP7] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:55] INFO:     127.0.0.1:50358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:50678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:50730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:51796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:52142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:52604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:53084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:53126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:53538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:54012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:54834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:55020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:55146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:56862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:55] INFO:     127.0.0.1:58622 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP3] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP1] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP2] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP0] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP6] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP5] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP7] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP4] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56] INFO:     127.0.0.1:49796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:50196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:50262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:50408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:50632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:50850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:52090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:53466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:56582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:58454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:58568 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP2] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP6] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP1] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP5] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP3] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP0] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP7] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP4] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56] INFO:     127.0.0.1:50438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:50822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:52662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:52728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:53286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:53504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:55110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:56180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:56256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:56522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:56594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:56968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:57700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:58098 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP0] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP3] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP1] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP2] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP4] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP7] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP6] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP5] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56] INFO:     127.0.0.1:49754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:50776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:50880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:55094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:56508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:57438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:57952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:58062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:58090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:56] INFO:     127.0.0.1:58564 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP0] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP4] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP6] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP2] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP1] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP3] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP7] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:56 TP5] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57] INFO:     127.0.0.1:51234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:51894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:52758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:52916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:53712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:54600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:55650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:56098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:56544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:57426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:58500 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP3] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP1] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP6] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP0] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP2] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP4] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP5] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP7] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57] INFO:     127.0.0.1:50294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:50354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:50430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:50918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:51730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:55208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:56164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:56316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:57604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:57644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:57656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:57746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:58084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:58738 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP1] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP3] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP6] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP4] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP2] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP0] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP7] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP5] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57] INFO:     127.0.0.1:49730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:49842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:51078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:53078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:53094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:53894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:54354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:54426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:55594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:56636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:58230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:58918 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP0] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP3] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP1] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP2] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP5] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP7] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP4] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP6] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57] INFO:     127.0.0.1:53164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:54416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:55496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:55770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:57408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:58220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:59332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:52858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:53456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:54728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:58136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:58422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:59588 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP6] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP3] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP1] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP2] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP0] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP4] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP7] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP5] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57] INFO:     127.0.0.1:50062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:50668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:51168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:52928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:53590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:53980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:54244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:54952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:55042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:56096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:57132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:58990 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP3] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP1] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP0] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP6] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP2] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP4] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP5] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP7] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57] INFO:     127.0.0.1:49970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:53604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:53766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:53924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:54358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:55692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:57090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:58242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:59050 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP1] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP3] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP6] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP4] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP2] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP7] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP0] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP5] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57] INFO:     127.0.0.1:50130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:51556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:51754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:51870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:52170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:52620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:52680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:53958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:54312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:55472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:55666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:56014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:57996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:58486 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP3] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP1] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP6] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP0] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP2] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP4] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP7] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP5] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57] INFO:     127.0.0.1:49888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:51334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:54000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:55402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:56118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:56724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:57402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:57518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:58272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57] INFO:     127.0.0.1:59988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:57 TP0] Decode batch, #running-req: 746, #token: 104112, token usage: 0.11, cuda graph: False, gen throughput (token/s): 5118.98, #queue-req: 0, 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP1] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP6] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP0] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP3] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP4] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP5] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP7] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57 TP2] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:57] INFO:     127.0.0.1:50926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:51802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:53516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:54278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:54636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:55052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:57166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:58408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:59492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:60008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:52834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:53514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:54452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:54994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:55084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:55626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:55932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:57818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:58384 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP4] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP1] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP6] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP3] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP5] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP7] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP0] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP2] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58] INFO:     127.0.0.1:50704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:50946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:54118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:56244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:56462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:57556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:58822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:58892 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP6] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP1] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP3] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP4] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP0] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP5] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP2] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP7] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58] INFO:     127.0.0.1:49798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:50652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:51142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:51154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:52272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:52876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:53070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:53470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:53490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:54102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:55388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:56302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:56438 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP1] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP6] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP3] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP4] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP2] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP0] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP7] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP5] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58] INFO:     127.0.0.1:50190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:50958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:52244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:52288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:56200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:57174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:58258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:59600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:59652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:59736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:50404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:51862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:52786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:53226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:53824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:54236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:54688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:56104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:57118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:59920 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP6] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP1] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP3] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP0] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP4] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP2] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP5] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP7] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58] INFO:     127.0.0.1:52116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:53032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:53366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:53526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:55200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:56690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:56982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:57222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:57382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:58590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:59668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:59752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:60134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:32814 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP1] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP6] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP0] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP4] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP2] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP5] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP3] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP7] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58] INFO:     127.0.0.1:49956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:52304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:53092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:53614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:54818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:54934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:56372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:58372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:59114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:59714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:60664 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP1] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP6] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP4] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP2] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP0] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP5] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP3] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP7] [fused_moe] using default for (651, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58] INFO:     127.0.0.1:49898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:50962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:51140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:51926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:51980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:52706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:52766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:53416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:54328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:54496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:56752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:57354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:58250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:59966 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP6] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP3] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP1] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP4] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP2] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP0] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP5] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP7] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58] INFO:     127.0.0.1:50768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:52438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:53232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:55074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:55976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:58056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:59424 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP1] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP3] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP6] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP2] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP0] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP4] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP7] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP5] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58] INFO:     127.0.0.1:50048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:50490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:51652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:51778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:53242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:55026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:55714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:55812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:56030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:56366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:57182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:58138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:58] INFO:     127.0.0.1:59740 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP6] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP1] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP4] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP0] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP2] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP5] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP3] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:58 TP7] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59] INFO:     127.0.0.1:53546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:55230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:55686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:58296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:58656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:58774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:58984 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP3] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP1] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP2] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP0] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP6] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP4] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP7] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP5] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59] INFO:     127.0.0.1:49968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:50924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:51096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:58948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:60294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:32922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:52484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:54562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:55182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:55838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:56534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:58746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:60116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:60482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:52878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:53234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:53710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:53930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:54164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:56738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:57900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:59620 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP6] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP0] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP2] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP4] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP3] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP7] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP1] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP5] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59] INFO:     127.0.0.1:50320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:51356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:51702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:52134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:56652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:56700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:60506 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP6] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP0] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP2] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP4] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP1] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP3] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP7] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP5] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59] INFO:     127.0.0.1:49740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:50184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:51856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:51950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:53326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:53362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:55076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:55616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:56766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:57194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:57298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:59642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:60110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:32844 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP6] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP2] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP0] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP4] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP3] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP7] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP1] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP5] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59] INFO:     127.0.0.1:49808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:49860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:50578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:50590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:50838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:50990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:52214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:52946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:53254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:53338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:55248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:56222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:56824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:57146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:58642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:58760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:59150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:59180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:59244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:59392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:60236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:32788 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP4] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP6] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP0] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP2] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP3] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP7] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP1] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP5] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59] INFO:     127.0.0.1:51126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:52202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:52802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:53882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:54340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:57838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:58374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:59036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:59120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:59560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:60690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:33098 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP3] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP2] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP6] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP1] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP4] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP0] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP7] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP5] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59] INFO:     127.0.0.1:53610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:54440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:54864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:55752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:58352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:59726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:59750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:59786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:59788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:60052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:60630 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP6] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP4] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP0] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP2] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP3] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP1] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP7] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP5] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59] INFO:     127.0.0.1:51058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:51136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:51248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:52888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:54478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:04:59] INFO:     127.0.0.1:57896 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP6] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP4] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP3] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP5] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP1] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP0] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP7] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:04:59 TP2] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:00] INFO:     127.0.0.1:51370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:54712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:57344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:57372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:59236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:59912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:51694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:52320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:55366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:56066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:56074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:56808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:57252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:57974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:59670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:51668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:53314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:53866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:54634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:55286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:55376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:56002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:56632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:57080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:58326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:32826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:51412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:52478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:54408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:56004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:58838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:59866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:60036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:49782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:51550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:52154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:53728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:56410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:57634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:59412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:60732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:50248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:52640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:55832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:57002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:58066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:58608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:59138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:60252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:52580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:54028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:58362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:52360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:55930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:57246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:57336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:57672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:59304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:53628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:55160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:56208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:57046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:59806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:59972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:60332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:60604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:32832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:51546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:51718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:52046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:52546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:53478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:53758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:53794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:54510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:54684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:55532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:55584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:58862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:59048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:60092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:49710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:49768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:52944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:56350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:57320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:58418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:59760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:60872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:60934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:51260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:52496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:53672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:55172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:55486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:57716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:59146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:60512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:60982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:32878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:33086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:52844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:57104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:59480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:60866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:52992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:53016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:54458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:55760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:60526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:60614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:32934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:51240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:52188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:52508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:53256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:53446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:53580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:54578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:55258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:58812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:60022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:60268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:60318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:60468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:60726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:00] INFO:     127.0.0.1:60820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:52286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:52540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:54846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:55350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:60588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:33060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:54922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:54966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:60340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:60380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:60840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:50600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:51490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:56938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:57464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:60708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:60710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:60722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:51474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:55412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:56068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:56468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:58140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:58796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01 TP0] Decode batch, #running-req: 367, #token: 64176, token usage: 0.07, cuda graph: True, gen throughput (token/s): 6542.53, #queue-req: 0, 
[2025-11-01 11:05:01] INFO:     127.0.0.1:50692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:55874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:55906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:56878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:58878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:60654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:60954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:50172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:51598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:53448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:54992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:56434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:57100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:60386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:32966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:51674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:56288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:57386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:57788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:49884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:51416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:57482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:60602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:52084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:53154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:53384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:58160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:58896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:60464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:32800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:53868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:60572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:60624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:58934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:60452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:60802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:50234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:52820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:54604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:60460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:51574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:58302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:58466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:58852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:60910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:33046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:33092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:33144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:50398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:51534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:51946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:60004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:60524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:33142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:51344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:53808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:57272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:53342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:58174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:59506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:01] INFO:     127.0.0.1:33176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:53738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:55886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:58686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:59066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:59164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:60158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:32858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:33164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:51792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:52432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:55962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:59278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:60014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:32890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:59218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:59352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:60772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:33120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:60260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:60436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:32984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:50374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:50502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:56676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:59676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:32794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:56940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:60418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:51244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:52770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:55630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:55924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:56420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:32774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:50482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:52922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:56920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:58974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:33000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:33130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:50272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:32938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:52160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:51994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:53252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:55448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:51930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:54920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:58080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:60062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:60760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:60978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:32912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:49776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:57108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:60288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:60344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:60896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:60990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:33022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:51506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:52764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:58772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:59204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:60308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:60688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:32780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:50388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:51346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:59654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:59940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:60918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:32846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:32894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:50034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:51318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:51940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:55334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:56882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:58438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:32950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:52078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:53156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:53568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:56196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:57722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:59228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:59838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:59872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:51934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:60156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:60222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:60684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:60856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:59268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:02] INFO:     127.0.0.1:59406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:52624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:56194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:56198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:58906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:60814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:52082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:59056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:59964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:53430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:54860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:55224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:60074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:60150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:60304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:33106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:49824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:50328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:56888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:59902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:60692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:60754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:51204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:54402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:54590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:60382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:60536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:57498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:59382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:59812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:50878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:51872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:57920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:60400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03 TP0] Decode batch, #running-req: 152, #token: 34138, token usage: 0.04, cuda graph: True, gen throughput (token/s): 4556.46, #queue-req: 0, 
[2025-11-01 11:05:03] INFO:     127.0.0.1:51620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:59384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:59952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:60190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:50806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:53904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:59826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:60174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:60556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:32864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:50114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:51828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:50232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:50442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:52650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:55122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:59260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:59318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:59892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:60970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:58708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:59470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:59250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:60966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:32964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:59078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:60546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:55308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:56376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:59340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:60782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:50348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:59288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:50638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:55730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:03] INFO:     127.0.0.1:60456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:55640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:57578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:59088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:60104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:52344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:54908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:55896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:59690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:60160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:60364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:60854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:59934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:60370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:53680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:58964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:33146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:60118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:50546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:60650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:32970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:58694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:59018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:52226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:54658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:56450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:32834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:50520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:60218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:52382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:59882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:32862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:53664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:58928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:60878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:32920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:52482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:54388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:56768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:60278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:59458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:50722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:55506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:59124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:59634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:60634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:54480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:60124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:60888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:53482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:60800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:57072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:60562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:52688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:55002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:60740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:33072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:50966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:33160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:54980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:56090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:59810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:60202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:53638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:04] INFO:     127.0.0.1:56974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:57988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05 TP0] Decode batch, #running-req: 49, #token: 13741, token usage: 0.01, cuda graph: True, gen throughput (token/s): 2403.15, #queue-req: 0, 
[2025-11-01 11:05:05] INFO:     127.0.0.1:60942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:59530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:59742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:54056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:58504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:60490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:32990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:59522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:60830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:60936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:59102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:58026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:60668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:32898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:50514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:56842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:58548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:56158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:60420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:60450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:54222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:49838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:55090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:33016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:59770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:60798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:53374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:58578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:59548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:51038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:33030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:05] INFO:     127.0.0.1:60084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:06] INFO:     127.0.0.1:60140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:06] INFO:     127.0.0.1:60360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:06] INFO:     127.0.0.1:51278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:06] INFO:     127.0.0.1:59426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:06 TP0] Decode batch, #running-req: 13, #token: 4654, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1019.32, #queue-req: 0, 
[2025-11-01 11:05:06] INFO:     127.0.0.1:56980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:06] INFO:     127.0.0.1:53350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:06] INFO:     127.0.0.1:59612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:06] INFO:     127.0.0.1:57968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:06] INFO:     127.0.0.1:32772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:06] INFO:     127.0.0.1:60960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:06] INFO:     127.0.0.1:59362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:06] INFO:     127.0.0.1:56668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:06] INFO:     127.0.0.1:60410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:06] INFO:     127.0.0.1:60128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:06] INFO:     127.0.0.1:58732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:06] INFO:     127.0.0.1:58728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:19] INFO:     127.0.0.1:41762 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-01 11:05:19 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-01 11:05:20] INFO:     127.0.0.1:41774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:20 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-01 11:05:20 TP0] Prefill batch, #new-seq: 42, #new-token: 42, #cached-token: 30515, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-11-01 11:05:20 TP0] Prefill batch, #new-seq: 48, #new-token: 48, #cached-token: 34849, token usage: 0.01, #running-req: 43, #queue-req: 0, 
[2025-11-01 11:05:20 TP0] Prefill batch, #new-seq: 52, #new-token: 52, #cached-token: 38195, token usage: 0.01, #running-req: 91, #queue-req: 0, 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP7] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP2] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP6] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP4] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP5] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP1] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP3] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP0] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP0] Prefill batch, #new-seq: 55, #new-token: 55, #cached-token: 39971, token usage: 0.01, #running-req: 143, #queue-req: 0, 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP5] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP7] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP2] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP6] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP4] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP3] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP1] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP0] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP0] Prefill batch, #new-seq: 61, #new-token: 61, #cached-token: 44294, token usage: 0.02, #running-req: 198, #queue-req: 0, 
[2025-11-01 11:05:20 TP0] Prefill batch, #new-seq: 60, #new-token: 60, #cached-token: 43616, token usage: 0.02, #running-req: 259, #queue-req: 0, 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP5] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP4] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP7] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP6] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP1] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP2] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP3] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP0] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:20 TP0] Prefill batch, #new-seq: 67, #new-token: 67, #cached-token: 48810, token usage: 0.02, #running-req: 319, #queue-req: 0, 
[2025-11-01 11:05:20 TP0] Prefill batch, #new-seq: 65, #new-token: 65, #cached-token: 47584, token usage: 0.03, #running-req: 386, #queue-req: 0, 
[2025-11-01 11:05:21 TP0] Prefill batch, #new-seq: 74, #new-token: 74, #cached-token: 53584, token usage: 0.03, #running-req: 451, #queue-req: 0, 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP4] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP7] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP6] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP5] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP2] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP1] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP3] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP0] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP0] Prefill batch, #new-seq: 72, #new-token: 72, #cached-token: 52355, token usage: 0.04, #running-req: 525, #queue-req: 0, 
[2025-11-01 11:05:21 TP0] Prefill batch, #new-seq: 79, #new-token: 79, #cached-token: 57382, token usage: 0.04, #running-req: 597, #queue-req: 0, 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP7] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP5] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP6] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP4] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP2] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP1] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP3] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP0] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP0] Prefill batch, #new-seq: 77, #new-token: 77, #cached-token: 56083, token usage: 0.05, #running-req: 676, #queue-req: 0, 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP5] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP2] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP7] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP4] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP6] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP1] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP3] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP0] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP0] Prefill batch, #new-seq: 43, #new-token: 43, #cached-token: 31346, token usage: 0.05, #running-req: 753, #queue-req: 0, 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP7] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP4] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP5] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP6] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP1] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP3] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP2] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP0] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP7] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP5] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP6] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP4] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP1] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP2] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP0] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:21 TP3] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:22 TP0] Prefill batch, #new-seq: 46, #new-token: 46, #cached-token: 33756, token usage: 0.05, #running-req: 796, #queue-req: 0, 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:22 TP0] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:22 TP5] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:22 TP4] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:22 TP6] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:22 TP7] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:22 TP2] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:22 TP1] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:22 TP3] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:22 TP0] Prefill batch, #new-seq: 52, #new-token: 52, #cached-token: 37588, token usage: 0.06, #running-req: 842, #queue-req: 0, 
[2025-11-01 11:05:22 TP0] Prefill batch, #new-seq: 55, #new-token: 55, #cached-token: 40202, token usage: 0.06, #running-req: 894, #queue-req: 0, 
[2025-11-01 11:05:22 TP0] Prefill batch, #new-seq: 59, #new-token: 59, #cached-token: 43346, token usage: 0.06, #running-req: 949, #queue-req: 0, 
[2025-11-01 11:05:22 TP0] Prefill batch, #new-seq: 16, #new-token: 16, #cached-token: 11720, token usage: 0.07, #running-req: 1008, #queue-req: 45, 
[2025-11-01 11:05:23 TP0] Decode batch, #running-req: 1024, #token: 67461, token usage: 0.07, cuda graph: False, gen throughput (token/s): 290.89, #queue-req: 276, 
[2025-11-01 11:05:25] INFO:     127.0.0.1:44606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:25 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 711, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-11-01 11:05:25] INFO:     127.0.0.1:44628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:26 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 710, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-11-01 11:05:26] INFO:     127.0.0.1:42198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:26] INFO:     127.0.0.1:45342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:26] INFO:     127.0.0.1:46326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:26] INFO:     127.0.0.1:46394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:26] INFO:     127.0.0.1:41826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:26] INFO:     127.0.0.1:43350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:26] INFO:     127.0.0.1:46406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:26] INFO:     127.0.0.1:47462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:26 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2991, token usage: 0.11, #running-req: 1020, #queue-req: 289, 
[2025-11-01 11:05:26] INFO:     127.0.0.1:43258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:26] INFO:     127.0.0.1:43836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:26] INFO:     127.0.0.1:44392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:26] INFO:     127.0.0.1:47442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:26 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5922, token usage: 0.11, #running-req: 1016, #queue-req: 281, 
[2025-11-01 11:05:26] INFO:     127.0.0.1:42742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:26] INFO:     127.0.0.1:44622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:26] INFO:     127.0.0.1:46578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:26] INFO:     127.0.0.1:50328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:26] INFO:     127.0.0.1:50440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3657, token usage: 0.11, #running-req: 1019, #queue-req: 276, 
[2025-11-01 11:05:27] INFO:     127.0.0.1:42268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27] INFO:     127.0.0.1:42334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27] INFO:     127.0.0.1:42350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27] INFO:     127.0.0.1:44056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27] INFO:     127.0.0.1:46194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27] INFO:     127.0.0.1:46590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27] INFO:     127.0.0.1:46680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5032, token usage: 0.11, #running-req: 1017, #queue-req: 269, 
[2025-11-01 11:05:27] INFO:     127.0.0.1:43440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27] INFO:     127.0.0.1:44970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27] INFO:     127.0.0.1:46774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2177, token usage: 0.11, #running-req: 1021, #queue-req: 266, 
[2025-11-01 11:05:27 TP0] Decode batch, #running-req: 1021, #token: 106166, token usage: 0.11, cuda graph: False, gen throughput (token/s): 8976.43, #queue-req: 266, 
[2025-11-01 11:05:27] INFO:     127.0.0.1:42238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27] INFO:     127.0.0.1:42304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27] INFO:     127.0.0.1:44880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27] INFO:     127.0.0.1:45526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27] INFO:     127.0.0.1:46570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27] INFO:     127.0.0.1:46962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27] INFO:     127.0.0.1:48600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5034, token usage: 0.11, #running-req: 1017, #queue-req: 259, 
[2025-11-01 11:05:27] INFO:     127.0.0.1:46154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27] INFO:     127.0.0.1:50430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27] INFO:     127.0.0.1:50562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2175, token usage: 0.11, #running-req: 1021, #queue-req: 256, 
[2025-11-01 11:05:27] INFO:     127.0.0.1:42002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27] INFO:     127.0.0.1:42908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27] INFO:     127.0.0.1:45110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27] INFO:     127.0.0.1:47476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:27] INFO:     127.0.0.1:49732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3607, token usage: 0.11, #running-req: 1019, #queue-req: 251, 
[2025-11-01 11:05:28] INFO:     127.0.0.1:41802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:43934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:44260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:44576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:48032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:49976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4420, token usage: 0.11, #running-req: 1018, #queue-req: 245, 
[2025-11-01 11:05:28] INFO:     127.0.0.1:42612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:43538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:46196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:47544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:48892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:49284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:49528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:49876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:50510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6639, token usage: 0.11, #running-req: 1015, #queue-req: 236, 
[2025-11-01 11:05:28] INFO:     127.0.0.1:42708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:45666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:45920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:46044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:46132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:46716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:47350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:48116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:49102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:49354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:49674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:50890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8822, token usage: 0.11, #running-req: 1012, #queue-req: 224, 
[2025-11-01 11:05:28] INFO:     127.0.0.1:43796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:46864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:47236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:48010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:48214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:48580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:49334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5049, token usage: 0.12, #running-req: 1017, #queue-req: 217, 
[2025-11-01 11:05:28] INFO:     127.0.0.1:42596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:42680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:43982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:44068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:44796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:45936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:46348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:46982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:47652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:48160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:28] INFO:     127.0.0.1:50752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7926, token usage: 0.12, #running-req: 1013, #queue-req: 206, 
[2025-11-01 11:05:29] INFO:     127.0.0.1:42462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:42800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:43512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:44960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:45308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:45840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:49754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5051, token usage: 0.12, #running-req: 1017, #queue-req: 199, 
[2025-11-01 11:05:29] INFO:     127.0.0.1:42770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:43478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:45210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:46298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:47526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:48080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:49688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5115, token usage: 0.12, #running-req: 1017, #queue-req: 192, 
[2025-11-01 11:05:29] INFO:     127.0.0.1:43552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:43582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:44398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:45032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:47228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:47888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:48230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:48680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5841, token usage: 0.12, #running-req: 1016, #queue-req: 184, 
[2025-11-01 11:05:29] INFO:     127.0.0.1:41790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:42420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:43124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:43680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:43714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:44148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:44296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:44364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:44468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:46770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:47294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7966, token usage: 0.12, #running-req: 1013, #queue-req: 173, 
[2025-11-01 11:05:29] INFO:     127.0.0.1:42868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:42890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:43108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:44236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:44680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:44988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:47724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:50050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:29] INFO:     127.0.0.1:50602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6456, token usage: 0.12, #running-req: 1015, #queue-req: 164, 
[2025-11-01 11:05:30] INFO:     127.0.0.1:42438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:43994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:46212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:46282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:47616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:47832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:48494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:49548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5792, token usage: 0.12, #running-req: 1016, #queue-req: 156, 
[2025-11-01 11:05:30] INFO:     127.0.0.1:44434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:46274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:46526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:48348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:48390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:49044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4374, token usage: 0.12, #running-req: 1018, #queue-req: 150, 
[2025-11-01 11:05:30] INFO:     127.0.0.1:43566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:46170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:48860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:49062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:49312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:49484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:50414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5157, token usage: 0.12, #running-req: 1017, #queue-req: 143, 
[2025-11-01 11:05:30] INFO:     127.0.0.1:41926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:42092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:43324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:43520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:44162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:46762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:47280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:47366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:48022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:50070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:50344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30] INFO:     127.0.0.1:50840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:30 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8734, token usage: 0.12, #running-req: 1012, #queue-req: 131, 
[2025-11-01 11:05:31] INFO:     127.0.0.1:42530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:43166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:44134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:46886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:47062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:50010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4363, token usage: 0.12, #running-req: 1018, #queue-req: 125, 
[2025-11-01 11:05:31] INFO:     127.0.0.1:44492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:45012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:45138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:45222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:46018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:47150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:47742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:47784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:48668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:48746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:48854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:49118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:49812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:50308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10360, token usage: 0.12, #running-req: 1010, #queue-req: 111, 
[2025-11-01 11:05:31] INFO:     127.0.0.1:42544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:42762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:43004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:43048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:43202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:44748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:44766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:45776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:49772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:49890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7424, token usage: 0.12, #running-req: 1014, #queue-req: 101, 
[2025-11-01 11:05:31] INFO:     127.0.0.1:42284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:42700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:43188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:43226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:43396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:44442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:44580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:44724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:45480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:46088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:46294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:46834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:48414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:50132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10151, token usage: 0.12, #running-req: 1010, #queue-req: 87, 
[2025-11-01 11:05:31] INFO:     127.0.0.1:42984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:44078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:44448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:49142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:49628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31] INFO:     127.0.0.1:49804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:31 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4314, token usage: 0.12, #running-req: 1018, #queue-req: 81, 
[2025-11-01 11:05:32] INFO:     127.0.0.1:41974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:42010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:42562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:43868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:44172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:44752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:44894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:45218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:46668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:47424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:47432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:47536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:48782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:49210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:49700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:50106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:50954 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:32 TP1] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:32 TP2] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:32 TP5] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:32 TP6] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:32 TP0] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:32 TP3] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:32 TP4] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:32 TP7] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:32 TP0] Prefill batch, #new-seq: 17, #new-token: 17, #cached-token: 12506, token usage: 0.12, #running-req: 1007, #queue-req: 64, 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:32 TP2] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:32 TP5] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:32 TP1] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:32 TP3] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:32 TP7] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:32 TP6] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:32 TP4] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:32 TP0] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:32] INFO:     127.0.0.1:42546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:43500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:44114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:45046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:46352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:46814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:47258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:47324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:47452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:48638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:49824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8176, token usage: 0.13, #running-req: 1013, #queue-req: 53, 
[2025-11-01 11:05:32] INFO:     127.0.0.1:43434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:44254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:45634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:45926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:46186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:46634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:46902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:47364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:47578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:47730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:48062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:49792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:49956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:50240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10177, token usage: 0.13, #running-req: 1010, #queue-req: 39, 
[2025-11-01 11:05:32] INFO:     127.0.0.1:44256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:44506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:44556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:44996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:47092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:47140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:48552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:48732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:49150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:49532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:49590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:49602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:50684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:50844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10168, token usage: 0.13, #running-req: 1010, #queue-req: 25, 
[2025-11-01 11:05:32] INFO:     127.0.0.1:42066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:43038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:43242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:43546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:44822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:47766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:48096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:48250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:48660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:50266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7323, token usage: 0.13, #running-req: 1014, #queue-req: 15, 
[2025-11-01 11:05:32] INFO:     127.0.0.1:42252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:42618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:44214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:45530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:46458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:47922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:48466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:49096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:32] INFO:     127.0.0.1:50390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6572, token usage: 0.13, #running-req: 1015, #queue-req: 6, 
[2025-11-01 11:05:33] INFO:     127.0.0.1:42644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:42690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:45182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:45198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:45236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:45678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:47844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:48906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:49014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:50828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4320, token usage: 0.13, #running-req: 1014, #queue-req: 0, 
[2025-11-01 11:05:33] INFO:     127.0.0.1:41976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:42964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:43604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:47548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:47632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:47902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:48358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:48608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:49022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:49664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:50716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:43418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:43700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:43834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:45622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:46168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:46236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:47918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:49724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:50880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:42280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:44420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:44698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:44900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:46118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:46310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:46720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:46848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:47820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:47864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:48352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:48698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:48920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:50538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:50656 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:33 TP5] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:33 TP2] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:33 TP6] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:33 TP0] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:33 TP7] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:33 TP1] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:33 TP3] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:33 TP4] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:33] INFO:     127.0.0.1:42692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:43266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:43488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:43644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:44004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:44474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:46000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:46020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:46222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:47050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:47566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:48424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:49636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:49970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:49988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:50044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:50576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:50786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:42224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:42316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:43254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:44828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:46754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:46840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:48840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:49006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:49156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:33] INFO:     127.0.0.1:49282 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:33 TP7] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:33 TP4] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:33 TP6] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:33 TP0] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:33 TP3] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:33 TP1] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:33 TP5] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:33 TP2] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34] INFO:     127.0.0.1:42230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:42244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:42576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:44514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:44638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:45794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:45796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:46238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:46426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:46800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:47038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:50304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:42000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:42188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:42486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:42796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:43944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:44082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:44824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:46488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:46596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:46926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:48686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:49860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:49942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:50278 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP6] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP7] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP3] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP2] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP5] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP1] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP4] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP0] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34] INFO:     127.0.0.1:42940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:42946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:43136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:46998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:48194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:48440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:49328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:49480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:50544 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP2] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP5] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP3] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP6] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP7] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP1] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP0] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP4] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34] INFO:     127.0.0.1:43074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:43692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:44476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:45062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:47312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:47708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:49882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:49910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:42500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:42988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:43856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:44032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:44944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:45384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:45648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:47976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:48760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:49242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:50238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:50660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:50686 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP2] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP5] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP1] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP6] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP4] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP7] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP0] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP3] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34] INFO:     127.0.0.1:42082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:42174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:43180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:43626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:45354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:46072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:47300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:48870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:48940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:49418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:49998 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP1] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP5] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP6] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP2] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP0] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP3] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP7] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP4] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP0] Decode batch, #running-req: 901, #token: 115003, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5658.72, #queue-req: 0, 
[2025-11-01 11:05:34] INFO:     127.0.0.1:42012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:42372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:42452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:43164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:43320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:45592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:45890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:46790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:47624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:49250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:49462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:51036 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP1] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP5] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP6] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP2] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP3] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP0] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP4] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP7] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34] INFO:     127.0.0.1:42324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:44052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:44380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:44624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:45102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:45624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:47058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:48278 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP1] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP2] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP6] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP5] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP0] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP3] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP4] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP7] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34] INFO:     127.0.0.1:41878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:42052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:42088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:42140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:43280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:43842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:44186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:44322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:45272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:45542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:49748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:49928 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP1] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP0] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP2] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP4] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP3] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP7] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP5] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP6] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34] INFO:     127.0.0.1:42120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:42842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:44756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:44906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:45334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:46664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:46806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:46942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:47126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:48374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:48624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:48956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:49392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:50254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:34] INFO:     127.0.0.1:50836 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP1] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP2] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP5] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP3] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP6] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP7] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP4] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:34 TP0] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35] INFO:     127.0.0.1:41854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:43152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:44528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:48614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:49818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:50516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:50566 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP1] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP2] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP5] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP4] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP0] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP3] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP6] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP7] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35] INFO:     127.0.0.1:45122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:45176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:46888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:48648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:49716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:49864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:50222 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP1] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP2] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP5] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP0] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP3] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP6] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP7] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP4] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35] INFO:     127.0.0.1:42134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:42854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:42896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:43314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:43910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:44040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:45428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:45802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:47240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:48318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:49580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:50080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:50894 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP1] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP2] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP0] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP3] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP4] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP5] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP6] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP7] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35] INFO:     127.0.0.1:41834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:42696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:42924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:43442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:45092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:45964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:46472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:46870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:47608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:48156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:48700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:49524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:50666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:50970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:51014 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP1] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP2] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP5] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP4] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP0] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP6] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP7] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP3] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35] INFO:     127.0.0.1:45616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:46374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:46452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:48298 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP1] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP2] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP5] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP6] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP0] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP3] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP7] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP4] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35] INFO:     127.0.0.1:44574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:44876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:46966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:47856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:49138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:49598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:50180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:50346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:50674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:51448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:51680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:41846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:42900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:44692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:44932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:45258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:45688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:45752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:46258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:48586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:48696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:49514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:49758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:50604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:50616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:51068 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP1] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP2] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP5] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP6] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP0] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP3] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP4] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP7] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35] INFO:     127.0.0.1:45150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:45704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:45880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:46002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:47070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:47402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:47756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:48168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:43336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:43642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:43816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:44310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:46104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:46242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:47490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:50148 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP1] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP2] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP5] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP6] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP0] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP4] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP3] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35 TP7] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:35] INFO:     127.0.0.1:41990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:42008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:47180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:47412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:48208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:48852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:49192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:50368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:35] INFO:     127.0.0.1:52198 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP5] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP6] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP1] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP2] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP4] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP0] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP7] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP3] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36] INFO:     127.0.0.1:43304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:43726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:46058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:46704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:50122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:50592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:51604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:52200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:42806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:44202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:44496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:45604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:46146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:46510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:47662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:48832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:49500 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP1] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP6] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP5] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP2] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP4] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP3] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP7] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP0] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP1] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP2] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP5] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP6] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP3] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP0] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP4] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP7] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36] INFO:     127.0.0.1:42074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:42720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:44860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:45734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:48346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:49274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:50498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:41882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:43210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:43786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:43876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:44094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:44372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:44888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:45076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:45532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:46204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:47406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:48398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:48564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:49160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:50438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:50490 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (706, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP1] [fused_moe] using default for (706, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (706, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP2] [fused_moe] using default for (706, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (706, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP5] [fused_moe] using default for (706, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (706, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (706, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (706, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP6] [fused_moe] using default for (706, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP0] [fused_moe] using default for (706, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP3] [fused_moe] using default for (706, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (706, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (706, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP4] [fused_moe] using default for (706, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP7] [fused_moe] using default for (706, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36] INFO:     127.0.0.1:42108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:42688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:43092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:45608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:46362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:46378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:48242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:49912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:51710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:51846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:45494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:46216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:46946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:47112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:47692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:48184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:48822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:51160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:51300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:52130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:42642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:43458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:44812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:48044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:48504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:49238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:49244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:50374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:50624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:50874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:51794 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP1] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP2] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP5] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP6] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP3] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP7] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP0] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP4] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36] INFO:     127.0.0.1:41920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:45240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:45918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:46768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:46826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:47076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:47212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:49230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:49380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:50996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:52324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:52898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:53344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:41952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:43186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:43400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:43890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:44466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:44732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:45302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:45466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:46554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:46608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:47110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:47996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:48110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:49290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:50372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:51226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:51818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:52182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:52472 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP1] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP2] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP5] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP6] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP3] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP7] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP4] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP0] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36] INFO:     127.0.0.1:41928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:42360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:45578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:45632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:46284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:47810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:50194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:36] INFO:     127.0.0.1:50476 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP1] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP2] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP0] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP3] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP5] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP7] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP6] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:36 TP4] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37] INFO:     127.0.0.1:42882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:43968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:44044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:44276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:45478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:48076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:48452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:49074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:49402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:51844 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP1] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP2] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP6] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP5] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP4] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP3] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP7] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP0] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37] INFO:     127.0.0.1:43246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:43742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:46438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:47264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:47734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:49082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:49318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:50746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:50860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:51060 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP1] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP2] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP5] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP6] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP0] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP3] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP7] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP4] [fused_moe] using default for (615, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37] INFO:     127.0.0.1:41936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:42728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:47018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:47638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:50280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:50764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:51024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:53502 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP1] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP2] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP5] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP6] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP3] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP0] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP4] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP7] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37] INFO:     127.0.0.1:43956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:46746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:48858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:50206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:50412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:51260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:51552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:53074 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP1] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP2] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP5] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP6] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP3] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP7] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP4] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP0] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37] INFO:     127.0.0.1:41896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:44410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:45024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:45242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:46028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:46102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:48026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:52282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:52702 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP1] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP2] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP0] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP6] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP5] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP4] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP7] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP3] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37] INFO:     127.0.0.1:42024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:42398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:43568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:43990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:45106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:47098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:47774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:48738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:48808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:50386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:50452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:51726 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP1] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP2] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP5] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP6] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP3] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP0] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP7] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP4] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37] INFO:     127.0.0.1:42780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:43732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:44018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:46016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:48716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:49750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:50732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:52276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:52706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:53320 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP1] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP2] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP5] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP6] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP3] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP7] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP4] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP0] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37] INFO:     127.0.0.1:44774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:44830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:44958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:45952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:48332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:49626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:50936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:51284 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP1] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP2] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP0] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP3] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP5] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP4] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP6] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP7] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37] INFO:     127.0.0.1:42470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:43380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:43528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:44220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:46372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:47196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:47270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:48882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:49260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:49346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:51144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:51366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:51518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:51864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:53420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:42506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:47302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:47806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:49226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:49450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:50702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:50720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:50754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:51678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:37] INFO:     127.0.0.1:52924 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP1] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP2] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP5] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP3] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP4] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP6] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP0] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:37 TP7] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:38] INFO:     127.0.0.1:42072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:42902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:43806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:44660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:44758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:44898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:48036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:51418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:51842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:51938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:52240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:52872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:53030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:43508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:43770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:44174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:47826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:52118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:52562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:44362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:44590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:45850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:46320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:46758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:48148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:50012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:51230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:51894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:52170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:52424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:44460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:45960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:46200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:47398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:49438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:50486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:51778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:52960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:42986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:44942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:45372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:45504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:47792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:49122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:51532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:51718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:41866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:42196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:42340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:43532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:43654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:44210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:45742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:45814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:46688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:48520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:49174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:50018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:51694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38 TP0] Decode batch, #running-req: 489, #token: 77878, token usage: 0.08, cuda graph: True, gen throughput (token/s): 7178.64, #queue-req: 0, 
[2025-11-01 11:05:38] INFO:     127.0.0.1:42424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:43364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:44540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:44980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:47450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:47866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:51248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:51944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:43480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:44608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:46108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:46500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:48480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:48780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:49648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:50098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:51350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:52056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:46448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:47998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:46536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:47158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:51116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:52224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:43620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:44098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:44572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:45864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:46736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:47564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:49302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:49406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:49776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:50812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:51748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:51924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:52274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:52506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:53378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:41810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:42954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:49172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:51582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:53122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:45762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:46520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:46544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:47194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:47514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:49432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:51268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:53194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:45324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:47596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:48046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:48536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:50522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:51986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:53096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:53288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:53432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:38] INFO:     127.0.0.1:53622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:44998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:48444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:49202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:53374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:53504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:53630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:44566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:45672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:46624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:46980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:47274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:48644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:51960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:53044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:42030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:43330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:44846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:51318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:49838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:51090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:53612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:42814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:43570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:45450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:48772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:50460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:46850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:48304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:48588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:50358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:51762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:44242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:47928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:47964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:49038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:49562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:50800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:51222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:51526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:53214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:43670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:44350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:50060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:51814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:51848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:53532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:44278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:48968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:50402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:50986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:51236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:41962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:42404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:45990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:49194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:42828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:45512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:49478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:49900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:51540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:47940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:48382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:51564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:51690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:51828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:53334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:53716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:43800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:46542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:50314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:51020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:51102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:51664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:42114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:42160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:43290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:43318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:46656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:47386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:49568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:51926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:43472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:43668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:47872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:51474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:43918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:44326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:47584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:50168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:51010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:51168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:51436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:52188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:53178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:53606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:53638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:39] INFO:     127.0.0.1:53698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:49370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:51618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:52440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:43020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:45404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:45826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:50790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:53684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:43748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:44118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:46642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:47370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:51194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:51312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:53670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:47992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:48128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:48524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:48692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:48856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:48982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:50342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:52342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:52530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:53468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:53710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:41856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:43872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:51324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:51406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:53292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:53426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:43588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:45418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:45502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:50654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:52462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:52738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:52998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:42282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:42658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:45558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:45720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:51812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:52644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:44338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:49056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:52078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:53358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:53552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:43970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:44918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:51458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:52500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:42044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:43082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:44784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:47480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:53306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:53560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:52842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:44648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:45904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:48286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:49028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:50570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:53530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:53656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:46938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:41886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:45288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:50192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:52624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:52226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:52344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:53484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:53580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:42386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:50638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:51070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:51326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:52244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:52990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:53290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40 TP0] Decode batch, #running-req: 213, #token: 42536, token usage: 0.04, cuda graph: True, gen throughput (token/s): 5478.23, #queue-req: 0, 
[2025-11-01 11:05:40] INFO:     127.0.0.1:45660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:46334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:47678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:51452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:52140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:52904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:44334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:45792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:46900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:50552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:53172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:53314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:42294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:43466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:45164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:51338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:52030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:40] INFO:     127.0.0.1:52366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:44302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:52902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:53106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:53402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:45978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:47242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:49208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:52392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:47802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:48290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:51008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:41862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:43844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:44886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:47010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:49852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:48016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:48262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:51128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:53052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:53460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:47032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:52026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:52100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:52936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:53528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:44772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:51486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:52154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:52068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:42998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:48992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:51630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:52014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:52576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:49612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:44664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:52596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:50034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:51502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:52766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:53448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:42514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:42626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:42670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:52132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:52374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:53256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:53270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:43716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:44844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:51382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:53280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:51404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:51594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:52338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:53156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:51424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:52774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:53518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:41904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:42148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:51808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:42752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:41] INFO:     127.0.0.1:47334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:51198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:53010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:43820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:51420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:52666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:42590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:45392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:52268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:52546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:43766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:46914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:47138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:53486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:53650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:48144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:49684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:52296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:52360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:53084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:42776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:45356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:47954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:51050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:50766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:52572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:52876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:53696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:52110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:53386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:53566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:50162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:51082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:43022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:44288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:46724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:50294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:47500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:47654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:52778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:46414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:48570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:53540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:51576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:53412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42 TP0] Decode batch, #running-req: 80, #token: 19688, token usage: 0.02, cuda graph: True, gen throughput (token/s): 3065.89, #queue-req: 0, 
[2025-11-01 11:05:42] INFO:     127.0.0.1:43756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:43900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:48912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:51736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:45758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:51018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:52388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:48796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:53126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:50094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:52978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:51246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:45564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:52298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:53142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:52792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:53020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:44710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:49184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:51970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:52088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:42] INFO:     127.0.0.1:43356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:43058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:52968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:53592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:48142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:46268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:42976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:53230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:47006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:48924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:49090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:51646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:51878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:53730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:52692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:52880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:52912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:53206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:51214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:42208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:52446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:48236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:53544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:44538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:52630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:52254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:52610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:47168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:53014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:53068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:43406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43] INFO:     127.0.0.1:45746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:43 TP0] Decode batch, #running-req: 26, #token: 7616, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1408.68, #queue-req: 0, 
[2025-11-01 11:05:43] INFO:     127.0.0.1:51662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:44] INFO:     127.0.0.1:42640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:44] INFO:     127.0.0.1:52522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:44] INFO:     127.0.0.1:53570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:44] INFO:     127.0.0.1:50920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:44] INFO:     127.0.0.1:53480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:44] INFO:     127.0.0.1:52336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:44] INFO:     127.0.0.1:45442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:44] INFO:     127.0.0.1:51184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:44] INFO:     127.0.0.1:50904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:44] INFO:     127.0.0.1:51910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:44] INFO:     127.0.0.1:51562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:44] INFO:     127.0.0.1:51712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:44] INFO:     127.0.0.1:52400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:44] INFO:     127.0.0.1:52604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:44] INFO:     127.0.0.1:53242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:44] INFO:     127.0.0.1:52314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:44 TP0] Decode batch, #running-req: 7, #token: 3298, token usage: 0.00, cuda graph: True, gen throughput (token/s): 605.96, #queue-req: 0, 
[2025-11-01 11:05:44] INFO:     127.0.0.1:48748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:44] INFO:     127.0.0.1:51468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:45] INFO:     127.0.0.1:50680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:45] INFO:     127.0.0.1:52438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:45] INFO:     127.0.0.1:50782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:45] INFO:     127.0.0.1:49068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:45 TP0] Decode batch, #running-req: 1, #token: 1077, token usage: 0.00, cuda graph: True, gen throughput (token/s): 144.70, #queue-req: 0, 
[2025-11-01 11:05:46 TP0] Decode batch, #running-req: 1, #token: 1117, token usage: 0.00, cuda graph: True, gen throughput (token/s): 56.36, #queue-req: 0, 
[2025-11-01 11:05:46] INFO:     127.0.0.1:43632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:59] INFO:     127.0.0.1:57390 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-01 11:05:59 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-01 11:05:59] INFO:     127.0.0.1:57392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:05:59 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-01 11:05:59 TP0] Prefill batch, #new-seq: 41, #new-token: 41, #cached-token: 29724, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:59 TP0] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:59 TP5] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:59 TP6] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:59 TP7] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:59 TP4] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:59 TP1] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:59 TP3] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:59 TP2] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:05:59 TP0] Prefill batch, #new-seq: 48, #new-token: 48, #cached-token: 34956, token usage: 0.01, #running-req: 42, #queue-req: 0, 
[2025-11-01 11:06:00 TP0] Prefill batch, #new-seq: 52, #new-token: 52, #cached-token: 37909, token usage: 0.01, #running-req: 90, #queue-req: 0, 
[2025-11-01 11:06:00 TP0] Prefill batch, #new-seq: 56, #new-token: 56, #cached-token: 40941, token usage: 0.01, #running-req: 142, #queue-req: 0, 
[2025-11-01 11:06:00 TP0] Prefill batch, #new-seq: 59, #new-token: 59, #cached-token: 43026, token usage: 0.02, #running-req: 198, #queue-req: 0, 
[2025-11-01 11:06:00 TP0] Prefill batch, #new-seq: 68, #new-token: 68, #cached-token: 49224, token usage: 0.02, #running-req: 257, #queue-req: 0, 
[2025-11-01 11:06:00 TP0] Prefill batch, #new-seq: 65, #new-token: 65, #cached-token: 47363, token usage: 0.02, #running-req: 325, #queue-req: 0, 
[2025-11-01 11:06:00 TP0] Prefill batch, #new-seq: 74, #new-token: 74, #cached-token: 54095, token usage: 0.03, #running-req: 390, #queue-req: 0, 
[2025-11-01 11:06:00 TP0] Prefill batch, #new-seq: 70, #new-token: 70, #cached-token: 50770, token usage: 0.03, #running-req: 464, #queue-req: 0, 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:00 TP5] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:00 TP6] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:00 TP7] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:00 TP4] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:00 TP2] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:00 TP3] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:00 TP1] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:00 TP0] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP0] Prefill batch, #new-seq: 82, #new-token: 82, #cached-token: 59589, token usage: 0.04, #running-req: 534, #queue-req: 0, 
[2025-11-01 11:06:01 TP0] Prefill batch, #new-seq: 72, #new-token: 72, #cached-token: 52268, token usage: 0.04, #running-req: 616, #queue-req: 0, 
[aiter] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP5] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP6] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP4] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP7] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP1] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP3] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP2] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP0] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP0] Prefill batch, #new-seq: 22, #new-token: 22, #cached-token: 16056, token usage: 0.05, #running-req: 688, #queue-req: 0, 
[aiter] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP5] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP4] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP6] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP0] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP7] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP2] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP3] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP1] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP0] Prefill batch, #new-seq: 50, #new-token: 50, #cached-token: 36411, token usage: 0.05, #running-req: 710, #queue-req: 0, 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP0] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP5] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP4] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP6] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP7] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP2] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP3] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP1] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP0] Prefill batch, #new-seq: 51, #new-token: 51, #cached-token: 37215, token usage: 0.05, #running-req: 760, #queue-req: 0, 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP5] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP4] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP6] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP2] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP7] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP3] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP1] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP0] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:01 TP0] Prefill batch, #new-seq: 60, #new-token: 60, #cached-token: 43745, token usage: 0.06, #running-req: 811, #queue-req: 0, 
[2025-11-01 11:06:01 TP0] Prefill batch, #new-seq: 59, #new-token: 59, #cached-token: 42845, token usage: 0.06, #running-req: 871, #queue-req: 0, 
[2025-11-01 11:06:02 TP0] Prefill batch, #new-seq: 70, #new-token: 70, #cached-token: 51376, token usage: 0.06, #running-req: 930, #queue-req: 0, 
[2025-11-01 11:06:02 TP0] Prefill batch, #new-seq: 24, #new-token: 24, #cached-token: 17697, token usage: 0.06, #running-req: 1000, #queue-req: 32, 
[2025-11-01 11:06:03 TP0] Decode batch, #running-req: 1024, #token: 72380, token usage: 0.07, cuda graph: False, gen throughput (token/s): 566.46, #queue-req: 295, 
[2025-11-01 11:06:04] INFO:     127.0.0.1:60368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:05 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 735, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-11-01 11:06:05] INFO:     127.0.0.1:60412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:05 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 737, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-11-01 11:06:06] INFO:     127.0.0.1:58132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:06] INFO:     127.0.0.1:59140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:06] INFO:     127.0.0.1:32884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:06] INFO:     127.0.0.1:34036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:06] INFO:     127.0.0.1:35126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:06] INFO:     127.0.0.1:57422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:06] INFO:     127.0.0.1:58492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:06] INFO:     127.0.0.1:33598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:06 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3663, token usage: 0.11, #running-req: 1019, #queue-req: 288, 
[2025-11-01 11:06:06] INFO:     127.0.0.1:58312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:06] INFO:     127.0.0.1:59484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:06] INFO:     127.0.0.1:60238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:06 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4436, token usage: 0.11, #running-req: 1018, #queue-req: 282, 
[2025-11-01 11:06:06] INFO:     127.0.0.1:57646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:06] INFO:     127.0.0.1:60390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:06] INFO:     127.0.0.1:34192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:06] INFO:     127.0.0.1:35132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:06 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2934, token usage: 0.11, #running-req: 1020, #queue-req: 278, 
[2025-11-01 11:06:06] INFO:     127.0.0.1:57788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:06] INFO:     127.0.0.1:57866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:06] INFO:     127.0.0.1:58260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:06] INFO:     127.0.0.1:58394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:06] INFO:     127.0.0.1:33722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:06] INFO:     127.0.0.1:34208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:06] INFO:     127.0.0.1:34302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:06] INFO:     127.0.0.1:38054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5761, token usage: 0.11, #running-req: 1016, #queue-req: 270, 
[2025-11-01 11:06:07] INFO:     127.0.0.1:57562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:59052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:34412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:37926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2897, token usage: 0.11, #running-req: 1020, #queue-req: 266, 
[2025-11-01 11:06:07] INFO:     127.0.0.1:60098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:60690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:33060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:34604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2922, token usage: 0.11, #running-req: 1020, #queue-req: 262, 
[2025-11-01 11:06:07] INFO:     127.0.0.1:33676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:34518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:35410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:38024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:38166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3573, token usage: 0.11, #running-req: 1019, #queue-req: 257, 
[2025-11-01 11:06:07] INFO:     127.0.0.1:58048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:58184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:59756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:60104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:60920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:34182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:35158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:36180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:37366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6573, token usage: 0.11, #running-req: 1015, #queue-req: 248, 
[2025-11-01 11:06:07] INFO:     127.0.0.1:57398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:58782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:59190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:59626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:60348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:36696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:07] INFO:     127.0.0.1:37612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5190, token usage: 0.11, #running-req: 1017, #queue-req: 241, 
[2025-11-01 11:06:08] INFO:     127.0.0.1:58616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:60810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:35224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:36952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:37154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:37290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:37520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:38100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5772, token usage: 0.11, #running-req: 1016, #queue-req: 233, 
[2025-11-01 11:06:08 TP0] Decode batch, #running-req: 1016, #token: 109695, token usage: 0.11, cuda graph: False, gen throughput (token/s): 8068.74, #queue-req: 233, 
[2025-11-01 11:06:08] INFO:     127.0.0.1:57946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:58322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:33208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:34046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:34102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:34338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:35020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:35694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:37022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:38394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7405, token usage: 0.11, #running-req: 1014, #queue-req: 223, 
[2025-11-01 11:06:08] INFO:     127.0.0.1:59956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:34900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:36504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:37006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2915, token usage: 0.12, #running-req: 1020, #queue-req: 219, 
[2025-11-01 11:06:08] INFO:     127.0.0.1:58436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:58742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:59642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:60596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:34610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:35300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:38504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5053, token usage: 0.12, #running-req: 1017, #queue-req: 212, 
[2025-11-01 11:06:08] INFO:     127.0.0.1:58706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:58746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:60200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:60796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:32862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:33358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:34088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:35492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:35658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:35846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:37210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:37388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:38284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:08] INFO:     127.0.0.1:38530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10095, token usage: 0.12, #running-req: 1010, #queue-req: 198, 
[2025-11-01 11:06:09] INFO:     127.0.0.1:58340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:59100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:60026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:33962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:35806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:37314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:37472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5116, token usage: 0.12, #running-req: 1017, #queue-req: 191, 
[2025-11-01 11:06:09] INFO:     127.0.0.1:57394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:59198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:59236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:33726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:35546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:37912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4372, token usage: 0.12, #running-req: 1018, #queue-req: 185, 
[2025-11-01 11:06:09] INFO:     127.0.0.1:58240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:58890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:59408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:59432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:59586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:59886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:60052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:60860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:34402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:35748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:35760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:36162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:37656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9410, token usage: 0.12, #running-req: 1011, #queue-req: 172, 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:09 TP6] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:09 TP5] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:09 TP3] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:09 TP2] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:09 TP7] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:09 TP4] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:09 TP1] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:09 TP0] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:09] INFO:     127.0.0.1:57754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:58382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:58944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:59600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:60460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:60820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:35384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:35420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:35862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:36274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:36350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:38242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8636, token usage: 0.12, #running-req: 1012, #queue-req: 160, 
[2025-11-01 11:06:09] INFO:     127.0.0.1:58086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:33756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:35290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:09] INFO:     127.0.0.1:37166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2884, token usage: 0.12, #running-req: 1020, #queue-req: 156, 
[2025-11-01 11:06:10] INFO:     127.0.0.1:59316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:60072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:60986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:33506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:33692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:33904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:34952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5097, token usage: 0.12, #running-req: 1017, #queue-req: 149, 
[2025-11-01 11:06:10] INFO:     127.0.0.1:57980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:59208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:59966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:32982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:33916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:36642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:36644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:37114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5960, token usage: 0.12, #running-req: 1016, #queue-req: 141, 
[2025-11-01 11:06:10] INFO:     127.0.0.1:58398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:58962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:59712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:34418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:34954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:35046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:35968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:36030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:37698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6489, token usage: 0.12, #running-req: 1015, #queue-req: 132, 
[2025-11-01 11:06:10] INFO:     127.0.0.1:57676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:58480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:58926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:34158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:34718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:36482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:36742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:37634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5839, token usage: 0.12, #running-req: 1016, #queue-req: 124, 
[2025-11-01 11:06:10] INFO:     127.0.0.1:59808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:60198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:60842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:32768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:33542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:34822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:35436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:35678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:36450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:36730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:10] INFO:     127.0.0.1:37322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8136, token usage: 0.12, #running-req: 1013, #queue-req: 113, 
[2025-11-01 11:06:11] INFO:     127.0.0.1:57784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:57860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:58632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:58834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:60504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:60540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:33276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:34698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:35764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:36336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:36962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:37406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:37550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:38528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:38560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 11084, token usage: 0.12, #running-req: 1009, #queue-req: 98, 
[2025-11-01 11:06:11] INFO:     127.0.0.1:57652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:57844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:59006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:59200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:59774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:60366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:60486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:60918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:60938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:33028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:33764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:33872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:33878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:34478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:36840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:37436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:37452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:37754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:37950 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:11 TP5] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:11 TP3] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:11 TP2] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:11 TP6] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:11 TP4] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:11 TP1] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:11 TP0] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:11 TP7] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:11 TP0] Prefill batch, #new-seq: 19, #new-token: 19, #cached-token: 13730, token usage: 0.12, #running-req: 1005, #queue-req: 79, 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:11 TP6] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:11 TP2] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:11 TP3] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:11 TP5] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:11 TP4] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:11 TP1] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:11 TP7] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:11 TP0] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:11] INFO:     127.0.0.1:57704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:58884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:58970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:59156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:59724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:60138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:60166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:34528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:35120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:35144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:37252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8119, token usage: 0.12, #running-req: 1013, #queue-req: 68, 
[2025-11-01 11:06:11] INFO:     127.0.0.1:57612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:58296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:58482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:60042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:60520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:32798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:33408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:34288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:35114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:35214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:36038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:36510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:37502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:37752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10382, token usage: 0.12, #running-req: 1010, #queue-req: 54, 
[2025-11-01 11:06:11] INFO:     127.0.0.1:59130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:59544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:59740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:60864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:33986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:34454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:34928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:34992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:11] INFO:     127.0.0.1:35394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6543, token usage: 0.12, #running-req: 1015, #queue-req: 45, 
[2025-11-01 11:06:12] INFO:     127.0.0.1:59032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:59364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:32834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:33154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:33402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:34228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:34392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:34538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:35034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:35264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:35742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:36264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:37430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:37854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10167, token usage: 0.12, #running-req: 1010, #queue-req: 31, 
[2025-11-01 11:06:12] INFO:     127.0.0.1:59622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:60204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:60314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:60622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:60836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:32998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:34742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:36192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:36218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:36392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:36536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:37158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:37200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:37590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:38424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 10948, token usage: 0.12, #running-req: 1009, #queue-req: 16, 
[2025-11-01 11:06:12] INFO:     127.0.0.1:58146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:58594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:58684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:58796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:60626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:33414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:33564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:35568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:36616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:36744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:37352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:37882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8735, token usage: 0.12, #running-req: 1012, #queue-req: 4, 
[2025-11-01 11:06:12] INFO:     127.0.0.1:57768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:59250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:59874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:33068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:33576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:36434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:36704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:37988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:38286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:38456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:38542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2890, token usage: 0.12, #running-req: 1013, #queue-req: 0, 
[2025-11-01 11:06:12] INFO:     127.0.0.1:58214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:58766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:58932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:60956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:32778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:35494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:35890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:36246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:36610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:38440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:12] INFO:     127.0.0.1:38592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:57638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:58976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:60728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:35236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:35310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:35544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:36056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:36110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:36148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:37302 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP6] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP2] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP3] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP5] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP4] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP1] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP7] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP0] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13] INFO:     127.0.0.1:57896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:59040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:59118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:59848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:60180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:33128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:33714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:33968 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP6] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP2] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP1] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP7] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP4] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP3] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP5] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP0] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13] INFO:     127.0.0.1:59500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:60474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:60754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:34000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:34010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:34346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:35250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:35448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:35790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:35978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:37078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:37194 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP6] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP2] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP3] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP1] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP5] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP7] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP4] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP0] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13] INFO:     127.0.0.1:58350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:58568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:58618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:59332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:59928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:33492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:33642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:37556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:37682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:38142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:38188 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP7] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP4] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP1] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP6] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP3] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP2] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP0] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP5] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13] INFO:     127.0.0.1:58826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:60638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:33740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:34468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:36300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:36592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:36768 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP1] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP3] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP7] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP4] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP5] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP0] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP2] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP6] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13] INFO:     127.0.0.1:57686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:58388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:58692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:59456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:34022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:34434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:34484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:34644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:34692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:37100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:37902 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP6] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP1] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP2] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP7] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP3] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP0] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP5] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP4] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13] INFO:     127.0.0.1:57578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:57880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:58010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:58470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:60082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:60434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:33138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:33334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:34218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:34558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:37574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:37880 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP3] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP5] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP6] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP4] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP7] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP1] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP2] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP0] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13] INFO:     127.0.0.1:58468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:58548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:60402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:35562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:36298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:36990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:37616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:38154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:13] INFO:     127.0.0.1:38344 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP4] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP7] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP1] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP0] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP2] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP3] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP6] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:13 TP5] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14] INFO:     127.0.0.1:58166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:58662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:58722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:59380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:60276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:60894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:33300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:34996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:35518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:36044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:37528 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP5] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP2] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP6] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP1] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP4] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP7] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP0] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP3] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14] INFO:     127.0.0.1:57792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:58256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:58872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:59762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:59866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:60798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:33162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:34878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:35824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:36278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:36888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:37838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:38124 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP3] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP5] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP6] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP4] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP2] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP1] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP7] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP0] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14] INFO:     127.0.0.1:58176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:58204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:58516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:59264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:59896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:32790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:32908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:33196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:34438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:34980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:36122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:36554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:36780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:36940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:37058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:37642 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP6] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP3] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP5] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP2] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP4] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP7] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP1] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP0] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14] INFO:     127.0.0.1:57730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:57816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:58518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:59444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:60000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:33094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:33374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:33520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:35772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:36358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:36756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:36912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:38724 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP5] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP3] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP6] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP2] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP4] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP1] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP7] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP0] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14] INFO:     127.0.0.1:57444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:58500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:59060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:59648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:59692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:60396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:60910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:33890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:34504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:34706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:34850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:35618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:36466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:36544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:37866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:57490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:58798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:60536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:33070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:35296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:35904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:37030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:37266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:37382 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP5] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP6] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP2] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP4] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP3] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP1] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP7] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP0] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14] INFO:     127.0.0.1:58004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:58188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:60452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:60740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:60932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:32896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:34282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:34794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:38434 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP5] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP2] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP4] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP6] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP3] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP1] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP0] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP7] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14] INFO:     127.0.0.1:57456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:57718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:58646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:58674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:59742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:60284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:33222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:34800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:36002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:38172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14 TP0] Decode batch, #running-req: 840, #token: 110554, token usage: 0.11, cuda graph: False, gen throughput (token/s): 6046.82, #queue-req: 0, 
[2025-11-01 11:06:14] INFO:     127.0.0.1:58982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:60928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:33186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:34408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:35502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:36204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:36290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:36684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:37510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:37566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:37852 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP5] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP6] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP3] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP2] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP4] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP1] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP7] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP0] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14] INFO:     127.0.0.1:57658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:57902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:58096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:59560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:59998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:33306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:34892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:35470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:36198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:38356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:14] INFO:     127.0.0.1:38482 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP3] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP5] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP6] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP2] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP4] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP7] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP1] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:14 TP0] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15] INFO:     127.0.0.1:57428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:58422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:58530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:58578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:33442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:33984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:34076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:34636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:35288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:35474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:35816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:36230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:37148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:37188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:37708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:37742 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP6] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP5] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP3] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP2] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP4] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP7] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP1] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP0] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15] INFO:     127.0.0.1:57858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:60980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:33122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:33246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:33268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:35200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:37834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:37990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:39120 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP3] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP5] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP6] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP7] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP2] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP1] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP4] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP0] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15] INFO:     127.0.0.1:35876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:37792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:38370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:38408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:39326 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP3] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP2] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP6] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP5] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP4] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP1] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP7] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP0] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15] INFO:     127.0.0.1:58038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:60676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:60758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:33220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:33662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:34062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:35930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:37134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:37390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:37460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:38226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:38768 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP3] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP4] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP6] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP5] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP2] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP7] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP1] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP0] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15] INFO:     127.0.0.1:58228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:60944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:32812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:33364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:33472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:34432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:34722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:35408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:35944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:36328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:38820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:59286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:59476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:60326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:33736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:33788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:35372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:36874 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP3] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP5] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP6] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP2] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP4] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP1] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP7] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP0] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15] INFO:     127.0.0.1:57622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:58078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:59452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:35084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:35822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:36814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:37336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:38010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:39768 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP2] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP3] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP5] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP6] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP4] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP1] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP7] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP0] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15] INFO:     127.0.0.1:33624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:34328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:35484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:36438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:37958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:38204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:38462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:39278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:39780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:59820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:33548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:33582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:35842 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP5] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP3] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP6] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP4] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP2] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP1] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP7] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15 TP0] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:15] INFO:     127.0.0.1:57766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:58794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:60652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:33708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:35056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:36904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:38096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:15] INFO:     127.0.0.1:38110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:57504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:58230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:58338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:59812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:60718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:33072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:33824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:34132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:35098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:58540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:59804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:33098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:33842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:34244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:36154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:37558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:39470 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP5] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP3] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP6] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP2] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP4] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP1] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP7] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP0] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16] INFO:     127.0.0.1:59534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:60878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:32814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:32948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:33608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:33774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:34380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:34780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:35358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:35952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:36014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:37122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:38264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:38444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:39222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:39342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:39708 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP5] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP3] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP6] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP2] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP4] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP0] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP7] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP1] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16] INFO:     127.0.0.1:58918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:34470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:35646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:36174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:37462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:37772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:37974 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP5] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP3] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP2] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP6] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP7] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP1] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP4] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP0] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16] INFO:     127.0.0.1:59070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:32856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:32972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:34736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:34882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:35830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:36412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:36870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:36896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:38904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:39396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:39878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:40460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:40850 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP5] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP3] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP6] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP2] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP4] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP1] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP7] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP0] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16] INFO:     127.0.0.1:57598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:58604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:59004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:59952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:60264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:60492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:33646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:33928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:34186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:35724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:36958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:37066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:37968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:38380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:39442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:39744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:40064 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP3] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP6] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP5] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP4] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP2] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP1] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP7] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP0] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16] INFO:     127.0.0.1:58112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:33092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:33922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:34592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:35582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:36094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:37418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:38084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:57748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:58892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:59918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:59974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:60606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:33114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:35074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:37040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:39468 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP2] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP3] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP5] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP6] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP4] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP7] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP1] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP0] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16] INFO:     127.0.0.1:58390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:60222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:34920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:35412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:38608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:38756 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP3] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP5] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP2] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP6] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP1] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP4] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP7] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16 TP0] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:16] INFO:     127.0.0.1:34750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:35322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:35730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:36976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:37892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:37934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:37996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:38580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:39866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:16] INFO:     127.0.0.1:40992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:57516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:58282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:58428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:60376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:35330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:36082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:37034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:38488 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP5] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP6] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP7] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP3] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP2] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP4] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP1] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP0] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17] INFO:     127.0.0.1:60782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:60852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:33870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:38300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:38682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:40290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:57558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:58062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:58954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:59982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:33430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:33482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:33640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:34764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:35428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:38942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:39362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:58378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:60084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:36428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:37016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:37236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:37386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:38480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:40302 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP3] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP2] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP6] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP5] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP1] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP7] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP4] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP0] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17] INFO:     127.0.0.1:57570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:58300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:58310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:58600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:59430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:59794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:60560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:32942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:35528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:36312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:36344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:36408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:37822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:40908 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP3] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP6] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP5] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP2] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP1] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP7] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP4] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP0] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17] INFO:     127.0.0.1:58414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:59428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:32818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:33854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:34872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:34938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:35168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:35960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:36926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:38976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:39054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:39178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:40480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:40598 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP5] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP3] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP6] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP2] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP1] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP7] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP0] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP4] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:57950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:59002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:60628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:36448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:36670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:36854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:38078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:38804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:39320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:39504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:39806 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP3] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP6] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP2] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP5] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP1] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP7] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP4] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP0] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17] INFO:     127.0.0.1:59940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:60554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:60734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:33400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:35278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:39458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:39558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:39852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:40422 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP2] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP6] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP3] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP5] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP1] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP7] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP4] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP0] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17] INFO:     127.0.0.1:59138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:59300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:35990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:36568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:38044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:39700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:40160 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP5] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP3] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP2] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP6] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP7] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP1] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP4] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17 TP0] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:17] INFO:     127.0.0.1:59574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:60150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:32846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:33346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:34394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:34530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:35708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:35776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:37620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:38914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:39034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:39516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:40000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:59170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:59682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:60080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:33030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:33418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:34312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:17] INFO:     127.0.0.1:38062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:57990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:58764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:59182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:59846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:60254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:34144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:35440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:35800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:36494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:39764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:39792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:40866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:57468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:58724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:59704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:33320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:36806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:37084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:37640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:39404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:40514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:32924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:33818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:35082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:35524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:35714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:37000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:38950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:39570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:39832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:60970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:33380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:33530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:34668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:35456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:37094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:37274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:37732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:39646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:58088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:60358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:34964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:36716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:39088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:59112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:60298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:32990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:36100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:36382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:40092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:59258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:59902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:34364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:35258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:35626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:37052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:38324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:38798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:39380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:39538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:39580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:40406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:40888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18 TP0] Decode batch, #running-req: 454, #token: 74064, token usage: 0.08, cuda graph: True, gen throughput (token/s): 6870.69, #queue-req: 0, 
[2025-11-01 11:06:18] INFO:     127.0.0.1:57412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:35924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:36256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:36788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:39242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:40660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:40710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:59714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:33130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:33270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:34174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:35186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:36960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:38960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:40786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:41122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:59420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:60660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:37048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:38136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:40322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:41136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:57836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:58986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:60812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:34860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:37606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:39574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:40948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:40996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:60062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:60158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:32872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:34248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:34942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:36818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:37486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:38574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:39790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:40086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:40338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:40398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:40518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:40616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:40652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:57666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:59276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:60772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:34496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:36066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:36130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:38622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:39002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:39336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:40288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:40394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:40622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:40818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:41118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:57740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:33076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:34622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:35130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:18] INFO:     127.0.0.1:38546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:58362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:59234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:35068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:40120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:40482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:40498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:59218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:60322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:34836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:35592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:36370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:40272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:34582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:36632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:37176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:38882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:40456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:59356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:33044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:34654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:37668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:37952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:40724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:41034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:59836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:59860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:35602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:36564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:40236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:57586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:33438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:36138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:36816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:37196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:38776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:38940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:40184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:40382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:37538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:38722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:40522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:41226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:40034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:40280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:40368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:40846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:58128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:59468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:38792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:40008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:58396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:34264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:36010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:36600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:38038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:38336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:40714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:41110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:41150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:41188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:60128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:35632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:37776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:40048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:41184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:59082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:33396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:34116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:38690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:40016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:58858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:60208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:32944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:33338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:40922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:57932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:33014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:33248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:33284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:41170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:41198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:57688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:37932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:38990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:40960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:60830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:39132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:19] INFO:     127.0.0.1:40584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:57462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:35672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:38870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:39018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:39206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:41054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:58908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:59346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:37914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:38002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:38826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:38854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:40256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:40306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:59790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:38214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:39418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:40832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:60588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:35348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:38278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:40214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:40430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:40540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:40806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:41070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:36638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:38764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:40108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:41160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:38312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:41018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:60426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:60766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:35900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:58760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:59614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:34570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:59528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:60650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:39794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:40770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:41092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:60016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:33804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:40526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:40794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:40982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:58672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:32958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:36126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:39012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:40132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:40810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:59238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:32914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:38464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:38652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:40472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:58290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:59552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:40898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:41022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:58272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:33458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:39612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:39722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:33236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:39978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:40462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:35150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:39024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:39072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:39924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20 TP0] Decode batch, #running-req: 195, #token: 41176, token usage: 0.04, cuda graph: True, gen throughput (token/s): 5195.66, #queue-req: 0, 
[2025-11-01 11:06:20] INFO:     127.0.0.1:36178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:36830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:59516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:38492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:38842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:39678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:39904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:40880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:40966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:36410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:39758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:39812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:20] INFO:     127.0.0.1:40656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:35908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:38514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:40070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:40334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:58738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:33172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:34280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:34912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:40700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:57526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:57954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:36582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:38164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:39586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:40172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:39432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:39726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:40332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:40546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:40666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:59394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:60444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:39146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:40188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:40932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:37650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:39286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:37222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:39068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:39112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:39970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:40494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:40946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:58022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:35266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:35896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:39610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:59656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:60574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:35692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:39654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:40694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:57918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:39258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:58162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:40768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:41006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:34682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:35010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:35594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:39668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:40552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:58452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:34524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:38896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:39840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:41152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:60112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:60154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:39098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:40240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:35346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:37282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:39130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:34550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:39692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:40636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:40752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:58846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:39872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:39956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:40140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:21] INFO:     127.0.0.1:41186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:37896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:40154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:38740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:41082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:57966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:59086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:36650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:41036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:60706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:39050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:34350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:59448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:40442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:58552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:40994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:36158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:40204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:38706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:39940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:38874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:38926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:40682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:38778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:40354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:40358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:33828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:35180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:36454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:57882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:59670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:33266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:39236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:39990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:40448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22 TP0] Decode batch, #running-req: 72, #token: 18356, token usage: 0.02, cuda graph: True, gen throughput (token/s): 2900.01, #queue-req: 0, 
[2025-11-01 11:06:22] INFO:     127.0.0.1:34812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:39364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:40572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:58824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:59022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:57802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:36654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:36792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:39874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:40524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:33088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:41134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:60342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:35786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:36522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:39164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:37724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:39592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:33940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:40732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:22] INFO:     127.0.0.1:34656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:39474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:60472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:40720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:57542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:41040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:41212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:38252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:39298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:39666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:40612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:58814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:40294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:32928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:34792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:36540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:37766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:40126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:37808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:40020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:33948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:35894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:38638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:39816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:40468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:40224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:40560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:33250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:58524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:60190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:38372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:39226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:41108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23 TP0] Decode batch, #running-req: 17, #token: 5390, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1254.32, #queue-req: 0, 
[2025-11-01 11:06:23] INFO:     127.0.0.1:40804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:40978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:60296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:23] INFO:     127.0.0.1:39890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:24] INFO:     127.0.0.1:39528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:24] INFO:     127.0.0.1:39346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:24] INFO:     127.0.0.1:40740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:24] INFO:     127.0.0.1:32968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:24] INFO:     127.0.0.1:39316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:24] INFO:     127.0.0.1:36372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:24] INFO:     127.0.0.1:39974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:24] INFO:     127.0.0.1:40196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:24] INFO:     127.0.0.1:39138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:24 TP0] Decode batch, #running-req: 4, #token: 1707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 457.92, #queue-req: 0, 
[2025-11-01 11:06:24] INFO:     127.0.0.1:38484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:24] INFO:     127.0.0.1:39876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:24] INFO:     127.0.0.1:38540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:24] INFO:     127.0.0.1:38668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:37] INFO:     127.0.0.1:46042 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-01 11:06:37 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-01 11:06:37] INFO:     127.0.0.1:46058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:37 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-01 11:06:37 TP0] Prefill batch, #new-seq: 41, #new-token: 41, #cached-token: 29649, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-11-01 11:06:38 TP0] Prefill batch, #new-seq: 49, #new-token: 49, #cached-token: 35779, token usage: 0.01, #running-req: 42, #queue-req: 0, 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP4] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP0] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP5] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP6] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP7] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP1] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP2] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP3] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP0] Prefill batch, #new-seq: 51, #new-token: 51, #cached-token: 37318, token usage: 0.01, #running-req: 91, #queue-req: 0, 
[2025-11-01 11:06:38 TP0] Prefill batch, #new-seq: 58, #new-token: 58, #cached-token: 42276, token usage: 0.01, #running-req: 142, #queue-req: 0, 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP7] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP6] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP4] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP5] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP3] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP0] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP2] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP1] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP0] Prefill batch, #new-seq: 57, #new-token: 57, #cached-token: 41347, token usage: 0.02, #running-req: 200, #queue-req: 0, 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP7] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP4] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP2] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP6] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP5] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP1] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP3] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP0] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:38 TP0] Prefill batch, #new-seq: 67, #new-token: 67, #cached-token: 48682, token usage: 0.02, #running-req: 257, #queue-req: 0, 
[2025-11-01 11:06:38 TP0] Prefill batch, #new-seq: 61, #new-token: 61, #cached-token: 44440, token usage: 0.02, #running-req: 324, #queue-req: 0, 
[2025-11-01 11:06:38 TP0] Prefill batch, #new-seq: 72, #new-token: 72, #cached-token: 52648, token usage: 0.03, #running-req: 385, #queue-req: 0, 
[2025-11-01 11:06:38 TP0] Prefill batch, #new-seq: 67, #new-token: 67, #cached-token: 48602, token usage: 0.03, #running-req: 457, #queue-req: 0, 
[2025-11-01 11:06:39 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7219, token usage: 0.03, #running-req: 524, #queue-req: 0, 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:39 TP5] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:39 TP6] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:39 TP4] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:39 TP7] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:39 TP1] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:39 TP3] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:39 TP2] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:39 TP0] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:39 TP0] Prefill batch, #new-seq: 16, #new-token: 16, #cached-token: 11521, token usage: 0.03, #running-req: 534, #queue-req: 0, 
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP0] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP4] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP7] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP5] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP6] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP1] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP2] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP3] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-01 11:06:39 TP0] Prefill batch, #new-seq: 48, #new-token: 48, #cached-token: 34964, token usage: 0.04, #running-req: 550, #queue-req: 0, 
[2025-11-01 11:06:39 TP0] Prefill batch, #new-seq: 49, #new-token: 49, #cached-token: 35592, token usage: 0.04, #running-req: 598, #queue-req: 0, 
[2025-11-01 11:06:39 TP0] Prefill batch, #new-seq: 60, #new-token: 60, #cached-token: 43756, token usage: 0.04, #running-req: 647, #queue-req: 0, 
[2025-11-01 11:06:39 TP0] Prefill batch, #new-seq: 55, #new-token: 55, #cached-token: 39988, token usage: 0.05, #running-req: 707, #queue-req: 0, 
[2025-11-01 11:06:39 TP0] Prefill batch, #new-seq: 68, #new-token: 68, #cached-token: 49777, token usage: 0.05, #running-req: 762, #queue-req: 0, 
[2025-11-01 11:06:39 TP0] Prefill batch, #new-seq: 65, #new-token: 65, #cached-token: 47119, token usage: 0.06, #running-req: 830, #queue-req: 0, 
[2025-11-01 11:06:40 TP0] Prefill batch, #new-seq: 75, #new-token: 75, #cached-token: 54958, token usage: 0.06, #running-req: 895, #queue-req: 0, 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:40 TP5] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:40 TP7] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:40 TP4] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:40 TP6] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:40 TP3] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:40 TP2] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:40 TP1] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:40 TP0] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:40 TP0] Prefill batch, #new-seq: 54, #new-token: 54, #cached-token: 39561, token usage: 0.06, #running-req: 970, #queue-req: 15, 
[2025-11-01 11:06:42] INFO:     127.0.0.1:49122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:43 TP0] Decode batch, #running-req: 1024, #token: 89863, token usage: 0.09, cuda graph: False, gen throughput (token/s): 1479.66, #queue-req: 295, 
[2025-11-01 11:06:43 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 711, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-11-01 11:06:43] INFO:     127.0.0.1:54820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:43 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 710, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-11-01 11:06:43] INFO:     127.0.0.1:49150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:43 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 737, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-11-01 11:06:44] INFO:     127.0.0.1:46420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:44] INFO:     127.0.0.1:49842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:44] INFO:     127.0.0.1:50796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:44] INFO:     127.0.0.1:46110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:44] INFO:     127.0.0.1:46970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:44] INFO:     127.0.0.1:51018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:44 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2254, token usage: 0.11, #running-req: 1021, #queue-req: 289, 
[2025-11-01 11:06:44] INFO:     127.0.0.1:47760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:44] INFO:     127.0.0.1:48364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:44] INFO:     127.0.0.1:48988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:44 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4457, token usage: 0.11, #running-req: 1018, #queue-req: 283, 
[2025-11-01 11:06:44] INFO:     127.0.0.1:47266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:44] INFO:     127.0.0.1:49134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:44] INFO:     127.0.0.1:51892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2198, token usage: 0.11, #running-req: 1021, #queue-req: 280, 
[2025-11-01 11:06:45] INFO:     127.0.0.1:46748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:46770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:46958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:47352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:48532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:50774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:51122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:51936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:54790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6504, token usage: 0.11, #running-req: 1015, #queue-req: 271, 
[2025-11-01 11:06:45] INFO:     127.0.0.1:47690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:47818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:51126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:51216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:53476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3606, token usage: 0.11, #running-req: 1019, #queue-req: 266, 
[2025-11-01 11:06:45] INFO:     127.0.0.1:49370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:49998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:51324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:53012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:54912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:55060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4353, token usage: 0.11, #running-req: 1018, #queue-req: 260, 
[2025-11-01 11:06:45] INFO:     127.0.0.1:50724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:50836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:51472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:54234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:54938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3594, token usage: 0.11, #running-req: 1019, #queue-req: 255, 
[2025-11-01 11:06:45] INFO:     127.0.0.1:46838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:47292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:48440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:48462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:52232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:45] INFO:     127.0.0.1:55424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4377, token usage: 0.11, #running-req: 1018, #queue-req: 249, 
[2025-11-01 11:06:46] INFO:     127.0.0.1:46082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:46372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:47950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:48542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:49092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:51992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:53758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:54034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:54386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:54990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7396, token usage: 0.11, #running-req: 1014, #queue-req: 239, 
[2025-11-01 11:06:46] INFO:     127.0.0.1:47542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:49494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:49650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:52474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:53520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:53850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:54204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:54472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:55440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6583, token usage: 0.11, #running-req: 1015, #queue-req: 230, 
[2025-11-01 11:06:46] INFO:     127.0.0.1:47634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:50190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:50956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:50978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:51108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:53306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:53820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5132, token usage: 0.11, #running-req: 1017, #queue-req: 223, 
[2025-11-01 11:06:46] INFO:     127.0.0.1:48190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:48316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:50440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:51254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:51824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:51964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:52566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:55284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5838, token usage: 0.12, #running-req: 1016, #queue-req: 215, 
[2025-11-01 11:06:46] INFO:     127.0.0.1:47032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:47602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:48004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:49286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:51392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:51732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:52444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:54118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:46] INFO:     127.0.0.1:54246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6422, token usage: 0.12, #running-req: 1015, #queue-req: 206, 
[2025-11-01 11:06:47] INFO:     127.0.0.1:46800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:47230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:47890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:49506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:49794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:50370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:52622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:54206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:54314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6510, token usage: 0.12, #running-req: 1015, #queue-req: 197, 
[2025-11-01 11:06:47] INFO:     127.0.0.1:47516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:50946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:51480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:52098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:53150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:54704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:54774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5108, token usage: 0.12, #running-req: 1017, #queue-req: 190, 
[2025-11-01 11:06:47] INFO:     127.0.0.1:46072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:47972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:48294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:49580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:50550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:52530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:52984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5103, token usage: 0.12, #running-req: 1017, #queue-req: 183, 
[2025-11-01 11:06:47] INFO:     127.0.0.1:46772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:47000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:48118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:48172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:48654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:48814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:48846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:48932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:48958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:52360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:52672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:53084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:55118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9446, token usage: 0.12, #running-req: 1011, #queue-req: 170, 
[2025-11-01 11:06:47] INFO:     127.0.0.1:47132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:47396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:48688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:48738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:49188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:49512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:50598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:51764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:52660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:47] INFO:     127.0.0.1:54054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7163, token usage: 0.12, #running-req: 1014, #queue-req: 160, 
[2025-11-01 11:06:48] INFO:     127.0.0.1:46866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:47844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:48518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:50742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:50966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:52192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:52238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:53452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5849, token usage: 0.12, #running-req: 1016, #queue-req: 152, 
[2025-11-01 11:06:48] INFO:     127.0.0.1:49696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:50712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:52102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:53312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:53982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:54398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4356, token usage: 0.12, #running-req: 1018, #queue-req: 146, 
[2025-11-01 11:06:48] INFO:     127.0.0.1:47554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:47988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:50614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:52788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:52822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:54578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4441, token usage: 0.12, #running-req: 1018, #queue-req: 140, 
[2025-11-01 11:06:48] INFO:     127.0.0.1:46262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:46482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:46556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:47136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:47212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:48546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:49686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:51304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:51908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:52322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:54512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7992, token usage: 0.12, #running-req: 1013, #queue-req: 129, 
[2025-11-01 11:06:48] INFO:     127.0.0.1:46910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:47706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:51294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:51774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:51836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:52464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:53526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:48] INFO:     127.0.0.1:54568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5848, token usage: 0.12, #running-req: 1016, #queue-req: 121, 
[2025-11-01 11:06:49] INFO:     127.0.0.1:48212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:48778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:48950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:49552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:49562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:51580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:52552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:53134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:53638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:53774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:54266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:54346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8922, token usage: 0.12, #running-req: 1012, #queue-req: 109, 
[2025-11-01 11:06:49] INFO:     127.0.0.1:46662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:47048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:47660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:47784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:49218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:49238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:50290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:50394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:51670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:52264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:54278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:54624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8830, token usage: 0.12, #running-req: 1012, #queue-req: 97, 
[2025-11-01 11:06:49] INFO:     127.0.0.1:46470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:46524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:47238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:47746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:47962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:49098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:49214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:49660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:49966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:50584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:50764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:50838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:51280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:51398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:54050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:54158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:54286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49 TP0] Prefill batch, #new-seq: 17, #new-token: 17, #cached-token: 12263, token usage: 0.12, #running-req: 1007, #queue-req: 80, 
[2025-11-01 11:06:49] INFO:     127.0.0.1:47476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:48812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:48926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:51364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:53430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:54628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:55172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5102, token usage: 0.12, #running-req: 1017, #queue-req: 73, 
[2025-11-01 11:06:49 TP0] Decode batch, #running-req: 1017, #token: 119417, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5917.72, #queue-req: 73, 
[2025-11-01 11:06:49] INFO:     127.0.0.1:46302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:46676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:46928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:47296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:47898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:48596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:48888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:48966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:49228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:49] INFO:     127.0.0.1:49734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7407, token usage: 0.12, #running-req: 1014, #queue-req: 63, 
[2025-11-01 11:06:50] INFO:     127.0.0.1:47908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:48106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:48320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:48420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:49588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:50752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:51198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:51982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:52526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:53200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:54274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8209, token usage: 0.13, #running-req: 1013, #queue-req: 52, 
[2025-11-01 11:06:50] INFO:     127.0.0.1:48150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:50160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:50420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:51034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:51342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:51744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:51810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:51884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:51912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:52176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:52330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:52342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:52950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:53036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:53348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:54106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:54120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:54468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:55358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50 TP0] Prefill batch, #new-seq: 19, #new-token: 19, #cached-token: 13781, token usage: 0.13, #running-req: 1005, #queue-req: 33, 
[2025-11-01 11:06:50] INFO:     127.0.0.1:47418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:48248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:49038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:49318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:51144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:51834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:52052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:52216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:52914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:53556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:54720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:55346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8752, token usage: 0.13, #running-req: 1012, #queue-req: 21, 
[2025-11-01 11:06:50] INFO:     127.0.0.1:46378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:46898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:47588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:49304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:50208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:50444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:50762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:51598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:51652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:53248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:53414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:53504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:54222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:55320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10220, token usage: 0.13, #running-req: 1010, #queue-req: 7, 
[2025-11-01 11:06:50] INFO:     127.0.0.1:48042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:48504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:52544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:52698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:53098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:54896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:50] INFO:     127.0.0.1:55206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5038, token usage: 0.13, #running-req: 1017, #queue-req: 0, 
[2025-11-01 11:06:51] INFO:     127.0.0.1:47448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:47560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:47572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:49712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:52116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:52894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:53072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:53390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:54182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:55238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:46324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:46992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:49400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:53320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:54874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:46758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:47798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:48332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:49720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:50118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:50526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:50806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:50930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:52000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:52796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:53024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:53944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:55036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:48692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:49410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:50800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:50908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:52758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:54454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:54538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:55088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:55296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:47064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:47504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:47864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:48830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:51054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:52024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:52272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:52808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:53114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:53264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:53568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:54236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:47026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:51550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:52858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:54748 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP1] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP3] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP2] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP0] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP6] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP4] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP7] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP5] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51] INFO:     127.0.0.1:46278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:46546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:46602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:46988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:47618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:48272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:49142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:50620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:51238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:51332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:51352 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP4] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP0] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP1] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP3] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP7] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP5] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP6] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP2] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51] INFO:     127.0.0.1:46792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:46938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:48852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:49334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:51370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:51534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:53116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:53790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:53958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:54450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:54486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:51] INFO:     127.0.0.1:55054 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP3] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP4] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP0] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP5] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP7] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP1] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP2] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:51 TP6] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52] INFO:     127.0.0.1:46704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:47002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:47120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:47254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:51132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:51444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:51708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:52864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:53536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:54358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:54408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:55390 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP7] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP5] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP4] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP1] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP3] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP0] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP2] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP6] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52] INFO:     127.0.0.1:47334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:48162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:49010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:49056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:49606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:50304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:50694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:51488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:52364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:52640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:53668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:54686 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP7] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP4] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP5] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP3] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP1] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP0] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP2] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP6] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52] INFO:     127.0.0.1:46364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:48724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:48856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:49480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:50176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:51808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:52336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:52906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:53572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:53726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:53918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:54516 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP7] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP4] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP1] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP3] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP5] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP0] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP2] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP6] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52] INFO:     127.0.0.1:46382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:46474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:46648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:48048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:48242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:48672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:49722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:52414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:46492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:46618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:47498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:48282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:49844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:50070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:51072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:51780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:53282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:54712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:55536 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP1] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP3] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP4] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP5] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP7] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP0] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP2] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP6] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52] INFO:     127.0.0.1:47810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:48560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:48796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:49138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:49614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:50132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:50652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:50848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:50896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:52084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:53370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:53372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:53864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:54168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:54244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:46196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:46630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:47060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:47546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:47670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:49766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:51466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:51554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:51678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:52370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:53270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:55258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:46508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:46688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:47170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:49182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:49246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:49434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:49658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:49834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:50228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:51008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:52510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:52810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:55062 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP1] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP3] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP7] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP5] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP4] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP0] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP2] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52 TP6] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:52] INFO:     127.0.0.1:46144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:47286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:48626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:51186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:51338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:51642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:52714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:53044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:53112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:53176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:53478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:54424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:52] INFO:     127.0.0.1:54674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:46256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:47530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:47666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:48284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:49694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:50264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:50646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:54070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:54356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:54600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:55198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:46506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:46612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:46882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:47272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:50012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:50330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:50860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:50882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:51308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:52614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:53056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:53552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:54024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:46122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:47068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:47090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:49626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:50472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:50558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:51328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:51720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:52690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:52842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:54260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:55520 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP1] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP3] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP5] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP7] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP4] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP0] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP2] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP6] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53] INFO:     127.0.0.1:51020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:51386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:52068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:53118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:54642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:54824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:55184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:55210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:55908 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP1] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP3] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP4] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP7] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP5] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP0] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP2] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP6] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53] INFO:     127.0.0.1:46136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:46408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:48090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:50022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:51568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:51974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:52306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:54012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:54302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:56136 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP1] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP4] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP3] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP7] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP5] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP0] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP2] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP6] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53] INFO:     127.0.0.1:46586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:46986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:47124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:49364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:49464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:50044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:50214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:50904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:51058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:51484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:52334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:55102 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP3] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP5] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP4] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP7] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP1] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP0] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP6] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP2] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53] INFO:     127.0.0.1:49664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:50388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:53590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:53662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:47462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:48468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:48538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:48584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:48610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:49198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:49386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:50666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:50680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:50910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:51414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:51426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:51858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:52228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:52636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:54210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:54836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:55006 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP1] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP4] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP7] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP3] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP0] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP5] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP2] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP6] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53] INFO:     127.0.0.1:46316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:46452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:48384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:50084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:50318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:50510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:53108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:53260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:54608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:55072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:56660 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP1] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP3] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP0] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP4] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP7] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP5] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP2] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53 TP6] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:53] INFO:     127.0.0.1:46536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:48398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:49012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:51870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:52290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:52658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:53996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:54878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:55410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:56082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:56168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:53] INFO:     127.0.0.1:56664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:47596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:47732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:49594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:50702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:50862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:51224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:53696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:54976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:55000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:50038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:51736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:52136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:52438 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP1] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP7] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP3] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP4] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP0] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP5] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP2] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP6] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54] INFO:     127.0.0.1:46204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:46830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:48408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:48820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:49404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:49952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:50054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:54690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:55220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:56022 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP1] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP3] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP2] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP4] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP7] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP5] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP0] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP6] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54] INFO:     127.0.0.1:46252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:46780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:46814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:50234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:51866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:52778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:52830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:52958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:56160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:56292 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP1] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP3] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP4] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP7] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP0] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP5] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP2] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP6] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54] INFO:     127.0.0.1:47490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:48340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:48868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:49744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:49754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:49894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:50100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:50874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:52994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:53684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:54186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:54438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:54636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:54864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:55254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:55272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:55494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:55650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:56572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:46338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:47826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:49564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:49954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:51272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:51606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:52164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:52654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:53236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:53332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:53648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:53876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:56202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:56322 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP1] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP3] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP7] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP4] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP5] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP0] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP2] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP6] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54] INFO:     127.0.0.1:50430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:52494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:52874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:53736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:53950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:54148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:54850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:54922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:55324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:55712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:56506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:56626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:56794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:57354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:57756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54 TP0] Decode batch, #running-req: 661, #token: 94520, token usage: 0.10, cuda graph: False, gen throughput (token/s): 7321.56, #queue-req: 0, 
[2025-11-01 11:06:54] INFO:     127.0.0.1:46296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:47434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:47616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:48238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:48992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:49002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:49054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:49216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:49936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:51100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:51348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:51596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:51710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:53508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:56244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:56970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:47726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:49484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:51470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:51604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:51632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:53878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:54722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:46572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:47224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:48700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:50784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:53804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:55264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:54] INFO:     127.0.0.1:56276 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP1] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP3] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP0] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP4] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP7] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP5] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP2] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:54 TP6] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55] INFO:     127.0.0.1:46274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:47350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:48980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:50148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:50662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:52246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:52392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:52886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:53704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:54728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:55566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:55764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:57374 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP1] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP4] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP3] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP0] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP7] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP5] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP2] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP6] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55] INFO:     127.0.0.1:47144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:49296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:52208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:54888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:55236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:55278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:55304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:55400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:57858 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (594, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP1] [fused_moe] using default for (594, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (594, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (594, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP3] [fused_moe] using default for (594, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP4] [fused_moe] using default for (594, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (594, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (594, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP7] [fused_moe] using default for (594, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP5] [fused_moe] using default for (594, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (594, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP0] [fused_moe] using default for (594, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (594, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP2] [fused_moe] using default for (594, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (594, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP6] [fused_moe] using default for (594, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55] INFO:     127.0.0.1:46220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:47854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:49280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:51048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:52132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:54962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:57184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:46720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:49388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:49778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:50218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:50572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:53040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:54954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:55270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:56754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:46460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:48664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:49640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:50486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:53244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:54254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:56176 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP1] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP3] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP7] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP5] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP4] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP0] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP2] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP6] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55] INFO:     127.0.0.1:46350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:48222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:48372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:52472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:52770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:53120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:53158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:53210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:55326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:55328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:57200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:47078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:47106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:47414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:48448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:49418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:49858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:53718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:55798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:56900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:57770 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP1] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP3] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP7] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP4] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP0] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP5] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP2] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP6] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55] INFO:     127.0.0.1:46646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:47372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:48788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:49270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:50898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:53642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:55850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:55970 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP1] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP3] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP4] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP7] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP5] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP0] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP2] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP6] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55] INFO:     127.0.0.1:49330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:51692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:51746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:51956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:52870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:53836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:54968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:55632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:56132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:56328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:56698 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP1] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP3] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP2] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP0] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP7] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP5] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP4] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP6] [fused_moe] using default for (530, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55] INFO:     127.0.0.1:46640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:47208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:49172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:50110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:51792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:52282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:52492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:54326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:56272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:56390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:56740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:57322 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP1] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP3] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP0] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP5] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP4] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP7] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP2] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55 TP6] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-01 11:06:55] INFO:     127.0.0.1:46896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:47356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:47880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:48062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:54502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:56560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:47938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:48354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:48716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:48864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:50382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:51286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:55] INFO:     127.0.0.1:52722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:48426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:49980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:50758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:50992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:52604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:53294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:56642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:47924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:49866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:51214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:53588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:54524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:46176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:46396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:46852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:48746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:48904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:50248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:52066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:56148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:56206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:57074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:57412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:50542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:51844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:52252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:53968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:54604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:55760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:56376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:49110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:50384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:50448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:52012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:52340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:52942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:53522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:53612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:54668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:55340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:55842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:49080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:50734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:53180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:56686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:56726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:50338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:52422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:53742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:53892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:54268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:55476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:56398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:57286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:48030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:48632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:49066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:52480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:53580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:53930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:55624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:56350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:56924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:57028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:57766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:46094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:48872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:51260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:55888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:56006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:56066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:57078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:57554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:57596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:46848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:50274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:51028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:51084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:51972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:52384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:53904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:55778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:57014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:57796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:58032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:47180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:49356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:51680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:57548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:57656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:58028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:46746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:49528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:49540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:50198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:55022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:57534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:57868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:56682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:56994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:56] INFO:     127.0.0.1:57310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:48638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:49466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:49824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:51154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:51758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:55824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:57178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:57270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:57520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:58010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:51146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:53382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:47464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:48000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:48922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:49918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:51432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:51486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:52398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:53194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:54948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:56466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:57384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:57396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:57714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:47996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:49030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:49994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:51928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:53436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:54086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:54826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:56192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:56476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:57224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:51378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:53004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:56026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:56312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:56406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:57340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:57624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:47646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:48138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:51840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:53376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:56222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:57206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:57430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:57920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:48198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:51500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:53596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:54552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:55492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:56594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:57994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:46286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:46388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:47358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:50456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:52816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:54370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:55590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:55748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:52742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:54330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:56042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:56360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:56436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:57170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:57422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:58118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:48274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:51676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:55706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:56120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:56156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:56260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:57496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:57742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:51164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:54090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:55150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:55544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:56182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:47312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:49762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:55434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:55612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:55952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:57154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:57164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57 TP0] Decode batch, #running-req: 317, #token: 57128, token usage: 0.06, cuda graph: True, gen throughput (token/s): 6180.87, #queue-req: 0, 
[2025-11-01 11:06:57] INFO:     127.0.0.1:46730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:48078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:48130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:48578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:52434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:54656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:56668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:58046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:58088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:48474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:48562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:50530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:50632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:53794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:55516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:55662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:55672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:55896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:56652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:57302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:57590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:57762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:58110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:49282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:51178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:51512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:55808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:58066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:48978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:49880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:50354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:57] INFO:     127.0.0.1:56084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:48762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:49956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:52926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:54804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:55784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:56824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:46160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:51838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:55532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:55886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:58074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:47380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:52568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:55464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:55830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:55928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:46188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:49350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:52454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:56920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:54760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:47010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:48544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:47160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:48402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:49808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:52352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:53464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:54894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:56998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:46914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:49448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:52148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:55584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:56218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:58048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:52038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:53406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:48014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:48306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:50406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:52708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:54658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:51452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:55064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:56690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:56942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:47192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:55372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:46994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:48260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:55500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:55838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:50204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:55044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:56592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:47838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:49160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:55874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:56860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:47828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:49680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:52266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:52596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:56492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:48380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:52716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:52738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:55840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:56836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:56912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:50500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:55686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:57670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:51948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:58] INFO:     127.0.0.1:53384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:48486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:55634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:57042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:57346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:56610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:49254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:53622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:56002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:56450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:56540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:57260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:57380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:49910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:54736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:56808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:51394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:51718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:57068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:47408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:47682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:54134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:56424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:56978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:47128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:57086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:57216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:54520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:57212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:57806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:53026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:55964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:56096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:56576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:57652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:46528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:48176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:55898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:57240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:57792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:49168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:55866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:56864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:57580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:51524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:56092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:48498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:56722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:56764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:57900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:57450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:57650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59 TP0] Decode batch, #running-req: 125, #token: 29557, token usage: 0.03, cuda graph: True, gen throughput (token/s): 4058.30, #queue-req: 0, 
[2025-11-01 11:06:59] INFO:     127.0.0.1:46436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:55922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:55700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:56528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:51816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:52890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:55856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:56524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:47470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:55892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:56738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:49848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:54194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:55448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:06:59] INFO:     127.0.0.1:58060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:48910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:56232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:56846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:57060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:57846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:56556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:57058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:50298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:51966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:52396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:58084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:50254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:55550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:57324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:46448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:48314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:53224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:57774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:46236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:55378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:55600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:47056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:47710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:51256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:48946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:52150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:52972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:46950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:48740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:55986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:50046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:50266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:50820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:55528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:56884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:56050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:57242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:53272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:55736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:51076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:53606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:56772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:57492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:57570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:57578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:56178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:47424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:56520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:47020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:52580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:57436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:58026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:51664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:58094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:54592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:00] INFO:     127.0.0.1:57964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:49208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:51490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:56304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:57000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:53356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:53490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:56956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:57522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:57634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:57194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:57936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:55134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:56104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:57608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01 TP0] Decode batch, #running-req: 39, #token: 10660, token usage: 0.01, cuda graph: True, gen throughput (token/s): 2009.90, #queue-req: 0, 
[2025-11-01 11:07:01] INFO:     127.0.0.1:56936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:57372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:57836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:51620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:54634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:56414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:56868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:50926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:47400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:52674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:57106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:57138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:57478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:49934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:47318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:57974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:49028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:55310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:47770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:56710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:01] INFO:     127.0.0.1:57642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:02] INFO:     127.0.0.1:56344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:02] INFO:     127.0.0.1:56820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:02] INFO:     127.0.0.1:53482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:02] INFO:     127.0.0.1:55676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:02] INFO:     127.0.0.1:49878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:02] INFO:     127.0.0.1:56040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:02] INFO:     127.0.0.1:56134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:02 TP0] Decode batch, #running-req: 8, #token: 3452, token usage: 0.00, cuda graph: True, gen throughput (token/s): 844.35, #queue-req: 0, 
[2025-11-01 11:07:02] INFO:     127.0.0.1:56162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:02] INFO:     127.0.0.1:50730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:02] INFO:     127.0.0.1:53168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:02] INFO:     127.0.0.1:56782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:02] INFO:     127.0.0.1:55936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:02] INFO:     127.0.0.1:57100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:02] INFO:     127.0.0.1:55164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:02] INFO:     127.0.0.1:55388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-01 11:07:07] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-11-01 11:07:12] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
